{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-07T11:56:23.081938Z",
     "start_time": "2024-10-07T11:56:23.078806Z"
    }
   },
   "source": [
    "# here for libs\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T11:56:23.123777Z",
     "start_time": "2024-10-07T11:56:23.119579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NVIDIA CUDA\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    \n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    \n",
    "    print(f\"Current GPU device: {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n"
   ],
   "id": "7f4792a7fa3563f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n",
      "Number of available GPUs: 1\n",
      "GPU 0: NVIDIA GeForce RTX 4070 SUPER\n",
      "Current GPU device: 0\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Project plan:\n",
    "https://docs.google.com/document/d/1o2FlMo1je0yv7LeCV_mMHfmbbjH_XfzrfiKAefOh450/edit"
   ],
   "id": "5ac9c3a4785c1b17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T11:56:23.335347Z",
     "start_time": "2024-10-07T11:56:23.169775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#data import\n",
    "data = pd.read_csv('./dataset/diabetes_dataset00.csv')\n",
    "data.head()"
   ],
   "id": "c5c094fc7693d888",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                             Target Genetic Markers Autoantibodies  \\\n",
       "0          Steroid-Induced Diabetes        Positive       Negative   \n",
       "1  Neonatal Diabetes Mellitus (NDM)        Positive       Negative   \n",
       "2                       Prediabetic        Positive       Positive   \n",
       "3                   Type 1 Diabetes        Negative       Positive   \n",
       "4                  Wolfram Syndrome        Negative       Negative   \n",
       "\n",
       "  Family History Environmental Factors  Insulin Levels  Age  BMI  \\\n",
       "0             No               Present              40   44   38   \n",
       "1             No               Present              13    1   17   \n",
       "2            Yes               Present              27   36   24   \n",
       "3             No               Present               8    7   16   \n",
       "4            Yes               Present              17   10   17   \n",
       "\n",
       "  Physical Activity Dietary Habits  ...  Pulmonary Function  \\\n",
       "0              High        Healthy  ...                  76   \n",
       "1              High        Healthy  ...                  60   \n",
       "2              High      Unhealthy  ...                  80   \n",
       "3               Low      Unhealthy  ...                  89   \n",
       "4              High        Healthy  ...                  41   \n",
       "\n",
       "   Cystic Fibrosis Diagnosis  Steroid Use History  Genetic Testing  \\\n",
       "0                         No                   No         Positive   \n",
       "1                        Yes                   No         Negative   \n",
       "2                        Yes                   No         Negative   \n",
       "3                        Yes                   No         Positive   \n",
       "4                         No                   No         Positive   \n",
       "\n",
       "  Neurological Assessments Liver Function Tests Digestive Enzyme Levels  \\\n",
       "0                        3               Normal                      56   \n",
       "1                        1               Normal                      28   \n",
       "2                        1             Abnormal                      55   \n",
       "3                        2             Abnormal                      60   \n",
       "4                        1               Normal                      24   \n",
       "\n",
       "        Urine Test Birth Weight Early Onset Symptoms  \n",
       "0  Ketones Present         2629                   No  \n",
       "1  Glucose Present         1881                  Yes  \n",
       "2  Ketones Present         3622                  Yes  \n",
       "3  Ketones Present         3542                   No  \n",
       "4  Protein Present         1770                   No  \n",
       "\n",
       "[5 rows x 34 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Genetic Markers</th>\n",
       "      <th>Autoantibodies</th>\n",
       "      <th>Family History</th>\n",
       "      <th>Environmental Factors</th>\n",
       "      <th>Insulin Levels</th>\n",
       "      <th>Age</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Physical Activity</th>\n",
       "      <th>Dietary Habits</th>\n",
       "      <th>...</th>\n",
       "      <th>Pulmonary Function</th>\n",
       "      <th>Cystic Fibrosis Diagnosis</th>\n",
       "      <th>Steroid Use History</th>\n",
       "      <th>Genetic Testing</th>\n",
       "      <th>Neurological Assessments</th>\n",
       "      <th>Liver Function Tests</th>\n",
       "      <th>Digestive Enzyme Levels</th>\n",
       "      <th>Urine Test</th>\n",
       "      <th>Birth Weight</th>\n",
       "      <th>Early Onset Symptoms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Steroid-Induced Diabetes</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>No</td>\n",
       "      <td>Present</td>\n",
       "      <td>40</td>\n",
       "      <td>44</td>\n",
       "      <td>38</td>\n",
       "      <td>High</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "      <td>Normal</td>\n",
       "      <td>56</td>\n",
       "      <td>Ketones Present</td>\n",
       "      <td>2629</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neonatal Diabetes Mellitus (NDM)</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>No</td>\n",
       "      <td>Present</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>High</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>28</td>\n",
       "      <td>Glucose Present</td>\n",
       "      <td>1881</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prediabetic</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Present</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>High</td>\n",
       "      <td>Unhealthy</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "      <td>Abnormal</td>\n",
       "      <td>55</td>\n",
       "      <td>Ketones Present</td>\n",
       "      <td>3622</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Type 1 Diabetes</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "      <td>No</td>\n",
       "      <td>Present</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>Low</td>\n",
       "      <td>Unhealthy</td>\n",
       "      <td>...</td>\n",
       "      <td>89</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>Abnormal</td>\n",
       "      <td>60</td>\n",
       "      <td>Ketones Present</td>\n",
       "      <td>3542</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wolfram Syndrome</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Present</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>High</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>...</td>\n",
       "      <td>41</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>24</td>\n",
       "      <td>Protein Present</td>\n",
       "      <td>1770</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.Preprocessing\n",
   "id": "d21af4f87de70822"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T11:56:23.439422Z",
     "start_time": "2024-10-07T11:56:23.399181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##check missing data\n",
    "missing_values = data.isnull().sum()\n",
    "missing_values[missing_values > 0]\n"
   ],
   "id": "c1beda73b90b8707",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T11:56:23.881624Z",
     "start_time": "2024-10-07T11:56:23.493502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##lable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "\n",
    "categorical_columns = ['Genetic Markers', 'Autoantibodies', 'Family History' , 'Environmental Factors', 'Dietary Habits', 'Ethnicity', 'Socioeconomic Factors', 'Smoking Status', 'Alcohol Consumption', 'Glucose Tolerance Test', 'Previous Gestational Diabetes', 'Pregnancy History', 'Cystic Fibrosis Diagnosis', 'Environmental Factors', 'Steroid Use History', 'Genetic Testing', 'Liver Function Tests', 'Urine Test', 'Early Onset Symptoms', 'Physical Activity', 'History of PCOS', 'Target']\n",
    "\n",
    "\n",
    "for col in categorical_columns:\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "output_file_path = './dataset/numeric_diabetes_dataset.csv'\n",
    "data.to_csv(output_file_path, index=False) \n",
    "\n",
    "\n",
    "\n",
    "data.head(50)\n",
    "\n",
    "X = data.drop('Target', axis=1)\n",
    "y = data['Target']\n",
    "\n",
    "\n"
   ],
   "id": "1c9c465a14908746",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T11:56:23.975754Z",
     "start_time": "2024-10-07T11:56:23.888641Z"
    }
   },
   "cell_type": "code",
   "source": "print(data.describe())\n",
   "id": "95f68e3d138b8523",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Target  Genetic Markers  Autoantibodies  Family History  \\\n",
      "count  70000.000000     70000.000000    70000.000000    70000.000000   \n",
      "mean       5.989729         0.501443        0.499171        0.502400   \n",
      "std        3.737753         0.500001        0.500003        0.499998   \n",
      "min        0.000000         0.000000        0.000000        0.000000   \n",
      "25%        3.000000         0.000000        0.000000        0.000000   \n",
      "50%        6.000000         1.000000        0.000000        1.000000   \n",
      "75%        9.000000         1.000000        1.000000        1.000000   \n",
      "max       12.000000         1.000000        1.000000        1.000000   \n",
      "\n",
      "       Environmental Factors  Insulin Levels           Age           BMI  \\\n",
      "count           70000.000000    70000.000000  70000.000000  70000.000000   \n",
      "mean                0.498743       21.607443     32.020700     24.782943   \n",
      "std                 0.500002       10.785852     21.043173      6.014236   \n",
      "min                 0.000000        5.000000      0.000000     12.000000   \n",
      "25%                 0.000000       13.000000     14.000000     20.000000   \n",
      "50%                 0.000000       19.000000     31.000000     25.000000   \n",
      "75%                 1.000000       28.000000     49.000000     29.000000   \n",
      "max                 1.000000       49.000000     79.000000     39.000000   \n",
      "\n",
      "       Physical Activity  Dietary Habits  ...  Pulmonary Function  \\\n",
      "count       70000.000000    70000.000000  ...        70000.000000   \n",
      "mean            1.002886        0.499714  ...           70.264671   \n",
      "std             0.816369        0.500003  ...           11.965600   \n",
      "min             0.000000        0.000000  ...           30.000000   \n",
      "25%             0.000000        0.000000  ...           63.000000   \n",
      "50%             1.000000        0.000000  ...           72.000000   \n",
      "75%             2.000000        1.000000  ...           79.000000   \n",
      "max             2.000000        1.000000  ...           89.000000   \n",
      "\n",
      "       Cystic Fibrosis Diagnosis  Steroid Use History  Genetic Testing  \\\n",
      "count               70000.000000         70000.000000     70000.000000   \n",
      "mean                    0.498071             0.497971         0.504500   \n",
      "std                     0.500000             0.499999         0.499983   \n",
      "min                     0.000000             0.000000         0.000000   \n",
      "25%                     0.000000             0.000000         0.000000   \n",
      "50%                     0.000000             0.000000         1.000000   \n",
      "75%                     1.000000             1.000000         1.000000   \n",
      "max                     1.000000             1.000000         1.000000   \n",
      "\n",
      "       Neurological Assessments  Liver Function Tests  \\\n",
      "count              70000.000000          70000.000000   \n",
      "mean                   1.804157              0.500271   \n",
      "std                    0.680154              0.500003   \n",
      "min                    1.000000              0.000000   \n",
      "25%                    1.000000              0.000000   \n",
      "50%                    2.000000              1.000000   \n",
      "75%                    2.000000              1.000000   \n",
      "max                    3.000000              1.000000   \n",
      "\n",
      "       Digestive Enzyme Levels    Urine Test  Birth Weight  \\\n",
      "count             70000.000000  70000.000000  70000.000000   \n",
      "mean                 46.420529      1.505171   3097.061071   \n",
      "std                  19.391089      1.118669    713.837300   \n",
      "min                  10.000000      0.000000   1500.000000   \n",
      "25%                  31.000000      1.000000   2629.000000   \n",
      "50%                  48.000000      2.000000   3103.000000   \n",
      "75%                  61.000000      3.000000   3656.250000   \n",
      "max                  99.000000      3.000000   4499.000000   \n",
      "\n",
      "       Early Onset Symptoms  \n",
      "count          70000.000000  \n",
      "mean               0.499157  \n",
      "std                0.500003  \n",
      "min                0.000000  \n",
      "25%                0.000000  \n",
      "50%                0.000000  \n",
      "75%                1.000000  \n",
      "max                1.000000  \n",
      "\n",
      "[8 rows x 34 columns]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.Validation methods",
   "id": "48b53611c9a36cf9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.1 cross validation",
   "id": "212e767daf192fda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T11:56:24.030319Z",
     "start_time": "2024-10-07T11:56:24.025321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def manual_train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    np.random.seed(random_state) \n",
    "    indices = np.random.permutation(len(X))  \n",
    "    test_set_size = int(len(X) * test_size)  \n",
    "    test_indices = indices[:test_set_size]  \n",
    "    train_indices = indices[test_set_size:]  \n",
    "    \n",
    "\n",
    "    X_train = X.iloc[train_indices]\n",
    "    X_test = X.iloc[test_indices]\n",
    "    y_train = y.iloc[train_indices]\n",
    "    y_test = y.iloc[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n"
   ],
   "id": "8d422868444ea729",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2 k fold validation",
   "id": "3bd73d2b69c1f715"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9ad32111e49b9393"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T11:56:24.045336Z",
     "start_time": "2024-10-07T11:56:24.037831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def manual_kfold(X, y, k=5, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(len(X))\n",
    "    fold_size = len(X) // k\n",
    "    folds = []\n",
    "\n",
    "    for i in range(k):\n",
    "        test_indices = indices[i*fold_size:(i+1)*fold_size]\n",
    "        train_indices = np.concatenate([indices[:i*fold_size], indices[(i+1)*fold_size:]])\n",
    "        folds.append((train_indices, test_indices))\n",
    "    \n",
    "    return folds\n",
    "\n",
    "\n",
    "k_folds = manual_kfold(X, y, k=5, random_state=42)\n",
    "\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(k_folds):\n",
    "    print(f\"Fold {i+1}: Train size = {len(train_idx)}, Test size = {len(test_idx)}\")\n"
   ],
   "id": "405d53fbf8995f84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Train size = 56000, Test size = 14000\n",
      "Fold 2: Train size = 56000, Test size = 14000\n",
      "Fold 3: Train size = 56000, Test size = 14000\n",
      "Fold 4: Train size = 56000, Test size = 14000\n",
      "Fold 5: Train size = 56000, Test size = 14000\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9ddd6b6d7df6fda8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T11:56:24.073371Z",
     "start_time": "2024-10-07T11:56:24.071368Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e554817496aa59af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.ML algorithms and Experimental results",
   "id": "36629c0afab2358a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1 Simple algorithms: Random Forest",
   "id": "efc89c6152cc6710"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-07T11:56:24.096655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "\n",
    "train_sizes = np.concatenate(([0.01], np.logspace(-1, 0, 3)))  # [0.01, 0.1, 0.5, 1.0]\n",
    "n_repeats = 20  \n",
    "accuracies = []\n",
    "train_sizes_actual = []\n",
    "all_y_test = []\n",
    "all_y_pred = []\n",
    "\n",
    "\n",
    "experiments_per_size = n_repeats // len(train_sizes)\n",
    "\n",
    "\n",
    "for size in train_sizes:\n",
    "    for i in range(experiments_per_size):\n",
    "        X_train, X_test, y_train, y_test = manual_train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "        \n",
    "       \n",
    "        subset_train_idx = X_train.index[:int(len(X_train) * size)]\n",
    "        X_train_subset = X_train.loc[subset_train_idx]\n",
    "        y_train_subset = y_train.loc[subset_train_idx]\n",
    "        \n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=200, random_state=42, max_depth=None)\n",
    "        model.fit(X_train_subset, y_train_subset)\n",
    "        \n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        train_sizes_actual.append(len(X_train_subset))\n",
    "        \n",
    "        \n",
    "        all_y_test.extend(y_test)\n",
    "        all_y_pred.extend(y_pred)\n",
    "        \n",
    "        print(f\"Experiment with Train Size {len(X_train_subset)}: Accuracy = {accuracy:.3f}\")\n",
    "\n",
    "\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "\n",
    "# learning curve\n",
    "plt.figure()\n",
    "plt.plot(train_sizes_actual, accuracies, marker='o', label='Accuracy')\n",
    "plt.title(\"Learning Curve with Train Sizes\")\n",
    "plt.xlabel(\"Training Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Error bar\n",
    "plt.figure()\n",
    "plt.errorbar(x=train_sizes_actual, y=accuracies, yerr=std_accuracy, fmt='o', label='Accuracy with error bar')\n",
    "plt.title(\"Learning Curve with Error Bars\")\n",
    "plt.xlabel(\"Training Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "#conf matrix\n",
    "conf_matrix = confusion_matrix(all_y_test, all_y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(classification_report(all_y_test, all_y_pred))\n"
   ],
   "id": "2a70a1bc22c32151",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment with Train Size 560: Accuracy = 0.789\n",
      "Experiment with Train Size 560: Accuracy = 0.760\n",
      "Experiment with Train Size 560: Accuracy = 0.799\n",
      "Experiment with Train Size 560: Accuracy = 0.769\n",
      "Experiment with Train Size 560: Accuracy = 0.781\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 30\u001B[0m\n\u001B[0;32m     26\u001B[0m y_train_subset \u001B[38;5;241m=\u001B[39m y_train\u001B[38;5;241m.\u001B[39mloc[subset_train_idx]\n\u001B[0;32m     29\u001B[0m model \u001B[38;5;241m=\u001B[39m RandomForestClassifier(n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m200\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, max_depth\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m---> 30\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_subset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_subset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[0;32m     34\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m accuracy_score(y_test, y_pred)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:1152\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1145\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1147\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1148\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1149\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1150\u001B[0m     )\n\u001B[0;32m   1151\u001B[0m ):\n\u001B[1;32m-> 1152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001B[0m, in \u001B[0;36mBaseForest.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    445\u001B[0m trees \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    446\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_estimator(append\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[0;32m    447\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_more_estimators)\n\u001B[0;32m    448\u001B[0m ]\n\u001B[0;32m    450\u001B[0m \u001B[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001B[39;00m\n\u001B[0;32m    451\u001B[0m \u001B[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001B[39;00m\n\u001B[0;32m    452\u001B[0m \u001B[38;5;66;03m# making threading more efficient than multiprocessing in\u001B[39;00m\n\u001B[0;32m    453\u001B[0m \u001B[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001B[39;00m\n\u001B[0;32m    454\u001B[0m \u001B[38;5;66;03m# parallel_backend contexts set at a higher level,\u001B[39;00m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;66;03m# since correctness does not rely on using threads.\u001B[39;00m\n\u001B[1;32m--> 456\u001B[0m trees \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    457\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    458\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    459\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprefer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mthreads\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    461\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_parallel_build_trees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    462\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    463\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbootstrap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    464\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    465\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    466\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    467\u001B[0m \u001B[43m        \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    468\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    469\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    470\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclass_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    471\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    472\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    473\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    474\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    476\u001B[0m \u001B[38;5;66;03m# Collect newly grown trees\u001B[39;00m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_\u001B[38;5;241m.\u001B[39mextend(trees)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     60\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     61\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     62\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     64\u001B[0m )\n\u001B[1;32m---> 65\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\joblib\\parallel.py:1918\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1916\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[0;32m   1917\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1920\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[0;32m   1921\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[0;32m   1922\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[0;32m   1923\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[0;32m   1924\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[0;32m   1925\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\joblib\\parallel.py:1847\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1845\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1846\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1847\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1848\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1849\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    125\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[1;32m--> 127\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001B[0m, in \u001B[0;36m_parallel_build_trees\u001B[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001B[0m\n\u001B[0;32m    185\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m class_weight \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced_subsample\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    186\u001B[0m         curr_sample_weight \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m compute_sample_weight(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m\"\u001B[39m, y, indices\u001B[38;5;241m=\u001B[39mindices)\n\u001B[1;32m--> 188\u001B[0m     \u001B[43mtree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurr_sample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    190\u001B[0m     tree\u001B[38;5;241m.\u001B[39mfit(X, y, sample_weight\u001B[38;5;241m=\u001B[39msample_weight, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:1152\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1145\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1147\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1148\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1149\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1150\u001B[0m     )\n\u001B[0;32m   1151\u001B[0m ):\n\u001B[1;32m-> 1152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001B[0m, in \u001B[0;36mDecisionTreeClassifier.fit\u001B[1;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[0;32m    928\u001B[0m \u001B[38;5;129m@_fit_context\u001B[39m(prefer_skip_nested_validation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    929\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m    930\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001B[39;00m\n\u001B[0;32m    931\u001B[0m \n\u001B[0;32m    932\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    956\u001B[0m \u001B[38;5;124;03m        Fitted estimator.\u001B[39;00m\n\u001B[0;32m    957\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 959\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    960\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    961\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    962\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    963\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheck_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    964\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    965\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001B[0m, in \u001B[0;36mBaseDecisionTree._fit\u001B[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001B[0m\n\u001B[0;32m    432\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    433\u001B[0m     builder \u001B[38;5;241m=\u001B[39m BestFirstTreeBuilder(\n\u001B[0;32m    434\u001B[0m         splitter,\n\u001B[0;32m    435\u001B[0m         min_samples_split,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    440\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_impurity_decrease,\n\u001B[0;32m    441\u001B[0m     )\n\u001B[1;32m--> 443\u001B[0m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtree_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmissing_values_in_feature_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    445\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_classifier(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    446\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-10-07T11:56:29.310922800Z",
     "start_time": "2024-10-01T10:02:44.353920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def nested_cross_validation(X, y, param_grid, k_outer=5, k_inner=3):\n",
    "    outer_folds = manual_kfold(X, y, k=k_outer, random_state=42)\n",
    "    outer_scores = []\n",
    "    \n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(outer_folds):\n",
    "        X_train_outer, X_test_outer = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train_outer, y_test_outer = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        best_params = None\n",
    "        best_score = -np.inf\n",
    "        \n",
    "        \n",
    "        inner_folds = manual_kfold(X_train_outer, y_train_outer, k=k_inner, random_state=42)\n",
    "        \n",
    "        for params in param_grid:\n",
    "            inner_scores = []\n",
    "            \n",
    "            \n",
    "            for inner_train_idx, inner_val_idx in inner_folds:\n",
    "                X_train_inner, X_val_inner = X_train_outer.iloc[inner_train_idx], X_train_outer.iloc[inner_val_idx]\n",
    "                y_train_inner, y_val_inner = y_train_outer.iloc[inner_train_idx], y_train_outer.iloc[inner_val_idx]\n",
    "                \n",
    "                model = RandomForestClassifier(n_estimators=params['n_estimators'], max_depth=params['max_depth'], random_state=42)\n",
    "                model.fit(X_train_inner, y_train_inner)\n",
    "                \n",
    "                y_val_pred = model.predict(X_val_inner)\n",
    "                inner_scores.append(accuracy_score(y_val_inner, y_val_pred))\n",
    "            \n",
    "            mean_inner_score = np.mean(inner_scores)\n",
    "            \n",
    "           \n",
    "            if mean_inner_score > best_score:\n",
    "                best_score = mean_inner_score\n",
    "                best_params = params\n",
    "        \n",
    "       \n",
    "        best_model = RandomForestClassifier(n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], random_state=42)\n",
    "        best_model.fit(X_train_outer, y_train_outer)\n",
    "        y_test_pred = best_model.predict(X_test_outer)\n",
    "        \n",
    "        test_accuracy = accuracy_score(y_test_outer, y_test_pred)\n",
    "        outer_scores.append(test_accuracy)\n",
    "        \n",
    "        print(f\"Fold {fold_idx+1}: Best Params = {best_params}, Test Accuracy = {test_accuracy:.3f}\")\n",
    "    \n",
    "    \n",
    "    mean_score = np.mean(outer_scores)\n",
    "    std_score = np.std(outer_scores)\n",
    "    print(f\"Nested CV Mean Accuracy: {mean_score:.3f}, Std: {std_score:.3f}\")\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': 50, 'max_depth': 10},\n",
    "    {'n_estimators': 100, 'max_depth': 20},\n",
    "    {'n_estimators': 200, 'max_depth': None}\n",
    "]\n",
    "\n",
    "\n",
    "nested_cross_validation(X, y, param_grid)\n"
   ],
   "id": "dafd41df0411368b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2 Medium algorithms: MLP",
   "id": "99f807555018350f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T11:56:34.551679Z",
     "start_time": "2024-10-07T11:56:34.548678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ],
   "id": "aff0291452afcd76",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "923c1cb73c7a7ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T11:56:35.864121Z",
     "start_time": "2024-10-07T11:56:35.784565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_perceptron_pytorch(X_train_scaled, X_test_scaled, y_train, y_test, n_epochs=30, eta=0.01, optimizer='Adam', params=None):\n",
    "    # Detect if CUDA is available and set the device accordingly\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Convert Pandas DataFrames to NumPy arrays\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = X_train_scaled.to_numpy(), X_test_scaled.to_numpy(), y_train.to_numpy(), y_test.to_numpy()\n",
    "    # Convert to PyTorch tensors and move them to the appropriate device\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = torch.from_numpy(X_train_scaled).to(device), torch.from_numpy(X_test_scaled).to(device), torch.from_numpy(y_train).to(device), torch.from_numpy(y_test).to(device)\n",
    "\n",
    "    # Check number of classes\n",
    "    n_classes = len(torch.unique(y_train))\n",
    "\n",
    "    # Create iterable dataset in Torch format\n",
    "    train_ds = torch.utils.data.TensorDataset(X_train_scaled, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=32)\n",
    "    test_ds = torch.utils.data.TensorDataset(X_test_scaled, y_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "    # Create the multi-layer perceptron model and move it to the device\n",
    "    model = nn.Sequential(\n",
    "            nn.Linear(33, 128, dtype=torch.float64),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64, dtype=torch.float64),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32, dtype=torch.float64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16, dtype=torch.float64),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.2),\n",
    "            nn.Linear(16, n_classes, dtype=torch.float64),\n",
    "            ).to(device)  # Move model to device\n",
    "\n",
    "    # Define the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Use the provided or default parameters\n",
    "    if params:\n",
    "        n_epochs = params.get('n_epochs', n_epochs)\n",
    "        eta = params.get('eta', eta)\n",
    "        optimizer = params.get('optimizer', optimizer)\n",
    "\n",
    "    # Setup the optimizer. This implements the basic gradient descent update\n",
    "    if optimizer == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=eta)\n",
    "    elif optimizer == 'Momentum':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=eta, momentum=0.9)\n",
    "    elif optimizer == 'Adagrad':\n",
    "        optimizer = torch.optim.Adagrad(model.parameters(), lr=eta, eps=1e-10)\n",
    "    elif optimizer == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=eta)\n",
    "    else:\n",
    "        raise ValueError('Invalid optimizer')\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, (xi, yi) in enumerate(train_loader):\n",
    "            # Compute loss F(w)\n",
    "            xi, yi = xi.to(device), yi.to(device).long()  # Move data to the device\n",
    "            logits = model(xi)\n",
    "            loss = criterion(logits, yi)\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            train_acc = torch.mean(torch.eq(predictions, yi).float()).item()\n",
    "\n",
    "            loss.backward()               # Backward pass (compute parameter gradients)\n",
    "            optimizer.step()              # Update weight parameter using SGD\n",
    "            optimizer.zero_grad()         # Reset gradients to zero for next iteration\n",
    "\n",
    "    # Evaluate the model, not necessary for training\n",
    "    with torch.no_grad():\n",
    "        for xi, yi in test_loader:\n",
    "            xi, yi = xi.to(device), yi.to(device).long()  # Move data to the device\n",
    "            logits = model(xi)\n",
    "            loss = criterion(logits, yi)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            accuracy = torch.mean(torch.eq(predictions, yi).float()).item()\n",
    "\n",
    "    return accuracy\n"
   ],
   "id": "8712ceb1e9b2b93f",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T12:13:37.026748900Z",
     "start_time": "2024-10-07T12:13:32.557761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = manual_train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "\n",
    "params = {\n",
    "    'n_epochs': 20,\n",
    "    'eta': 0.01,\n",
    "    'optimizer': 'Adam'\n",
    "}\n",
    "\n",
    "\n",
    "accuracy = train_perceptron_pytorch(X_train_scaled, X_test_scaled, y_train, y_test, n_epochs=params['n_epochs'], eta=params['eta'], optimizer=params['optimizer'])\n",
    "\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ],
   "id": "503d2adbeab53deb",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 20\u001B[0m\n\u001B[0;32m     10\u001B[0m X_test_scaled \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(X_test_scaled, columns\u001B[38;5;241m=\u001B[39mX_test\u001B[38;5;241m.\u001B[39mcolumns)\n\u001B[0;32m     13\u001B[0m params \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m20\u001B[39m,\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meta\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.01\u001B[39m,\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moptimizer\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAdam\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     17\u001B[0m }\n\u001B[1;32m---> 20\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_perceptron_pytorch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_scaled\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test_scaled\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mn_epochs\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meta\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moptimizer\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest Accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[20], line 64\u001B[0m, in \u001B[0;36mtrain_perceptron_pytorch\u001B[1;34m(X_train_scaled, X_test_scaled, y_train, y_test, n_epochs, eta, optimizer, params)\u001B[0m\n\u001B[0;32m     61\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(logits, yi)\n\u001B[0;32m     63\u001B[0m predictions \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39margmax(logits, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 64\u001B[0m train_acc \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mmean(torch\u001B[38;5;241m.\u001B[39meq(predictions, yi)\u001B[38;5;241m.\u001B[39mfloat())\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     66\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()               \u001B[38;5;66;03m# Backward pass (compute parameter gradients)\u001B[39;00m\n\u001B[0;32m     67\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()              \u001B[38;5;66;03m# Update weight parameter using SGD\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "cd61492827ab58e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T11:56:29.312924200Z",
     "start_time": "2024-10-01T08:39:22.555928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def nested_cross_validation(X, y, param_grid, outer_k=5, inner_k=3, n_epochs=30, eta=0.01, optimizer='Adam'):\n",
    "    outer_kf = manual_kfold(X, y, k=outer_k, random_state=42)\n",
    "    outer_scores = []\n",
    "\n",
    "    for outer_fold, (train_index, test_index) in enumerate(outer_kf):\n",
    "        #print(f\"Outer Fold {outer_fold + 1}\")\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        best_params = None\n",
    "        best_score = -np.inf\n",
    "\n",
    "        inner_kf = manual_kfold(X_train, y_train, k=inner_k, random_state=42)\n",
    "\n",
    "        for params in param_grid:\n",
    "            inner_scores = []\n",
    "            for inner_fold, (inner_train_index, inner_val_index) in enumerate(inner_kf):\n",
    "                X_train_inner, X_val_inner = X_train.iloc[inner_train_index], X_train.iloc[inner_val_index]\n",
    "                y_train_inner, y_val_inner = y_train.iloc[inner_train_index], y_train.iloc[inner_val_index]\n",
    "\n",
    "                # Scaling\n",
    "                scaler = StandardScaler()\n",
    "                X_train_inner_scaled = scaler.fit_transform(X_train_inner)\n",
    "                X_val_inner_scaled = scaler.transform(X_val_inner)\n",
    "\n",
    "                # Convert to Pandas DataFrame\n",
    "                X_train_inner_scaled = pd.DataFrame(X_train_inner_scaled, columns=X_train_inner.columns)\n",
    "                X_val_inner_scaled = pd.DataFrame(X_val_inner_scaled, columns=X_val_inner.columns)\n",
    "\n",
    "                # Train model\n",
    "                val_acc = train_perceptron_pytorch(X_train_inner_scaled, X_val_inner_scaled, y_train_inner, y_val_inner, n_epochs, eta, optimizer, params)\n",
    "                inner_scores.append(val_acc)\n",
    "\n",
    "            mean_inner_score = np.mean(inner_scores)\n",
    "            if mean_inner_score > best_score:\n",
    "                best_score = mean_inner_score\n",
    "                best_params = params\n",
    "\n",
    "        # Train the model with the best parameters\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Convert to Pandas DataFrame\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "        test_acc = train_perceptron_pytorch(X_train_scaled, X_test_scaled, y_train, y_test, n_epochs, eta, optimizer, best_params)\n",
    "        outer_scores.append(test_acc)\n",
    "\n",
    "        print(f\"Outer Fold {outer_fold + 1}: Best Params = {best_params}, Test Accuracy = {test_acc:.3f}\")\n",
    "\n",
    "    mean_score = np.mean(outer_scores)\n",
    "    std_score = np.std(outer_scores)\n",
    "    print(f\"Nested CV Mean Accuracy: {mean_score:.3f}, Std: {std_score:.3f}\")\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {'n_epochs': 20, 'eta': 0.01, 'optimizer': 'Momentum'},\n",
    "    {'n_epochs': 30, 'eta': 0.01, 'optimizer': 'Momentum'},\n",
    "    {'n_epochs': 20, 'eta': 0.01, 'optimizer': 'Adam'},\n",
    "    {'n_epochs': 30, 'eta': 0.01, 'optimizer': 'Adam'}\n",
    "]\n",
    "\n",
    "\n",
    "nested_cross_validation(X, y, param_grid, outer_k=5, inner_k=3, n_epochs=30, eta=0.01, optimizer='Adam')"
   ],
   "id": "4a4cdff795127ee0",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[67], line 66\u001B[0m\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNested CV Mean Accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmean_score\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Std: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstd_score\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     58\u001B[0m param_grid \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     59\u001B[0m     {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m20\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meta\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moptimizer\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMomentum\u001B[39m\u001B[38;5;124m'\u001B[39m},\n\u001B[0;32m     60\u001B[0m     {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m30\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meta\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moptimizer\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMomentum\u001B[39m\u001B[38;5;124m'\u001B[39m},\n\u001B[0;32m     61\u001B[0m     {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m20\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meta\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moptimizer\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAdam\u001B[39m\u001B[38;5;124m'\u001B[39m},\n\u001B[0;32m     62\u001B[0m     {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m30\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meta\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moptimizer\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAdam\u001B[39m\u001B[38;5;124m'\u001B[39m}\n\u001B[0;32m     63\u001B[0m ]\n\u001B[1;32m---> 66\u001B[0m \u001B[43mnested_cross_validation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_grid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mouter_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minner_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mAdam\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[67], line 7\u001B[0m, in \u001B[0;36mnested_cross_validation\u001B[1;34m(X, y, param_grid, outer_k, inner_k, n_epochs, eta, optimizer)\u001B[0m\n\u001B[0;32m      3\u001B[0m outer_scores \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m outer_fold, (train_index, test_index) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(outer_kf):\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;66;03m#print(f\"Outer Fold {outer_fold + 1}\")\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m     X_train, X_test \u001B[38;5;241m=\u001B[39m \u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miloc\u001B[49m[train_index], X\u001B[38;5;241m.\u001B[39miloc[test_index]\n\u001B[0;32m      8\u001B[0m     y_train, y_test \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39miloc[train_index], y\u001B[38;5;241m.\u001B[39miloc[test_index]\n\u001B[0;32m     10\u001B[0m     best_params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b963ab3d46b1221f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.3 Complex algorithms: CNN + LSTM",
   "id": "4f099234a4b57f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T11:56:29.313924700Z",
     "start_time": "2024-10-01T08:57:28.129056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the processed dataset\n",
    "data = pd.read_csv('./dataset/numeric_diabetes_dataset.csv')\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = data.drop('Target', axis=1).values\n",
    "y = data['Target'].values\n",
    "\n",
    "# 检查目标列中的唯一值以确定类别数\n",
    "num_classes = len(np.unique(y))\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Use DataLoader to handle batches\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)  # Reduce batch_size if necessary\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the CNN-BiLSTM Model for multi-class classification\n",
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(CNN_BiLSTM, self).__init__()\n",
    "\n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=1)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size=1)\n",
    "        \n",
    "        # BiLSTM layers\n",
    "        self.lstm = nn.LSTM(input_size=33, hidden_size=32, num_layers=1, bidirectional=True, batch_first=True)  # Adjust input_size to 33\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(64, num_classes)  # Adjust num_classes for multi-class output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # CNN part\n",
    "        x = x.unsqueeze(1)  # Add channel dimension for Conv1D\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        # BiLSTM part\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        # Classification part\n",
    "        x = x[:, -1, :]  # Take the last hidden state\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Initialize and train the CNN-BiLSTM model for multi-class classification\n",
    "input_size = X_train.shape[1]  # Number of features in your dataset\n",
    "model = CNN_BiLSTM(input_size=input_size, num_classes=num_classes)  # Using actual number of classes\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, test_loader, num_epochs=100, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss is suitable for multi-class classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        # Training\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in test_loader:\n",
    "                    test_outputs = model(X_batch)\n",
    "                    _, predicted = torch.max(test_outputs.data, 1)\n",
    "                    total += y_batch.size(0)\n",
    "                    correct += (predicted == y_batch).sum().item()\n",
    "            accuracy = correct / total\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Train the model with DataLoader\n",
    "train_model(model, train_loader, test_loader, num_epochs=100)\n"
   ],
   "id": "49ee626e77de4cd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 13\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[69], line 111\u001B[0m\n\u001B[0;32m    108\u001B[0m             \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m], Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Test Accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    110\u001B[0m \u001B[38;5;66;03m# Train the model with DataLoader\u001B[39;00m\n\u001B[1;32m--> 111\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[69], line 95\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, train_loader, test_loader, num_epochs, learning_rate)\u001B[0m\n\u001B[0;32m     93\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     94\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m---> 95\u001B[0m     \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     97\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (epoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     98\u001B[0m     model\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\optimizer.py:469\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    467\u001B[0m \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m cast(Optimizer, \u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    468\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 469\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[0;32m    470\u001B[0m     \u001B[38;5;66;03m# call optimizer step pre hooks\u001B[39;00m\n\u001B[0;32m    471\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m pre_hook \u001B[38;5;129;01min\u001B[39;00m chain(\n\u001B[0;32m    472\u001B[0m         _global_optimizer_pre_hooks\u001B[38;5;241m.\u001B[39mvalues(),\n\u001B[0;32m    473\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_pre_hooks\u001B[38;5;241m.\u001B[39mvalues(),\n\u001B[0;32m    474\u001B[0m     ):\n\u001B[0;32m    475\u001B[0m         result \u001B[38;5;241m=\u001B[39m pre_hook(\u001B[38;5;28mself\u001B[39m, args, kwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\autograd\\profiler.py:688\u001B[0m, in \u001B[0;36mrecord_function.__enter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    687\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__enter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 688\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecord \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprofiler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_record_function_enter_new\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    689\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\n\u001B[0;32m    690\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    691\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\_ops.py:1061\u001B[0m, in \u001B[0;36mOpOverloadPacket.__call__\u001B[1;34m(self_, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1059\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m self_\u001B[38;5;241m.\u001B[39m_has_torchbind_op_overload \u001B[38;5;129;01mand\u001B[39;00m _must_dispatch_in_python(args, kwargs):\n\u001B[0;32m   1060\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _call_overload_packet_from_python(self_, args, kwargs)\n\u001B[1;32m-> 1061\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mself_\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_op\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 69
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
