{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Af9TkeJYmtE1",
    "outputId": "e5df38f2-683e-45af-f058-a6b19722213b",
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:18.822624Z",
     "start_time": "2024-10-09T03:27:18.816378Z"
    }
   },
   "source": [
    "# NVIDIA CUDA\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available！\")\n",
    "\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "    print(f\"Current GPU device: {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available！\n",
      "Number of available GPUs: 1\n",
      "GPU 0: NVIDIA GeForce RTX 4070 SUPER\n",
      "Current GPU device: 0\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2HQxZxkvm5bz",
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:18.905881Z",
     "start_time": "2024-10-09T03:27:18.901755Z"
    }
   },
   "source": [
    "# here for libs\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uSd37dItm9Tc",
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:20.151410Z",
     "start_time": "2024-10-09T03:27:19.092943Z"
    }
   },
   "source": [
    "# Load individual datasets\n",
    "olist_customers_df = pd.read_csv('./data/olist_customers_dataset.csv')\n",
    "order_items_df = pd.read_csv('./data/olist_order_items_dataset.csv')\n",
    "order_payments_df = pd.read_csv('./data/olist_order_payments_dataset.csv')\n",
    "order_reviews_df = pd.read_csv('./data/olist_order_reviews_dataset.csv')\n",
    "olist_orders_df = pd.read_csv('./data/olist_orders_dataset.csv')\n",
    "sellers_df = pd.read_csv('./data/olist_sellers_dataset.csv')\n",
    "products_df = pd.read_csv('./data/olist_products_dataset.csv')\n",
    "product_category_name_translation_df = pd.read_csv('./data/product_category_name_translation.csv')"
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Aji1QZ1CoOK1",
    "outputId": "d7c60f4e-9eff-45cd-ed05-9a4e3ae7b219",
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:20.335047Z",
     "start_time": "2024-10-09T03:27:20.329024Z"
    }
   },
   "source": [
    "olist_customers_df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                        customer_id                customer_unique_id  \\\n",
       "0  06b8999e2fba1a1fbc88172c00ba8bc7  861eff4711a542e4b93843c6dd7febb0   \n",
       "1  18955e83d337fd6b2def6b18a428ac77  290c77bc529b7ac935b93aa66c333dc3   \n",
       "2  4e7b3e00288586ebd08712fdd0374a03  060e732b5b29e8181a18229c7b0b2b5e   \n",
       "3  b2b6027bc5c5109e529d4dc6358b12c3  259dac757896d24d7702b9acbbff3f3c   \n",
       "4  4f2d8ab171c80ec8364f7c12e35b23ad  345ecd01c38d18a9036ed96c73b8d066   \n",
       "\n",
       "   customer_zip_code_prefix          customer_city customer_state  \n",
       "0                     14409                 franca             SP  \n",
       "1                      9790  sao bernardo do campo             SP  \n",
       "2                      1151              sao paulo             SP  \n",
       "3                      8775        mogi das cruzes             SP  \n",
       "4                     13056               campinas             SP  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_unique_id</th>\n",
       "      <th>customer_zip_code_prefix</th>\n",
       "      <th>customer_city</th>\n",
       "      <th>customer_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>06b8999e2fba1a1fbc88172c00ba8bc7</td>\n",
       "      <td>861eff4711a542e4b93843c6dd7febb0</td>\n",
       "      <td>14409</td>\n",
       "      <td>franca</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18955e83d337fd6b2def6b18a428ac77</td>\n",
       "      <td>290c77bc529b7ac935b93aa66c333dc3</td>\n",
       "      <td>9790</td>\n",
       "      <td>sao bernardo do campo</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4e7b3e00288586ebd08712fdd0374a03</td>\n",
       "      <td>060e732b5b29e8181a18229c7b0b2b5e</td>\n",
       "      <td>1151</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b2b6027bc5c5109e529d4dc6358b12c3</td>\n",
       "      <td>259dac757896d24d7702b9acbbff3f3c</td>\n",
       "      <td>8775</td>\n",
       "      <td>mogi das cruzes</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4f2d8ab171c80ec8364f7c12e35b23ad</td>\n",
       "      <td>345ecd01c38d18a9036ed96c73b8d066</td>\n",
       "      <td>13056</td>\n",
       "      <td>campinas</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "0ATiGySQoP4w",
    "outputId": "1d53f2b0-4297-436b-f007-c232248a5782",
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:20.573808Z",
     "start_time": "2024-10-09T03:27:20.557251Z"
    }
   },
   "source": [
    "order_items_df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                           order_id  order_item_id  \\\n",
       "0  00010242fe8c5a6d1ba2dd792cb16214              1   \n",
       "1  00018f77f2f0320c557190d7a144bdd3              1   \n",
       "2  000229ec398224ef6ca0657da4fc703e              1   \n",
       "3  00024acbcdf0a6daa1e931b038114c75              1   \n",
       "4  00042b26cf59d7ce69dfabb4e55b4fd9              1   \n",
       "\n",
       "                         product_id                         seller_id  \\\n",
       "0  4244733e06e7ecb4970a6e2683c13e61  48436dade18ac8b2bce089ec2a041202   \n",
       "1  e5f2d52b802189ee658865ca93d83a8f  dd7ddc04e1b6c2c614352b383efe2d36   \n",
       "2  c777355d18b72b67abbeef9df44fd0fd  5b51032eddd242adc84c38acab88f23d   \n",
       "3  7634da152a4610f1595efa32f14722fc  9d7a1d34a5052409006425275ba1c2b4   \n",
       "4  ac6c3623068f30de03045865e4e10089  df560393f3a51e74553ab94004ba5c87   \n",
       "\n",
       "   shipping_limit_date   price  freight_value  \n",
       "0  2017-09-19 09:45:35   58.90          13.29  \n",
       "1  2017-05-03 11:05:13  239.90          19.93  \n",
       "2  2018-01-18 14:48:30  199.00          17.87  \n",
       "3  2018-08-15 10:10:18   12.99          12.79  \n",
       "4  2017-02-13 13:57:51  199.90          18.14  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>shipping_limit_date</th>\n",
       "      <th>price</th>\n",
       "      <th>freight_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00010242fe8c5a6d1ba2dd792cb16214</td>\n",
       "      <td>1</td>\n",
       "      <td>4244733e06e7ecb4970a6e2683c13e61</td>\n",
       "      <td>48436dade18ac8b2bce089ec2a041202</td>\n",
       "      <td>2017-09-19 09:45:35</td>\n",
       "      <td>58.90</td>\n",
       "      <td>13.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00018f77f2f0320c557190d7a144bdd3</td>\n",
       "      <td>1</td>\n",
       "      <td>e5f2d52b802189ee658865ca93d83a8f</td>\n",
       "      <td>dd7ddc04e1b6c2c614352b383efe2d36</td>\n",
       "      <td>2017-05-03 11:05:13</td>\n",
       "      <td>239.90</td>\n",
       "      <td>19.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000229ec398224ef6ca0657da4fc703e</td>\n",
       "      <td>1</td>\n",
       "      <td>c777355d18b72b67abbeef9df44fd0fd</td>\n",
       "      <td>5b51032eddd242adc84c38acab88f23d</td>\n",
       "      <td>2018-01-18 14:48:30</td>\n",
       "      <td>199.00</td>\n",
       "      <td>17.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00024acbcdf0a6daa1e931b038114c75</td>\n",
       "      <td>1</td>\n",
       "      <td>7634da152a4610f1595efa32f14722fc</td>\n",
       "      <td>9d7a1d34a5052409006425275ba1c2b4</td>\n",
       "      <td>2018-08-15 10:10:18</td>\n",
       "      <td>12.99</td>\n",
       "      <td>12.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00042b26cf59d7ce69dfabb4e55b4fd9</td>\n",
       "      <td>1</td>\n",
       "      <td>ac6c3623068f30de03045865e4e10089</td>\n",
       "      <td>df560393f3a51e74553ab94004ba5c87</td>\n",
       "      <td>2017-02-13 13:57:51</td>\n",
       "      <td>199.90</td>\n",
       "      <td>18.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "RxmEcHjRoPwR",
    "outputId": "b95a2caf-19dd-4ff6-ba19-5b89e28078cc",
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:20.746931Z",
     "start_time": "2024-10-09T03:27:20.739178Z"
    }
   },
   "source": [
    "order_payments_df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                           order_id  payment_sequential payment_type  \\\n",
       "0  b81ef226f3fe1789b1e8b2acac839d17                   1  credit_card   \n",
       "1  a9810da82917af2d9aefd1278f1dcfa0                   1  credit_card   \n",
       "2  25e8ea4e93396b6fa0d3dd708e76c1bd                   1  credit_card   \n",
       "3  ba78997921bbcdc1373bb41e913ab953                   1  credit_card   \n",
       "4  42fdf880ba16b47b59251dd489d4441a                   1  credit_card   \n",
       "\n",
       "   payment_installments  payment_value  \n",
       "0                     8          99.33  \n",
       "1                     1          24.39  \n",
       "2                     1          65.71  \n",
       "3                     8         107.78  \n",
       "4                     2         128.45  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>payment_sequential</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>payment_installments</th>\n",
       "      <th>payment_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b81ef226f3fe1789b1e8b2acac839d17</td>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>8</td>\n",
       "      <td>99.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a9810da82917af2d9aefd1278f1dcfa0</td>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>1</td>\n",
       "      <td>24.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25e8ea4e93396b6fa0d3dd708e76c1bd</td>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>1</td>\n",
       "      <td>65.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ba78997921bbcdc1373bb41e913ab953</td>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>8</td>\n",
       "      <td>107.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42fdf880ba16b47b59251dd489d4441a</td>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>2</td>\n",
       "      <td>128.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "id": "3p6g6V0C5J2V",
    "outputId": "2732d716-51c2-4f3d-aff2-b4e0eea7402e",
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:20.965937Z",
     "start_time": "2024-10-09T03:27:20.958211Z"
    }
   },
   "source": [
    "order_reviews_df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                          review_id                          order_id  \\\n",
       "0  7bc2406110b926393aa56f80a40eba40  73fc7af87114b39712e6da79b0a377eb   \n",
       "1  80e641a11e56f04c1ad469d5645fdfde  a548910a1c6147796b98fdf73dbeba33   \n",
       "2  228ce5500dc1d8e020d8d1322874b6f0  f9e4b658b201a9f2ecdecbb34bed034b   \n",
       "3  e64fb393e7b32834bb789ff8bb30750e  658677c97b385a9be170737859d3511b   \n",
       "4  f7c4243c7fe1938f181bec41a392bdeb  8e6bfb81e283fa7e4f11123a3fb894f1   \n",
       "\n",
       "   review_score review_comment_title  \\\n",
       "0             4                  NaN   \n",
       "1             5                  NaN   \n",
       "2             5                  NaN   \n",
       "3             5                  NaN   \n",
       "4             5                  NaN   \n",
       "\n",
       "                              review_comment_message review_creation_date  \\\n",
       "0                                                NaN  2018-01-18 00:00:00   \n",
       "1                                                NaN  2018-03-10 00:00:00   \n",
       "2                                                NaN  2018-02-17 00:00:00   \n",
       "3              Recebi bem antes do prazo estipulado.  2017-04-21 00:00:00   \n",
       "4  Parabéns lojas lannister adorei comprar pela I...  2018-03-01 00:00:00   \n",
       "\n",
       "  review_answer_timestamp  \n",
       "0     2018-01-18 21:46:59  \n",
       "1     2018-03-11 03:05:13  \n",
       "2     2018-02-18 14:36:24  \n",
       "3     2017-04-21 22:02:06  \n",
       "4     2018-03-02 10:26:53  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_comment_title</th>\n",
       "      <th>review_comment_message</th>\n",
       "      <th>review_creation_date</th>\n",
       "      <th>review_answer_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7bc2406110b926393aa56f80a40eba40</td>\n",
       "      <td>73fc7af87114b39712e6da79b0a377eb</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-18 00:00:00</td>\n",
       "      <td>2018-01-18 21:46:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80e641a11e56f04c1ad469d5645fdfde</td>\n",
       "      <td>a548910a1c6147796b98fdf73dbeba33</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-03-10 00:00:00</td>\n",
       "      <td>2018-03-11 03:05:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>228ce5500dc1d8e020d8d1322874b6f0</td>\n",
       "      <td>f9e4b658b201a9f2ecdecbb34bed034b</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-02-17 00:00:00</td>\n",
       "      <td>2018-02-18 14:36:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e64fb393e7b32834bb789ff8bb30750e</td>\n",
       "      <td>658677c97b385a9be170737859d3511b</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Recebi bem antes do prazo estipulado.</td>\n",
       "      <td>2017-04-21 00:00:00</td>\n",
       "      <td>2017-04-21 22:02:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f7c4243c7fe1938f181bec41a392bdeb</td>\n",
       "      <td>8e6bfb81e283fa7e4f11123a3fb894f1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parabéns lojas lannister adorei comprar pela I...</td>\n",
       "      <td>2018-03-01 00:00:00</td>\n",
       "      <td>2018-03-02 10:26:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "nmOFoku25JzK",
    "outputId": "2502bd23-ae9d-4ebf-cb91-3ad2605a4148",
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:21.139116Z",
     "start_time": "2024-10-09T03:27:21.132600Z"
    }
   },
   "source": [
    "olist_orders_df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                           order_id                       customer_id  \\\n",
       "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "1  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
       "2  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
       "3  949d5b44dbf5de918fe9c16f97b45f8a  f88197465ea7920adcdbec7375364d82   \n",
       "4  ad21c59c0840e6cb83a9ceb5573f8159  8ab97904e6daea8866dbdbc4fb7aad2c   \n",
       "\n",
       "  order_status order_purchase_timestamp    order_approved_at  \\\n",
       "0    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
       "1    delivered      2018-07-24 20:41:37  2018-07-26 03:24:27   \n",
       "2    delivered      2018-08-08 08:38:49  2018-08-08 08:55:23   \n",
       "3    delivered      2017-11-18 19:28:06  2017-11-18 19:45:59   \n",
       "4    delivered      2018-02-13 21:18:39  2018-02-13 22:20:29   \n",
       "\n",
       "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
       "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "1          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
       "2          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
       "3          2017-11-22 13:39:59           2017-12-02 00:28:42   \n",
       "4          2018-02-14 19:46:34           2018-02-16 18:17:02   \n",
       "\n",
       "  order_estimated_delivery_date  \n",
       "0           2017-10-18 00:00:00  \n",
       "1           2018-08-13 00:00:00  \n",
       "2           2018-09-04 00:00:00  \n",
       "3           2017-12-15 00:00:00  \n",
       "4           2018-02-26 00:00:00  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>order_approved_at</th>\n",
       "      <th>order_delivered_carrier_date</th>\n",
       "      <th>order_delivered_customer_date</th>\n",
       "      <th>order_estimated_delivery_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53cdb2fc8bc7dce0b6741e2150273451</td>\n",
       "      <td>b0830fb4747a6c6d20dea0b8c802d7ef</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-07-24 20:41:37</td>\n",
       "      <td>2018-07-26 03:24:27</td>\n",
       "      <td>2018-07-26 14:31:00</td>\n",
       "      <td>2018-08-07 15:27:45</td>\n",
       "      <td>2018-08-13 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47770eb9100c2d0c44946d9cf07ec65d</td>\n",
       "      <td>41ce2a54c0b03bf3443c3d931a367089</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-08-08 08:38:49</td>\n",
       "      <td>2018-08-08 08:55:23</td>\n",
       "      <td>2018-08-08 13:50:00</td>\n",
       "      <td>2018-08-17 18:06:29</td>\n",
       "      <td>2018-09-04 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>949d5b44dbf5de918fe9c16f97b45f8a</td>\n",
       "      <td>f88197465ea7920adcdbec7375364d82</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-11-18 19:28:06</td>\n",
       "      <td>2017-11-18 19:45:59</td>\n",
       "      <td>2017-11-22 13:39:59</td>\n",
       "      <td>2017-12-02 00:28:42</td>\n",
       "      <td>2017-12-15 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ad21c59c0840e6cb83a9ceb5573f8159</td>\n",
       "      <td>8ab97904e6daea8866dbdbc4fb7aad2c</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-02-13 21:18:39</td>\n",
       "      <td>2018-02-13 22:20:29</td>\n",
       "      <td>2018-02-14 19:46:34</td>\n",
       "      <td>2018-02-16 18:17:02</td>\n",
       "      <td>2018-02-26 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "hg9LVjUr5Jwh",
    "outputId": "68cdd701-69de-42ff-ccd4-15886d89d923",
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:21.351899Z",
     "start_time": "2024-10-09T03:27:21.344379Z"
    }
   },
   "source": [
    "sellers_df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                          seller_id  seller_zip_code_prefix  \\\n",
       "0  3442f8959a84dea7ee197c632cb2df15                   13023   \n",
       "1  d1b65fc7debc3361ea86b5f14c68d2e2                   13844   \n",
       "2  ce3ad9de960102d0677a81f5d0bb7b2d                   20031   \n",
       "3  c0f3eea2e14555b6faeea3dd58c1b1c3                    4195   \n",
       "4  51a04a8a6bdcb23deccc82b0b80742cf                   12914   \n",
       "\n",
       "         seller_city seller_state  \n",
       "0           campinas           SP  \n",
       "1         mogi guacu           SP  \n",
       "2     rio de janeiro           RJ  \n",
       "3          sao paulo           SP  \n",
       "4  braganca paulista           SP  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seller_id</th>\n",
       "      <th>seller_zip_code_prefix</th>\n",
       "      <th>seller_city</th>\n",
       "      <th>seller_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3442f8959a84dea7ee197c632cb2df15</td>\n",
       "      <td>13023</td>\n",
       "      <td>campinas</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d1b65fc7debc3361ea86b5f14c68d2e2</td>\n",
       "      <td>13844</td>\n",
       "      <td>mogi guacu</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ce3ad9de960102d0677a81f5d0bb7b2d</td>\n",
       "      <td>20031</td>\n",
       "      <td>rio de janeiro</td>\n",
       "      <td>RJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c0f3eea2e14555b6faeea3dd58c1b1c3</td>\n",
       "      <td>4195</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51a04a8a6bdcb23deccc82b0b80742cf</td>\n",
       "      <td>12914</td>\n",
       "      <td>braganca paulista</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "gAKZCAPj5OGk",
    "outputId": "083c69de-6c5a-4e09-fd19-fc7e9f6d1955",
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:21.530420Z",
     "start_time": "2024-10-09T03:27:21.523412Z"
    }
   },
   "source": [
    "products_df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                         product_id  product_category_name  \\\n",
       "0  1e9e8ef04dbcff4541ed26657ea517e5             perfumaria   \n",
       "1  3aa071139cb16b67ca9e5dea641aaa2f                  artes   \n",
       "2  96bd76ec8810374ed1b65e291975717f          esporte_lazer   \n",
       "3  cef67bcfe19066a932b7673e239eb23d                  bebes   \n",
       "4  9dc1a7de274444849c219cff195d0b71  utilidades_domesticas   \n",
       "\n",
       "   product_name_lenght  product_description_lenght  product_photos_qty  \\\n",
       "0                 40.0                       287.0                 1.0   \n",
       "1                 44.0                       276.0                 1.0   \n",
       "2                 46.0                       250.0                 1.0   \n",
       "3                 27.0                       261.0                 1.0   \n",
       "4                 37.0                       402.0                 4.0   \n",
       "\n",
       "   product_weight_g  product_length_cm  product_height_cm  product_width_cm  \n",
       "0             225.0               16.0               10.0              14.0  \n",
       "1            1000.0               30.0               18.0              20.0  \n",
       "2             154.0               18.0                9.0              15.0  \n",
       "3             371.0               26.0                4.0              26.0  \n",
       "4             625.0               20.0               17.0              13.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_category_name</th>\n",
       "      <th>product_name_lenght</th>\n",
       "      <th>product_description_lenght</th>\n",
       "      <th>product_photos_qty</th>\n",
       "      <th>product_weight_g</th>\n",
       "      <th>product_length_cm</th>\n",
       "      <th>product_height_cm</th>\n",
       "      <th>product_width_cm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1e9e8ef04dbcff4541ed26657ea517e5</td>\n",
       "      <td>perfumaria</td>\n",
       "      <td>40.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3aa071139cb16b67ca9e5dea641aaa2f</td>\n",
       "      <td>artes</td>\n",
       "      <td>44.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96bd76ec8810374ed1b65e291975717f</td>\n",
       "      <td>esporte_lazer</td>\n",
       "      <td>46.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cef67bcfe19066a932b7673e239eb23d</td>\n",
       "      <td>bebes</td>\n",
       "      <td>27.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9dc1a7de274444849c219cff195d0b71</td>\n",
       "      <td>utilidades_domesticas</td>\n",
       "      <td>37.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>625.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "tpBlW2FK5ODg",
    "outputId": "38750f33-5de0-4ed5-95c0-8d851d312628",
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:21.726187Z",
     "start_time": "2024-10-09T03:27:21.719675Z"
    }
   },
   "source": [
    "product_category_name_translation_df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    product_category_name product_category_name_english\n",
       "0            beleza_saude                 health_beauty\n",
       "1  informatica_acessorios         computers_accessories\n",
       "2              automotivo                          auto\n",
       "3         cama_mesa_banho                bed_bath_table\n",
       "4        moveis_decoracao               furniture_decor"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_category_name</th>\n",
       "      <th>product_category_name_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beleza_saude</td>\n",
       "      <td>health_beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>informatica_acessorios</td>\n",
       "      <td>computers_accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>automotivo</td>\n",
       "      <td>auto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cama_mesa_banho</td>\n",
       "      <td>bed_bath_table</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>moveis_decoracao</td>\n",
       "      <td>furniture_decor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDrTdi375o-v"
   },
   "source": [
    "#### Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QRf0a1Wm5OAk",
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:24.280745Z",
     "start_time": "2024-10-09T03:27:21.889797Z"
    }
   },
   "source": [
    "# marging datasets\n",
    "merged_1 = pd.merge(olist_orders_df, olist_customers_df, on='customer_id', how='left')\n",
    "merged_2 = pd.merge(sellers_df, order_items_df, on='seller_id', how='left')\n",
    "merged_3 = pd.merge(products_df, product_category_name_translation_df, on = 'product_category_name', how = 'left')\n",
    "merged_4 = pd.merge(merged_2, merged_3, on='product_id', how='left')\n",
    "merged_5 = pd.merge(merged_1, merged_4, on='order_id', how='left')\n",
    "merged_6 = pd.merge(merged_5, order_payments_df, on='order_id', how='left')\n",
    "merged = pd.merge(merged_6, order_reviews_df, on='order_id', how='left')\n",
    "merged.to_csv('./data/merged_data.csv', index=False)\n",
    "merged.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 119143 entries, 0 to 119142\n",
      "Data columns (total 40 columns):\n",
      " #   Column                         Non-Null Count   Dtype  \n",
      "---  ------                         --------------   -----  \n",
      " 0   order_id                       119143 non-null  object \n",
      " 1   customer_id                    119143 non-null  object \n",
      " 2   order_status                   119143 non-null  object \n",
      " 3   order_purchase_timestamp       119143 non-null  object \n",
      " 4   order_approved_at              118966 non-null  object \n",
      " 5   order_delivered_carrier_date   117057 non-null  object \n",
      " 6   order_delivered_customer_date  115722 non-null  object \n",
      " 7   order_estimated_delivery_date  119143 non-null  object \n",
      " 8   customer_unique_id             119143 non-null  object \n",
      " 9   customer_zip_code_prefix       119143 non-null  int64  \n",
      " 10  customer_city                  119143 non-null  object \n",
      " 11  customer_state                 119143 non-null  object \n",
      " 12  seller_id                      118310 non-null  object \n",
      " 13  seller_zip_code_prefix         118310 non-null  float64\n",
      " 14  seller_city                    118310 non-null  object \n",
      " 15  seller_state                   118310 non-null  object \n",
      " 16  order_item_id                  118310 non-null  float64\n",
      " 17  product_id                     118310 non-null  object \n",
      " 18  shipping_limit_date            118310 non-null  object \n",
      " 19  price                          118310 non-null  float64\n",
      " 20  freight_value                  118310 non-null  float64\n",
      " 21  product_category_name          116601 non-null  object \n",
      " 22  product_name_lenght            116601 non-null  float64\n",
      " 23  product_description_lenght     116601 non-null  float64\n",
      " 24  product_photos_qty             116601 non-null  float64\n",
      " 25  product_weight_g               118290 non-null  float64\n",
      " 26  product_length_cm              118290 non-null  float64\n",
      " 27  product_height_cm              118290 non-null  float64\n",
      " 28  product_width_cm               118290 non-null  float64\n",
      " 29  product_category_name_english  116576 non-null  object \n",
      " 30  payment_sequential             119140 non-null  float64\n",
      " 31  payment_type                   119140 non-null  object \n",
      " 32  payment_installments           119140 non-null  float64\n",
      " 33  payment_value                  119140 non-null  float64\n",
      " 34  review_id                      118146 non-null  object \n",
      " 35  review_score                   118146 non-null  float64\n",
      " 36  review_comment_title           13989 non-null   object \n",
      " 37  review_comment_message         50245 non-null   object \n",
      " 38  review_creation_date           118146 non-null  object \n",
      " 39  review_answer_timestamp        118146 non-null  object \n",
      "dtypes: float64(15), int64(1), object(24)\n",
      "memory usage: 36.4+ MB\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ms4Ryf8ZMOEP",
    "outputId": "746c13c1-48a9-42ce-9d4d-6f50d68f698c",
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:24.501861Z",
     "start_time": "2024-10-09T03:27:24.409266Z"
    }
   },
   "source": [
    "merged.isnull().sum()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_id                              0\n",
       "customer_id                           0\n",
       "order_status                          0\n",
       "order_purchase_timestamp              0\n",
       "order_approved_at                   177\n",
       "order_delivered_carrier_date       2086\n",
       "order_delivered_customer_date      3421\n",
       "order_estimated_delivery_date         0\n",
       "customer_unique_id                    0\n",
       "customer_zip_code_prefix              0\n",
       "customer_city                         0\n",
       "customer_state                        0\n",
       "seller_id                           833\n",
       "seller_zip_code_prefix              833\n",
       "seller_city                         833\n",
       "seller_state                        833\n",
       "order_item_id                       833\n",
       "product_id                          833\n",
       "shipping_limit_date                 833\n",
       "price                               833\n",
       "freight_value                       833\n",
       "product_category_name              2542\n",
       "product_name_lenght                2542\n",
       "product_description_lenght         2542\n",
       "product_photos_qty                 2542\n",
       "product_weight_g                    853\n",
       "product_length_cm                   853\n",
       "product_height_cm                   853\n",
       "product_width_cm                    853\n",
       "product_category_name_english      2567\n",
       "payment_sequential                    3\n",
       "payment_type                          3\n",
       "payment_installments                  3\n",
       "payment_value                         3\n",
       "review_id                           997\n",
       "review_score                        997\n",
       "review_comment_title             105154\n",
       "review_comment_message            68898\n",
       "review_creation_date                997\n",
       "review_answer_timestamp             997\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rPiMXRJQMrnm",
    "outputId": "f1f74700-9df0-4580-bd61-537bc2ba7493",
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:24.748197Z",
     "start_time": "2024-10-09T03:27:24.525883Z"
    }
   },
   "source": [
    "merged = merged.drop(columns=['review_comment_title', 'review_comment_message'], axis=1)\n",
    "merged = merged.dropna()\n",
    "merged.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 113193 entries, 0 to 119142\n",
      "Data columns (total 38 columns):\n",
      " #   Column                         Non-Null Count   Dtype  \n",
      "---  ------                         --------------   -----  \n",
      " 0   order_id                       113193 non-null  object \n",
      " 1   customer_id                    113193 non-null  object \n",
      " 2   order_status                   113193 non-null  object \n",
      " 3   order_purchase_timestamp       113193 non-null  object \n",
      " 4   order_approved_at              113193 non-null  object \n",
      " 5   order_delivered_carrier_date   113193 non-null  object \n",
      " 6   order_delivered_customer_date  113193 non-null  object \n",
      " 7   order_estimated_delivery_date  113193 non-null  object \n",
      " 8   customer_unique_id             113193 non-null  object \n",
      " 9   customer_zip_code_prefix       113193 non-null  int64  \n",
      " 10  customer_city                  113193 non-null  object \n",
      " 11  customer_state                 113193 non-null  object \n",
      " 12  seller_id                      113193 non-null  object \n",
      " 13  seller_zip_code_prefix         113193 non-null  float64\n",
      " 14  seller_city                    113193 non-null  object \n",
      " 15  seller_state                   113193 non-null  object \n",
      " 16  order_item_id                  113193 non-null  float64\n",
      " 17  product_id                     113193 non-null  object \n",
      " 18  shipping_limit_date            113193 non-null  object \n",
      " 19  price                          113193 non-null  float64\n",
      " 20  freight_value                  113193 non-null  float64\n",
      " 21  product_category_name          113193 non-null  object \n",
      " 22  product_name_lenght            113193 non-null  float64\n",
      " 23  product_description_lenght     113193 non-null  float64\n",
      " 24  product_photos_qty             113193 non-null  float64\n",
      " 25  product_weight_g               113193 non-null  float64\n",
      " 26  product_length_cm              113193 non-null  float64\n",
      " 27  product_height_cm              113193 non-null  float64\n",
      " 28  product_width_cm               113193 non-null  float64\n",
      " 29  product_category_name_english  113193 non-null  object \n",
      " 30  payment_sequential             113193 non-null  float64\n",
      " 31  payment_type                   113193 non-null  object \n",
      " 32  payment_installments           113193 non-null  float64\n",
      " 33  payment_value                  113193 non-null  float64\n",
      " 34  review_id                      113193 non-null  object \n",
      " 35  review_score                   113193 non-null  float64\n",
      " 36  review_creation_date           113193 non-null  object \n",
      " 37  review_answer_timestamp        113193 non-null  object \n",
      "dtypes: float64(15), int64(1), object(22)\n",
      "memory usage: 33.7+ MB\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D47_wuPheJtH"
   },
   "source": [
    "#### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wudCw2gBObuA",
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:25.141532Z",
     "start_time": "2024-10-09T03:27:24.785885Z"
    }
   },
   "source": [
    "datetime = ['order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date',\n",
    "                    'order_delivered_customer_date', 'order_estimated_delivery_date',\n",
    "                    'shipping_limit_date', 'review_creation_date', 'review_answer_timestamp']\n",
    "\n",
    "for col in datetime:\n",
    "    merged[col] = pd.to_datetime(merged[col])\n",
    "\n",
    "# Extract components of dates\n",
    "merged['purchase_day'] = merged['order_purchase_timestamp'].dt.weekday # Monday=0, Sunday=6\n",
    "merged['purchase_month'] = merged['order_purchase_timestamp'].dt.month\n",
    "\n",
    "# The total time from the purchase of the order to the actual delivery to the customer\n",
    "merged['total_delivery_time'] = (merged['order_delivered_customer_date'] - merged['order_purchase_timestamp']).dt.days\n",
    "\n",
    "# Time from order creation to payment approval\n",
    "merged['approval_time'] = (merged['order_approved_at'] - merged['order_purchase_timestamp']).dt.days\n",
    "\n",
    "# Time from logistics to customer delivery\n",
    "merged['shipping_time'] = (merged['order_delivered_customer_date'] - merged['order_delivered_carrier_date']).dt.days\n",
    "\n",
    "# Deviation between actual delivery time and estimated delivery time\n",
    "merged['delivery_estimate_deviation'] = (merged['order_estimated_delivery_date'] - merged['order_delivered_customer_date']).dt.days\n",
    "\n",
    "# Late delivery\n",
    "merged['late_delivery'] = (merged['order_delivered_customer_date'] > merged['order_estimated_delivery_date']).astype(int)\n",
    "\n",
    "# Total order item values and the total freight value\n",
    "merged['total_order_item_value'] = merged['order_item_id'] * merged['price']\n",
    "merged['total_freight_value'] = merged['order_item_id'] * merged['freight_value']\n",
    "\n",
    "# Total order freight value\n",
    "merged['total_order_freight_value'] = merged['total_order_item_value'] + merged['total_freight_value']\n"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6MpHbqQrHue",
    "outputId": "1bd57389-484a-42cd-a1c4-a0c6b10106f3",
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:25.178830Z",
     "start_time": "2024-10-09T03:27:25.166319Z"
    }
   },
   "source": [
    "merged.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                           order_id                       customer_id  \\\n",
       "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "1  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "2  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "3  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
       "4  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
       "\n",
       "  order_status order_purchase_timestamp   order_approved_at  \\\n",
       "0    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15   \n",
       "1    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15   \n",
       "2    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15   \n",
       "3    delivered      2018-07-24 20:41:37 2018-07-26 03:24:27   \n",
       "4    delivered      2018-08-08 08:38:49 2018-08-08 08:55:23   \n",
       "\n",
       "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
       "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "1          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "2          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "3          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
       "4          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
       "\n",
       "  order_estimated_delivery_date                customer_unique_id  \\\n",
       "0                    2017-10-18  7c396fd4830fd04220f754e42b4e5bff   \n",
       "1                    2017-10-18  7c396fd4830fd04220f754e42b4e5bff   \n",
       "2                    2017-10-18  7c396fd4830fd04220f754e42b4e5bff   \n",
       "3                    2018-08-13  af07308b275d755c9edb36a90c618231   \n",
       "4                    2018-09-04  3a653a41f6f9fc3d2a113cf8398680e8   \n",
       "\n",
       "   customer_zip_code_prefix  ... purchase_day purchase_month  \\\n",
       "0                      3149  ...            0             10   \n",
       "1                      3149  ...            0             10   \n",
       "2                      3149  ...            0             10   \n",
       "3                     47813  ...            1              7   \n",
       "4                     75265  ...            2              8   \n",
       "\n",
       "  total_delivery_time  approval_time shipping_time  \\\n",
       "0                   8              0             6   \n",
       "1                   8              0             6   \n",
       "2                   8              0             6   \n",
       "3                  13              1            12   \n",
       "4                   9              0             9   \n",
       "\n",
       "  delivery_estimate_deviation  late_delivery total_order_item_value  \\\n",
       "0                           7              0                  29.99   \n",
       "1                           7              0                  29.99   \n",
       "2                           7              0                  29.99   \n",
       "3                           5              0                 118.70   \n",
       "4                          17              0                 159.90   \n",
       "\n",
       "  total_freight_value  total_order_freight_value  \n",
       "0                8.72                      38.71  \n",
       "1                8.72                      38.71  \n",
       "2                8.72                      38.71  \n",
       "3               22.76                     141.46  \n",
       "4               19.22                     179.12  \n",
       "\n",
       "[5 rows x 48 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>order_approved_at</th>\n",
       "      <th>order_delivered_carrier_date</th>\n",
       "      <th>order_delivered_customer_date</th>\n",
       "      <th>order_estimated_delivery_date</th>\n",
       "      <th>customer_unique_id</th>\n",
       "      <th>customer_zip_code_prefix</th>\n",
       "      <th>...</th>\n",
       "      <th>purchase_day</th>\n",
       "      <th>purchase_month</th>\n",
       "      <th>total_delivery_time</th>\n",
       "      <th>approval_time</th>\n",
       "      <th>shipping_time</th>\n",
       "      <th>delivery_estimate_deviation</th>\n",
       "      <th>late_delivery</th>\n",
       "      <th>total_order_item_value</th>\n",
       "      <th>total_freight_value</th>\n",
       "      <th>total_order_freight_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>38.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>38.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>38.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53cdb2fc8bc7dce0b6741e2150273451</td>\n",
       "      <td>b0830fb4747a6c6d20dea0b8c802d7ef</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-07-24 20:41:37</td>\n",
       "      <td>2018-07-26 03:24:27</td>\n",
       "      <td>2018-07-26 14:31:00</td>\n",
       "      <td>2018-08-07 15:27:45</td>\n",
       "      <td>2018-08-13</td>\n",
       "      <td>af07308b275d755c9edb36a90c618231</td>\n",
       "      <td>47813</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>118.70</td>\n",
       "      <td>22.76</td>\n",
       "      <td>141.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47770eb9100c2d0c44946d9cf07ec65d</td>\n",
       "      <td>41ce2a54c0b03bf3443c3d931a367089</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-08-08 08:38:49</td>\n",
       "      <td>2018-08-08 08:55:23</td>\n",
       "      <td>2018-08-08 13:50:00</td>\n",
       "      <td>2018-08-17 18:06:29</td>\n",
       "      <td>2018-09-04</td>\n",
       "      <td>3a653a41f6f9fc3d2a113cf8398680e8</td>\n",
       "      <td>75265</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>159.90</td>\n",
       "      <td>19.22</td>\n",
       "      <td>179.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uXcVDyEouB1b",
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:25.275970Z",
     "start_time": "2024-10-09T03:27:25.245913Z"
    }
   },
   "source": [
    "# Labeling\n",
    "category_to_label = {\n",
    "    'cama_mesa_banho': 0,\n",
    "    'esporte_lazer': 1,\n",
    "    'other': 2\n",
    "}\n",
    "merged['product_category_name'] = merged['product_category_name'].apply(lambda x: x if x in category_to_label else 'other')\n",
    "merged['product_category_name'] = merged['product_category_name'].map(category_to_label)\n",
    "\n",
    "\n",
    "name_to_label = {\n",
    "    'bed_bath_table': 0,\n",
    "    'sports_leisure': 1,\n",
    "    'other': 2\n",
    "}\n",
    "merged['product_category_name_english'] = merged['product_category_name_english'].apply(lambda x: x if x in name_to_label else 'other')\n",
    "merged['product_category_name_english'] = merged['product_category_name_english'].map(name_to_label)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4KeODBwk1Tw_",
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:25.342592Z",
     "start_time": "2024-10-09T03:27:25.323561Z"
    }
   },
   "source": [
    "category_to_label = {\n",
    "    'credit_card': 0,\n",
    "    'boleto': 1,\n",
    "    'other': 2\n",
    "}\n",
    "merged['payment_type'] = merged['payment_type'].apply(lambda x: x if x in category_to_label else 'other')\n",
    "merged['payment_type'] = merged['payment_type'].map(category_to_label)\n"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "mprYe1TUqiRz",
    "outputId": "17b26111-2f57-4ace-e57d-33ef49184f2b",
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:25.409749Z",
     "start_time": "2024-10-09T03:27:25.401748Z"
    }
   },
   "source": [
    "merged['review_score'].value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_score\n",
       "5.0    65145\n",
       "4.0    21811\n",
       "1.0    12865\n",
       "3.0     9533\n",
       "2.0     3839\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aoRUm2D4WV--",
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:26.602774Z",
     "start_time": "2024-10-09T03:27:25.632265Z"
    }
   },
   "source": [
    "# Remove unnecessary features\n",
    "merged = merged.select_dtypes(include=['int', 'float'])\n",
    "merged.drop(['customer_zip_code_prefix', 'seller_zip_code_prefix', 'order_item_id', 'product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'payment_sequential']\n",
    "               , axis=1, inplace=True)\n",
    "# save the cleaned dataset\n",
    "merged.to_csv('./data/merged_data_cleaned.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "AbgWgMH9fwGI",
    "outputId": "4c9248a5-c691-4f21-a9a5-2ee3db927fb6",
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:26.645857Z",
     "start_time": "2024-10-09T03:27:26.632831Z"
    }
   },
   "source": [
    "merged.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    price  freight_value  product_category_name  product_weight_g  \\\n",
       "0   29.99           8.72                      2             500.0   \n",
       "1   29.99           8.72                      2             500.0   \n",
       "2   29.99           8.72                      2             500.0   \n",
       "3  118.70          22.76                      2             400.0   \n",
       "4  159.90          19.22                      2             420.0   \n",
       "\n",
       "   product_length_cm  product_height_cm  product_width_cm  \\\n",
       "0               19.0                8.0              13.0   \n",
       "1               19.0                8.0              13.0   \n",
       "2               19.0                8.0              13.0   \n",
       "3               19.0               13.0              19.0   \n",
       "4               24.0               19.0              21.0   \n",
       "\n",
       "   product_category_name_english  payment_type  payment_installments  ...  \\\n",
       "0                              2             0                   1.0  ...   \n",
       "1                              2             2                   1.0  ...   \n",
       "2                              2             2                   1.0  ...   \n",
       "3                              2             1                   1.0  ...   \n",
       "4                              2             0                   3.0  ...   \n",
       "\n",
       "   purchase_day  purchase_month  total_delivery_time  approval_time  \\\n",
       "0             0              10                    8              0   \n",
       "1             0              10                    8              0   \n",
       "2             0              10                    8              0   \n",
       "3             1               7                   13              1   \n",
       "4             2               8                    9              0   \n",
       "\n",
       "   shipping_time  delivery_estimate_deviation  late_delivery  \\\n",
       "0              6                            7              0   \n",
       "1              6                            7              0   \n",
       "2              6                            7              0   \n",
       "3             12                            5              0   \n",
       "4              9                           17              0   \n",
       "\n",
       "   total_order_item_value  total_freight_value  total_order_freight_value  \n",
       "0                   29.99                 8.72                      38.71  \n",
       "1                   29.99                 8.72                      38.71  \n",
       "2                   29.99                 8.72                      38.71  \n",
       "3                  118.70                22.76                     141.46  \n",
       "4                  159.90                19.22                     179.12  \n",
       "\n",
       "[5 rows x 22 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>freight_value</th>\n",
       "      <th>product_category_name</th>\n",
       "      <th>product_weight_g</th>\n",
       "      <th>product_length_cm</th>\n",
       "      <th>product_height_cm</th>\n",
       "      <th>product_width_cm</th>\n",
       "      <th>product_category_name_english</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>payment_installments</th>\n",
       "      <th>...</th>\n",
       "      <th>purchase_day</th>\n",
       "      <th>purchase_month</th>\n",
       "      <th>total_delivery_time</th>\n",
       "      <th>approval_time</th>\n",
       "      <th>shipping_time</th>\n",
       "      <th>delivery_estimate_deviation</th>\n",
       "      <th>late_delivery</th>\n",
       "      <th>total_order_item_value</th>\n",
       "      <th>total_freight_value</th>\n",
       "      <th>total_order_freight_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>2</td>\n",
       "      <td>500.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>38.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>2</td>\n",
       "      <td>500.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>38.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>2</td>\n",
       "      <td>500.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>38.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>118.70</td>\n",
       "      <td>22.76</td>\n",
       "      <td>2</td>\n",
       "      <td>400.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>118.70</td>\n",
       "      <td>22.76</td>\n",
       "      <td>141.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>159.90</td>\n",
       "      <td>19.22</td>\n",
       "      <td>2</td>\n",
       "      <td>420.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>159.90</td>\n",
       "      <td>19.22</td>\n",
       "      <td>179.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:26.801202Z",
     "start_time": "2024-10-09T03:27:26.702810Z"
    }
   },
   "source": [
    "# extract the correlation matrix of review_score\n",
    "corr_matrix = merged.corr()\n",
    "corr_matrix['review_score'].sort_values(ascending=False)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_score                     1.000000\n",
       "delivery_estimate_deviation      0.228154\n",
       "product_category_name_english    0.032753\n",
       "product_category_name            0.032753\n",
       "purchase_month                   0.028419\n",
       "price                            0.003203\n",
       "payment_type                     0.000278\n",
       "product_width_cm                -0.011835\n",
       "purchase_day                    -0.013958\n",
       "approval_time                   -0.018635\n",
       "product_length_cm               -0.020080\n",
       "product_height_cm               -0.023240\n",
       "product_weight_g                -0.026894\n",
       "freight_value                   -0.033637\n",
       "total_order_item_value          -0.038652\n",
       "payment_installments            -0.043283\n",
       "total_order_freight_value       -0.047604\n",
       "payment_value                   -0.081312\n",
       "total_freight_value             -0.098066\n",
       "shipping_time                   -0.267258\n",
       "total_delivery_time             -0.303545\n",
       "late_delivery                   -0.329049\n",
       "Name: review_score, dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:26.852025Z",
     "start_time": "2024-10-09T03:27:26.847909Z"
    }
   },
   "source": [
    "# Set the correlation threshold\n",
    "threshold = 0.05\n",
    "\n",
    "# Get the features with correlation greater than 5% or less than -5% with review_score\n",
    "high_corr_features = corr_matrix[(corr_matrix['review_score'].abs() > threshold) & (corr_matrix['review_score'].index != 'review_score')].index\n",
    "\n",
    "# Print the features with high correlation\n",
    "print(\"Features with high correlation with review_score:\")\n",
    "print(high_corr_features)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with high correlation with review_score:\n",
      "Index(['payment_value', 'total_delivery_time', 'shipping_time',\n",
      "       'delivery_estimate_deviation', 'late_delivery', 'total_freight_value'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:26.945096Z",
     "start_time": "2024-10-09T03:27:26.936073Z"
    }
   },
   "source": [
    "data=merged\n",
    "X = data.drop('review_score', axis=1)\n",
    "y = data['review_score']\n"
   ],
   "outputs": [],
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:28.113464Z",
     "start_time": "2024-10-09T03:27:27.014857Z"
    }
   },
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smt, y_smt = smote.fit_resample(X, y)\n",
    "\n",
    "\n",
    "scaling_factor = 0.2\n",
    "X_smt, y_smt = resample(X_smt, y_smt, replace=False, n_samples=int(len(X_smt) * scaling_factor), random_state=42)\n",
    "\n",
    "\n",
    "print(y_smt.value_counts())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_score\n",
      "1.0    13121\n",
      "2.0    13105\n",
      "5.0    13070\n",
      "4.0    12941\n",
      "3.0    12908\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:28.401532Z",
     "start_time": "2024-10-09T03:27:28.134395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "n_components = 10\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "X_pca_df = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "\n",
    "\n",
    "print(X_pca_df.head())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           PC1         PC2        PC3        PC4        PC5       PC6  \\\n",
      "0 -1614.451834 -175.761881 -15.581950   9.582007 -11.240671  9.545183   \n",
      "1 -1614.804328 -185.258519 -27.347666  15.160755 -11.117467  9.550348   \n",
      "2 -1614.441561 -175.485157 -15.239362   9.420771 -11.243962  9.544531   \n",
      "3 -1706.293476   31.225333 -17.862921  -5.500186   8.638355  7.096706   \n",
      "4 -1683.241733  107.738288 -31.746319 -11.872653   2.846366  3.774542   \n",
      "\n",
      "        PC7       PC8       PC9      PC10  \n",
      "0  0.777170 -6.486278 -3.183233  3.785414  \n",
      "1  0.758098 -6.458747 -3.206877  3.777090  \n",
      "2  0.779334 -6.487550 -3.183430  3.786863  \n",
      "3  6.649366 -1.372562  2.330340 -1.072980  \n",
      "4 -4.665561  5.456546  1.156105 -3.326500  \n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Validation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 cross validation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:28.594350Z",
     "start_time": "2024-10-09T03:27:28.588854Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "def manual_train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    np.random.seed(random_state) \n",
    "    indices = np.random.permutation(len(X))  \n",
    "    test_set_size = int(len(X) * test_size)  \n",
    "    test_indices = indices[:test_set_size]  \n",
    "    train_indices = indices[test_set_size:]  \n",
    "    \n",
    "\n",
    "    X_train = X.iloc[train_indices]\n",
    "    X_test = X.iloc[test_indices]\n",
    "    y_train = y.iloc[train_indices]\n",
    "    y_test = y.iloc[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 83
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 k fold validation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T03:27:29.010796Z",
     "start_time": "2024-10-09T03:27:28.996747Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "def manual_kfold(X, y, k=5, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(len(X))\n",
    "    fold_size = len(X) // k\n",
    "    folds = []\n",
    "\n",
    "    for i in range(k):\n",
    "        test_indices = indices[i*fold_size:(i+1)*fold_size]\n",
    "        train_indices = np.concatenate([indices[:i*fold_size], indices[(i+1)*fold_size:]])\n",
    "        folds.append((train_indices, test_indices))\n",
    "    \n",
    "    return folds\n",
    "\n",
    "\n",
    "k_folds = manual_kfold(X, y, k=5, random_state=42)\n",
    "\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(k_folds):\n",
    "    print(f\"Fold {i+1}: Train size = {len(train_idx)}, Test size = {len(test_idx)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Train size = 90555, Test size = 22638\n",
      "Fold 2: Train size = 90555, Test size = 22638\n",
      "Fold 3: Train size = 90555, Test size = 22638\n",
      "Fold 4: Train size = 90555, Test size = 22638\n",
      "Fold 5: Train size = 90555, Test size = 22638\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.ML algorithms and Experimental results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Simple algorithms: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:13:14.894643Z",
     "start_time": "2024-10-07T15:13:14.888131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_score\n",
      "4.0    65145\n",
      "5.0    65145\n",
      "1.0    65145\n",
      "2.0    65145\n",
      "3.0    65145\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T03:47:55.383689300Z",
     "start_time": "2024-10-09T03:47:51.460916Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "train_sizes = np.concatenate(([0.01], np.logspace(-1, 0, 3)))  # [0.01, 0.1, 0.5, 1.0]\n",
    "n_repeats = 20  \n",
    "accuracies = []\n",
    "train_sizes_actual = []\n",
    "all_y_test = []\n",
    "all_y_pred = []\n",
    "\n",
    "experiments_per_size = n_repeats // len(train_sizes)\n",
    "\n",
    "for size in train_sizes:\n",
    "    for i in range(experiments_per_size):\n",
    "        X_train, X_test, y_train, y_test = manual_train_test_split(X_smt, y_smt, test_size=0.2, random_state=i)\n",
    "       \n",
    "       \n",
    "        subset_train_idx = X_train.index[:int(len(X_train) * size)]\n",
    "        X_train_subset = X_train.loc[subset_train_idx]\n",
    "        y_train_subset = y_train.loc[subset_train_idx]\n",
    "        \n",
    "       \n",
    "        model = RandomForestClassifier(n_estimators=200, random_state=42, max_depth=None)\n",
    "        model.fit(X_train_subset, y_train_subset)\n",
    "        \n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        train_sizes_actual.append(len(X_train_subset))\n",
    "        \n",
    "        all_y_test.extend(y_test)\n",
    "        all_y_pred.extend(y_pred)\n",
    "        \n",
    "        print(f\"Experiment with Train Size {len(X_train_subset)}: Accuracy = {accuracy:.3f}\")\n",
    "\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_sizes_actual, accuracies, marker='o', label='Accuracy')\n",
    "plt.title(\"Learning Curve with Train Sizes\")\n",
    "plt.xlabel(\"Training Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(x=train_sizes_actual, y=accuracies, yerr=std_accuracy, fmt='o', label='Accuracy with error bar')\n",
    "plt.title(\"Learning Curve with Error Bars\")\n",
    "plt.xlabel(\"Training Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "conf_matrix = confusion_matrix(all_y_test, all_y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix for Original 5-Class Classification\")\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(all_y_test, all_y_pred))\n",
    "\n",
    "\n",
    "y_test_coarse = pd.Series(all_y_test).replace({1: 0, 2: 0, 4: 1, 5: 1})\n",
    "y_pred_coarse = pd.Series(all_y_pred).replace({1: 0, 2: 0, 4: 1, 5: 1})\n",
    "\n",
    "valid_indices = y_test_coarse != 3\n",
    "y_test_coarse = y_test_coarse[valid_indices]\n",
    "y_pred_coarse = y_pred_coarse[valid_indices]\n",
    "\n",
    "\n",
    "conf_matrix_coarse = confusion_matrix(y_test_coarse, y_pred_coarse)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_coarse, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Bad', 'Good'], yticklabels=['Bad', 'Good'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix for Coarse Classification (Good vs. Bad)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "accuracy_coarse = accuracy_score(y_test_coarse, y_pred_coarse)\n",
    "print(f\"Accuracy for Coarse Classification (Good vs. Bad): {accuracy_coarse:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment with Train Size 521: Accuracy = 0.294\n",
      "Experiment with Train Size 521: Accuracy = 0.296\n",
      "Experiment with Train Size 521: Accuracy = 0.302\n",
      "Experiment with Train Size 521: Accuracy = 0.284\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[86], line 27\u001B[0m\n\u001B[0;32m     23\u001B[0m y_train_subset \u001B[38;5;241m=\u001B[39m y_train\u001B[38;5;241m.\u001B[39mloc[subset_train_idx]\n\u001B[0;32m     26\u001B[0m model \u001B[38;5;241m=\u001B[39m RandomForestClassifier(n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m200\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, max_depth\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m---> 27\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_subset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_subset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     30\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[0;32m     31\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m accuracy_score(y_test, y_pred)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:1152\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1145\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1147\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1148\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1149\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1150\u001B[0m     )\n\u001B[0;32m   1151\u001B[0m ):\n\u001B[1;32m-> 1152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001B[0m, in \u001B[0;36mBaseForest.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    445\u001B[0m trees \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    446\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_estimator(append\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[0;32m    447\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_more_estimators)\n\u001B[0;32m    448\u001B[0m ]\n\u001B[0;32m    450\u001B[0m \u001B[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001B[39;00m\n\u001B[0;32m    451\u001B[0m \u001B[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001B[39;00m\n\u001B[0;32m    452\u001B[0m \u001B[38;5;66;03m# making threading more efficient than multiprocessing in\u001B[39;00m\n\u001B[0;32m    453\u001B[0m \u001B[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001B[39;00m\n\u001B[0;32m    454\u001B[0m \u001B[38;5;66;03m# parallel_backend contexts set at a higher level,\u001B[39;00m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;66;03m# since correctness does not rely on using threads.\u001B[39;00m\n\u001B[1;32m--> 456\u001B[0m trees \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    457\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    458\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    459\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprefer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mthreads\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    461\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_parallel_build_trees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    462\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    463\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbootstrap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    464\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    465\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    466\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    467\u001B[0m \u001B[43m        \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    468\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    469\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    470\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclass_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    471\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    472\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    473\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    474\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    476\u001B[0m \u001B[38;5;66;03m# Collect newly grown trees\u001B[39;00m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_\u001B[38;5;241m.\u001B[39mextend(trees)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     60\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     61\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     62\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     64\u001B[0m )\n\u001B[1;32m---> 65\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\joblib\\parallel.py:1918\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1916\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[0;32m   1917\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1920\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[0;32m   1921\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[0;32m   1922\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[0;32m   1923\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[0;32m   1924\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[0;32m   1925\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\joblib\\parallel.py:1847\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1845\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1846\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1847\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1848\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1849\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    125\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[1;32m--> 127\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001B[0m, in \u001B[0;36m_parallel_build_trees\u001B[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001B[0m\n\u001B[0;32m    185\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m class_weight \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced_subsample\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    186\u001B[0m         curr_sample_weight \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m compute_sample_weight(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m\"\u001B[39m, y, indices\u001B[38;5;241m=\u001B[39mindices)\n\u001B[1;32m--> 188\u001B[0m     \u001B[43mtree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurr_sample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    190\u001B[0m     tree\u001B[38;5;241m.\u001B[39mfit(X, y, sample_weight\u001B[38;5;241m=\u001B[39msample_weight, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:1152\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1145\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1147\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1148\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1149\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1150\u001B[0m     )\n\u001B[0;32m   1151\u001B[0m ):\n\u001B[1;32m-> 1152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001B[0m, in \u001B[0;36mDecisionTreeClassifier.fit\u001B[1;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[0;32m    928\u001B[0m \u001B[38;5;129m@_fit_context\u001B[39m(prefer_skip_nested_validation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    929\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m    930\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001B[39;00m\n\u001B[0;32m    931\u001B[0m \n\u001B[0;32m    932\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    956\u001B[0m \u001B[38;5;124;03m        Fitted estimator.\u001B[39;00m\n\u001B[0;32m    957\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 959\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    960\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    961\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    962\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    963\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheck_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    964\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    965\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\tree\\_classes.py:284\u001B[0m, in \u001B[0;36mBaseDecisionTree._fit\u001B[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001B[0m\n\u001B[0;32m    281\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_classification:\n\u001B[1;32m--> 284\u001B[0m     \u001B[43mcheck_classification_targets\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    285\u001B[0m     y \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mcopy(y)\n\u001B[0;32m    287\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses_ \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\multiclass.py:208\u001B[0m, in \u001B[0;36mcheck_classification_targets\u001B[1;34m(y)\u001B[0m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_classification_targets\u001B[39m(y):\n\u001B[0;32m    197\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Ensure that target y is of a non-regression type.\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \n\u001B[0;32m    199\u001B[0m \u001B[38;5;124;03m    Only the following target types (as defined in type_of_target) are allowed:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;124;03m        Target values.\u001B[39;00m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 208\u001B[0m     y_type \u001B[38;5;241m=\u001B[39m \u001B[43mtype_of_target\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43my\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m y_type \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m [\n\u001B[0;32m    210\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    211\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulticlass\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    214\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultilabel-sequences\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    215\u001B[0m     ]:\n\u001B[0;32m    216\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    217\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown label type: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00my_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Maybe you are trying to fit a \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    218\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclassifier, which expects discrete classes on a \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    219\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mregression target with continuous values.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    220\u001B[0m         )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\multiclass.py:388\u001B[0m, in \u001B[0;36mtype_of_target\u001B[1;34m(y, input_name)\u001B[0m\n\u001B[0;32m    386\u001B[0m \u001B[38;5;66;03m# Check multiclass\u001B[39;00m\n\u001B[0;32m    387\u001B[0m first_row \u001B[38;5;241m=\u001B[39m y[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m issparse(y) \u001B[38;5;28;01melse\u001B[39;00m y\u001B[38;5;241m.\u001B[39mgetrow(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mdata\n\u001B[1;32m--> 388\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mxp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munique_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m (y\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(first_row) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m    389\u001B[0m     \u001B[38;5;66;03m# [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\u001B[39;00m\n\u001B[0;32m    390\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulticlass\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m suffix\n\u001B[0;32m    391\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\_array_api.py:262\u001B[0m, in \u001B[0;36m_NumPyAPIWrapper.unique_values\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    261\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munique_values\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m--> 262\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnumpy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munique\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m<__array_function__ internals>:200\u001B[0m, in \u001B[0;36munique\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\lib\\arraysetops.py:274\u001B[0m, in \u001B[0;36munique\u001B[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001B[0m\n\u001B[0;32m    272\u001B[0m ar \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masanyarray(ar)\n\u001B[0;32m    273\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 274\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43m_unique1d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_inverse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_counts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m    275\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mequal_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mequal_nan\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    276\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _unpack_tuple(ret)\n\u001B[0;32m    278\u001B[0m \u001B[38;5;66;03m# axis was specified and not None\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\lib\\arraysetops.py:336\u001B[0m, in \u001B[0;36m_unique1d\u001B[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001B[0m\n\u001B[0;32m    334\u001B[0m     aux \u001B[38;5;241m=\u001B[39m ar[perm]\n\u001B[0;32m    335\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 336\u001B[0m     \u001B[43mar\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    337\u001B[0m     aux \u001B[38;5;241m=\u001B[39m ar\n\u001B[0;32m    338\u001B[0m mask \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mempty(aux\u001B[38;5;241m.\u001B[39mshape, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mbool_)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Best Params = {'n_estimators': 200, 'max_depth': None}, Test Accuracy = 0.541\n",
      "Fold 2: Best Params = {'n_estimators': 200, 'max_depth': None}, Test Accuracy = 0.549\n",
      "Fold 3: Best Params = {'n_estimators': 200, 'max_depth': None}, Test Accuracy = 0.537\n",
      "Fold 4: Best Params = {'n_estimators': 200, 'max_depth': None}, Test Accuracy = 0.556\n",
      "Fold 5: Best Params = {'n_estimators': 200, 'max_depth': None}, Test Accuracy = 0.558\n",
      "Nested CV Mean Accuracy: 0.548, Std: 0.009\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def nested_cross_validation(X, y, param_grid, k_outer=5, k_inner=3):\n",
    "    outer_folds = manual_kfold(X, y, k=k_outer, random_state=42)\n",
    "    outer_scores = []\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(outer_folds):\n",
    "        X_train_outer, X_test_outer = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train_outer, y_test_outer = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        best_params = None\n",
    "        best_score = -np.inf\n",
    "        \n",
    "        inner_folds = manual_kfold(X_train_outer, y_train_outer, k=k_inner, random_state=42)\n",
    "        \n",
    "        for params in param_grid:\n",
    "            inner_scores = []\n",
    "            \n",
    "            for inner_train_idx, inner_val_idx in inner_folds:\n",
    "                X_train_inner, X_val_inner = X_train_outer.iloc[inner_train_idx], X_train_outer.iloc[inner_val_idx]\n",
    "                y_train_inner, y_val_inner = y_train_outer.iloc[inner_train_idx], y_train_outer.iloc[inner_val_idx]\n",
    "                \n",
    "                model = RandomForestClassifier(n_estimators=params['n_estimators'], max_depth=params['max_depth'], random_state=42)\n",
    "                model.fit(X_train_inner, y_train_inner)\n",
    "                \n",
    "                y_val_pred = model.predict(X_val_inner)\n",
    "                inner_scores.append(accuracy_score(y_val_inner, y_val_pred))\n",
    "            \n",
    "            mean_inner_score = np.mean(inner_scores)\n",
    "            \n",
    "            if mean_inner_score > best_score:\n",
    "                best_score = mean_inner_score\n",
    "                best_params = params\n",
    "        \n",
    "        best_model = RandomForestClassifier(n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], random_state=42)\n",
    "        best_model.fit(X_train_outer, y_train_outer)\n",
    "        y_test_pred = best_model.predict(X_test_outer)\n",
    "        \n",
    "        test_accuracy = accuracy_score(y_test_outer, y_test_pred)\n",
    "        outer_scores.append(test_accuracy)\n",
    "        \n",
    "        print(f\"Fold {fold_idx+1}: Best Params = {best_params}, Test Accuracy = {test_accuracy:.3f}\")\n",
    "    \n",
    "    mean_score = np.mean(outer_scores)\n",
    "    std_score = np.std(outer_scores)\n",
    "    print(f\"Nested CV Mean Accuracy: {mean_score:.3f}, Std: {std_score:.3f}\")\n",
    "\n",
    "# Sample 10% of the dataset\n",
    "X_sample, _, y_sample, _ = train_test_split(X, y, test_size=0.9, random_state=42)\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': 50, 'max_depth': 10},\n",
    "    {'n_estimators': 100, 'max_depth': 20},\n",
    "    {'n_estimators': 200, 'max_depth': None}\n",
    "]\n",
    "\n",
    "nested_cross_validation(X_sample, y_sample, param_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Medium algorithms: MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:29:40.115988Z",
     "start_time": "2024-10-07T14:29:40.103992Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def train_perceptron_pytorch(X_train_scaled, X_test_scaled, y_train, y_test, n_epochs=30, eta=0.01, optimizer='Adam', params=None):\n",
    "    # Detect if CUDA is available and set the device accordingly\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Convert Pandas DataFrames to NumPy arrays\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = X_train_scaled.to_numpy(), X_test_scaled.to_numpy(), y_train.to_numpy(), y_test.to_numpy()\n",
    "\n",
    "    # Convert to PyTorch tensors and move them to the appropriate device\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = torch.from_numpy(X_train_scaled).to(device), torch.from_numpy(X_test_scaled).to(device), torch.from_numpy(y_train).to(device), torch.from_numpy(y_test).to(device)\n",
    "\n",
    "    # Check input shapes\n",
    "    print(f\"X_train_scaled shape: {X_train_scaled.shape}\")  # 输出形状信息\n",
    "    print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
    "\n",
    "    # Check number of classes\n",
    "    n_classes = len(torch.unique(y_train))\n",
    "    y_train = y_train - 1\n",
    "    y_test = y_test - 1\n",
    "\n",
    "    # Create iterable dataset in Torch format\n",
    "    train_ds = torch.utils.data.TensorDataset(X_train_scaled, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=32)\n",
    "    test_ds = torch.utils.data.TensorDataset(X_test_scaled, y_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "    # Create the multi-layer perceptron model and move it to the device\n",
    "    # Ensure input size matches the data\n",
    "    input_size = X_train_scaled.shape[1]  # 动态获取输入特征数\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_size, 128, dtype=torch.float64),  # 使用数据的特征数作为输入\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 64, dtype=torch.float64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 32, dtype=torch.float64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 16, dtype=torch.float64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, n_classes, dtype=torch.float64),\n",
    "    ).to(device)  # Move model to device\n",
    "\n",
    "    # Define the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Use the provided or default parameters\n",
    "    if params:\n",
    "        n_epochs = params.get('n_epochs', n_epochs)\n",
    "        eta = params.get('eta', eta)\n",
    "        optimizer = params.get('optimizer', optimizer)\n",
    "\n",
    "    # Setup the optimizer. This implements the basic gradient descent update\n",
    "    if optimizer == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=eta)\n",
    "    elif optimizer == 'Momentum':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=eta, momentum=0.9)\n",
    "    elif optimizer == 'Adagrad':\n",
    "        optimizer = torch.optim.Adagrad(model.parameters(), lr=eta, eps=1e-10)\n",
    "    elif optimizer == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=eta)\n",
    "    else:\n",
    "        raise ValueError('Invalid optimizer')\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, (xi, yi) in enumerate(train_loader):\n",
    "            # Compute loss F(w)\n",
    "            xi, yi = xi.to(device), yi.to(device).long()  # Move data to the device\n",
    "            logits = model(xi)\n",
    "            loss = criterion(logits, yi)\n",
    "\n",
    "            # Calculate training accuracy for monitoring purposes\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            train_acc = torch.mean(torch.eq(predictions, yi).float()).item()\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()               # Backward pass (compute parameter gradients)\n",
    "            optimizer.step()              # Update weight parameter using SGD\n",
    "            optimizer.zero_grad()         # Reset gradients to zero for next iteration\n",
    "\n",
    "    # Evaluation loop\n",
    "    with torch.no_grad():\n",
    "        for xi, yi in test_loader:\n",
    "            xi, yi = xi.to(device), yi.to(device).long()  # Move data to the device\n",
    "            logits = model(xi)\n",
    "            loss = criterion(logits, yi)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            accuracy = torch.mean(torch.eq(predictions, yi).float()).item()\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:31:33.731649Z",
     "start_time": "2024-10-07T14:29:42.273649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled shape: torch.Size([260580, 19])\n",
      "X_test_scaled shape: torch.Size([65145, 19])\n",
      "Test Accuracy: 0.2800\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = manual_train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "\n",
    "params = {\n",
    "    'n_epochs': 20,\n",
    "    'eta': 0.01,\n",
    "    'optimizer': 'Adam'\n",
    "}\n",
    "\n",
    "\n",
    "accuracy = train_perceptron_pytorch(X_train_scaled, X_test_scaled, y_train, y_test, n_epochs=params['n_epochs'], eta=params['eta'], optimizer=params['optimizer'])\n",
    "\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled shape: torch.Size([173720, 19])\n",
      "X_test_scaled shape: torch.Size([86860, 19])\n",
      "X_train_scaled shape: torch.Size([173720, 19])\n",
      "X_test_scaled shape: torch.Size([86860, 19])\n",
      "X_train_scaled shape: torch.Size([173720, 19])\n",
      "X_test_scaled shape: torch.Size([86860, 19])\n",
      "X_train_scaled shape: torch.Size([173720, 19])\n",
      "X_test_scaled shape: torch.Size([86860, 19])\n",
      "X_train_scaled shape: torch.Size([173720, 19])\n",
      "X_test_scaled shape: torch.Size([86860, 19])\n",
      "X_train_scaled shape: torch.Size([173720, 19])\n",
      "X_test_scaled shape: torch.Size([86860, 19])\n",
      "X_train_scaled shape: torch.Size([173720, 19])\n",
      "X_test_scaled shape: torch.Size([86860, 19])\n",
      "X_train_scaled shape: torch.Size([173720, 19])\n",
      "X_test_scaled shape: torch.Size([86860, 19])\n",
      "X_train_scaled shape: torch.Size([173720, 19])\n",
      "X_test_scaled shape: torch.Size([86860, 19])\n",
      "X_train_scaled shape: torch.Size([173720, 19])\n",
      "X_test_scaled shape: torch.Size([86860, 19])\n",
      "X_train_scaled shape: torch.Size([173720, 19])\n",
      "X_test_scaled shape: torch.Size([86860, 19])\n",
      "X_train_scaled shape: torch.Size([173720, 19])\n",
      "X_test_scaled shape: torch.Size([86860, 19])\n",
      "X_train_scaled shape: torch.Size([260580, 19])\n",
      "X_test_scaled shape: torch.Size([65145, 19])\n",
      "Outer Fold 1: Best Params = {'n_epochs': 30, 'eta': 0.01, 'optimizer': 'Momentum'}, Test Accuracy = 0.560\n",
      "X_train_scaled shape: torch.Size([173720, 19])\n",
      "X_test_scaled shape: torch.Size([86860, 19])\n",
      "X_train_scaled shape: torch.Size([173720, 19])\n",
      "X_test_scaled shape: torch.Size([86860, 19])\n",
      "X_train_scaled shape: torch.Size([173720, 19])\n",
      "X_test_scaled shape: torch.Size([86860, 19])\n",
      "X_train_scaled shape: torch.Size([173720, 19])\n",
      "X_test_scaled shape: torch.Size([86860, 19])\n",
      "X_train_scaled shape: torch.Size([173720, 19])\n",
      "X_test_scaled shape: torch.Size([86860, 19])\n",
      "X_train_scaled shape: torch.Size([173720, 19])\n",
      "X_test_scaled shape: torch.Size([86860, 19])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[41], line 66\u001B[0m\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNested CV Mean Accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmean_score\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Std: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstd_score\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     58\u001B[0m param_grid \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     59\u001B[0m     {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m20\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meta\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moptimizer\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMomentum\u001B[39m\u001B[38;5;124m'\u001B[39m},\n\u001B[0;32m     60\u001B[0m     {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m30\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meta\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moptimizer\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMomentum\u001B[39m\u001B[38;5;124m'\u001B[39m},\n\u001B[0;32m     61\u001B[0m     {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m20\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meta\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moptimizer\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAdam\u001B[39m\u001B[38;5;124m'\u001B[39m},\n\u001B[0;32m     62\u001B[0m     {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m30\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meta\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moptimizer\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAdam\u001B[39m\u001B[38;5;124m'\u001B[39m}\n\u001B[0;32m     63\u001B[0m ]\n\u001B[1;32m---> 66\u001B[0m \u001B[43mnested_cross_validation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_grid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mouter_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minner_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mAdam\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[41], line 31\u001B[0m, in \u001B[0;36mnested_cross_validation\u001B[1;34m(X, y, param_grid, outer_k, inner_k, n_epochs, eta, optimizer)\u001B[0m\n\u001B[0;32m     28\u001B[0m     X_val_inner_scaled \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(X_val_inner_scaled, columns\u001B[38;5;241m=\u001B[39mX_val_inner\u001B[38;5;241m.\u001B[39mcolumns)\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;66;03m# Train model\u001B[39;00m\n\u001B[1;32m---> 31\u001B[0m     val_acc \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_perceptron_pytorch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_inner_scaled\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val_inner_scaled\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_inner\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val_inner\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m     inner_scores\u001B[38;5;241m.\u001B[39mappend(val_acc)\n\u001B[0;32m     34\u001B[0m mean_inner_score \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(inner_scores)\n",
      "Cell \u001B[1;32mIn[39], line 72\u001B[0m, in \u001B[0;36mtrain_perceptron_pytorch\u001B[1;34m(X_train_scaled, X_test_scaled, y_train, y_test, n_epochs, eta, optimizer, params)\u001B[0m\n\u001B[0;32m     69\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (xi, yi) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[0;32m     70\u001B[0m     \u001B[38;5;66;03m# Compute loss F(w)\u001B[39;00m\n\u001B[0;32m     71\u001B[0m     xi, yi \u001B[38;5;241m=\u001B[39m xi\u001B[38;5;241m.\u001B[39mto(device), yi\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mlong()  \u001B[38;5;66;03m# Move data to the device\u001B[39;00m\n\u001B[1;32m---> 72\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     73\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(logits, yi)\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;66;03m# Calculate training accuracy for monitoring purposes\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\Freya\\anaconda3.1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\Users\\Freya\\anaconda3.1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\Freya\\anaconda3.1\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 219\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    220\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32mc:\\Users\\Freya\\anaconda3.1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\Users\\Freya\\anaconda3.1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\Freya\\anaconda3.1\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:104\u001B[0m, in \u001B[0;36mReLU.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 104\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\Users\\Freya\\anaconda3.1\\Lib\\site-packages\\torch\\nn\\functional.py:1500\u001B[0m, in \u001B[0;36mrelu\u001B[1;34m(input, inplace)\u001B[0m\n\u001B[0;32m   1498\u001B[0m     result \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu_(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m   1499\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1500\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1501\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def nested_cross_validation(X, y, param_grid, outer_k=5, inner_k=3, n_epochs=30, eta=0.01, optimizer='Adam'):\n",
    "    outer_kf = manual_kfold(X, y, k=outer_k, random_state=42)\n",
    "    outer_scores = []\n",
    "\n",
    "    for outer_fold, (train_index, test_index) in enumerate(outer_kf):\n",
    "        #print(f\"Outer Fold {outer_fold + 1}\")\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        best_params = None\n",
    "        best_score = -np.inf\n",
    "\n",
    "        inner_kf = manual_kfold(X_train, y_train, k=inner_k, random_state=42)\n",
    "\n",
    "        for params in param_grid:\n",
    "            inner_scores = []\n",
    "            for inner_fold, (inner_train_index, inner_val_index) in enumerate(inner_kf):\n",
    "                X_train_inner, X_val_inner = X_train.iloc[inner_train_index], X_train.iloc[inner_val_index]\n",
    "                y_train_inner, y_val_inner = y_train.iloc[inner_train_index], y_train.iloc[inner_val_index]\n",
    "\n",
    "                # Scaling\n",
    "                scaler = StandardScaler()\n",
    "                X_train_inner_scaled = scaler.fit_transform(X_train_inner)\n",
    "                X_val_inner_scaled = scaler.transform(X_val_inner)\n",
    "\n",
    "                # Convert to Pandas DataFrame\n",
    "                X_train_inner_scaled = pd.DataFrame(X_train_inner_scaled, columns=X_train_inner.columns)\n",
    "                X_val_inner_scaled = pd.DataFrame(X_val_inner_scaled, columns=X_val_inner.columns)\n",
    "\n",
    "                # Train model\n",
    "                val_acc = train_perceptron_pytorch(X_train_inner_scaled, X_val_inner_scaled, y_train_inner, y_val_inner, n_epochs, eta, optimizer, params)\n",
    "                inner_scores.append(val_acc)\n",
    "\n",
    "            mean_inner_score = np.mean(inner_scores)\n",
    "            if mean_inner_score > best_score:\n",
    "                best_score = mean_inner_score\n",
    "                best_params = params\n",
    "\n",
    "        # Train the model with the best parameters\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Convert to Pandas DataFrame\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "        test_acc = train_perceptron_pytorch(X_train_scaled, X_test_scaled, y_train, y_test, n_epochs, eta, optimizer, best_params)\n",
    "        outer_scores.append(test_acc)\n",
    "\n",
    "        print(f\"Outer Fold {outer_fold + 1}: Best Params = {best_params}, Test Accuracy = {test_acc:.3f}\")\n",
    "\n",
    "    mean_score = np.mean(outer_scores)\n",
    "    std_score = np.std(outer_scores)\n",
    "    print(f\"Nested CV Mean Accuracy: {mean_score:.3f}, Std: {std_score:.3f}\")\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {'n_epochs': 20, 'eta': 0.01, 'optimizer': 'Momentum'},\n",
    "    {'n_epochs': 30, 'eta': 0.01, 'optimizer': 'Momentum'},\n",
    "    {'n_epochs': 20, 'eta': 0.01, 'optimizer': 'Adam'},\n",
    "    {'n_epochs': 30, 'eta': 0.01, 'optimizer': 'Adam'}\n",
    "]\n",
    "\n",
    "\n",
    "nested_cross_validation(X, y, param_grid, outer_k=5, inner_k=3, n_epochs=30, eta=0.01, optimizer='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Complex algorithms: TabNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:50.260911900Z",
     "start_time": "2024-10-07T14:04:38.632193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.93206 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.1077  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.93141 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.10706 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.93134 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.10699 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.93906 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.1322  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.93846 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.13163 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.93839 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.13156 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.94276 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.14698 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.94224 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.14649 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.94218 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.14644 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.39511 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.12997 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.39454 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.12939 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.39448 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.12933 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.34576 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.16998 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.3453  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.1695  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.34525 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.16945 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.32099 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.2419  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.32061 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.24156 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.32057 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.24152 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.90081 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.53675 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.90033 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.53621 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.90027 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.53614 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.7314  | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.2764  | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.731   | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.276   | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.73096 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.27595 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.05484 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.93188 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.05456 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.9316  | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.05453 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.93157 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 1.96192 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.21782 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.96131 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.2173  | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.96125 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.21724 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.95663 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.22336 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.95608 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.22287 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.95602 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.22282 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.00833 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.18083 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.00786 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.18041 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.00781 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.18036 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.58551 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.36224 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.58498 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.36172 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.58492 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.36167 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.52515 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.47105 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.52468 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.47061 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.52463 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.47056 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.50185 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.5227  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.50152 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.52238 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.50148 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.52234 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.98183 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.85071 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.98133 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.85021 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.98127 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.85015 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.77769 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.93552 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.77728 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.93515 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.77724 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.93511 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.78586 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 3.25939 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.78562 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 3.25914 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.7856  | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 3.25911 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.08224 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.97681 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.08163 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.97617 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.08156 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.9761  | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.09037 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 1.96101 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.0898  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 1.96045 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.08974 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 1.96038 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.05614 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.87184 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.05568 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.87138 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.05562 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.87133 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.91139 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.02853 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.91085 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.02805 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.91079 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.02799 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.92382 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.03277 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.92339 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.03238 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.92334 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.03234 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.88371 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.04135 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.88339 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.04107 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.88336 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.04104 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.94733 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.61451 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.94684 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.61403 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.94678 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.61397 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.99986 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.53891 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.99949 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.53857 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.99945 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.53853 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.9996  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 6.27984 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.99934 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 6.27963 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.99931 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 6.27961 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.11878 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.46273 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.11819 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.46208 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.11812 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.46201 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.09536 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.47371 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.09483 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.47312 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.09477 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.47306 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.0269  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.42973 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.02648 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.42922 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.02643 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.42917 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.25611 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.30217 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.25563 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.30157 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.25557 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.3015  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.09592 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.18456 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.09552 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.18406 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.09547 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.18401 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.12181 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.23486 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.1215  | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.23453 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.12146 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.2345  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.33149 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.94563 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.33096 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.94513 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.3309  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.94507 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.32297 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 6.01628 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.32257 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 6.01591 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.32253 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 6.01587 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.35225 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.94024 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.35198 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.94    | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.35195 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.93998 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.36764 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.10414 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.36698 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.10349 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.36691 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.10342 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.43474 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.08091 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.43413 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.08033 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.43407 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.08026 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.45723 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.0105  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.45671 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.01002 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.45665 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.00996 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.62148 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.52857 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.62089 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.52802 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.62082 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.52796 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.52341 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.42272 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.52291 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.42228 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.52286 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.42223 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.3889  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.46056 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.38855 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.46027 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.38851 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.46023 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.64151 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.19564 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.64102 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.19515 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.64097 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.1951  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.54922 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.2857  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.54883 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.28533 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.54879 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.28529 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.06424 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.35433 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.06401 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.35408 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.06398 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.35405 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.67006 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.54756 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.66944 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.54696 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.66937 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.54689 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.65908 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.54002 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.65849 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.53948 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.65843 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.53942 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.64113 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.59821 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.64065 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.59776 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.64059 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.59771 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 4.11568 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.77697 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.11521 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.77642 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.11515 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.77636 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.2443  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.78217 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.2439  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.78169 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.24385 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.78163 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.02263 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.88459 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.02234 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.88425 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.02231 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.88421 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.93892 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.72413 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.93843 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.72363 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.93838 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.72357 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.58083 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.34162 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.58043 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.34122 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.58038 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.34118 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.56793 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.42115 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.56765 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.4209  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.56762 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.42087 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.13173 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.68678 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.13104 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.68616 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.13096 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.68609 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.04801 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.66899 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.04739 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.66843 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.04732 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.66837 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.03598 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.61866 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.03545 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.61819 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.0354  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.61813 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.05316 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.33089 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.05253 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.33035 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.05246 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.33029 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.04487 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.34157 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.04433 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.34109 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.04427 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.34104 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.18815 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.27534 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.18776 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.27501 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.18772 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.27497 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.52251 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.02819 | val_0_accuracy: 0.45455 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.45455\n",
      "epoch 0  | loss: 3.52197 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.0277  | val_0_accuracy: 0.45455 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.45455\n",
      "epoch 0  | loss: 3.52191 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.02764 | val_0_accuracy: 0.45455 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.45455\n",
      "epoch 0  | loss: 3.54677 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.64495 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.54636 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.64458 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.54631 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.64453 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.44931 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.41154 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.44905 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.41129 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.44902 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.41126 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.06608 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 1.98212 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.0655  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 1.98152 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.06544 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 1.98145 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.08609 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.95104 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.08558 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.9505  | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.08552 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.95044 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.06599 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.87012 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.06557 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.86967 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.06552 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.86962 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.17294 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 4.00217 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.17241 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 4.00163 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.17235 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 4.00156 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.27034 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.93051 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.26991 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.93008 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.26986 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.93003 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.36932 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.09096 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.36899 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.09063 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.36895 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.09059 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.13615 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.68603 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.13567 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.6856  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.13562 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.68555 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.30218 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.91781 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.30183 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.91748 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.30179 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.91744 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.95876 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.16017 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.9585  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.15993 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.95847 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.15991 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.96821 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.73747 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.96763 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.73691 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.96756 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.73685 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.00784 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.74807 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.00731 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.74757 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.00725 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.74751 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.12126 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.95301 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.12084 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.95258 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.1208  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.95254 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.88835 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.52414 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.8878  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.52357 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.88774 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.5235  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.80295 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.68281 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.80248 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.6823  | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.80243 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.68224 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.03423 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.56487 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 3.03391 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.56449 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 3.03387 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.56445 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.37291 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.05406 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.37246 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.05358 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.37242 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.05353 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.23403 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.41296 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.23368 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.41261 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.23364 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.41258 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.21195 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.3316  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.2117  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.33136 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.21167 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.33133 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.16766 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.39674 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.81132 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.39611 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.8107  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.39604 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.81063 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.36678 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.80813 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.36619 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.80757 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.36612 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.80751 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.30727 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.7823  | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.30677 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.78183 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.30671 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.78178 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.28542 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.78802 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.28486 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.78748 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.2848  | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.78742 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.34726 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.86025 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.34678 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.85977 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.34672 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.85972 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.23365 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.86989 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.2333  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.86952 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.23327 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.86948 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.96895 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.02849 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.96842 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.02791 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.96836 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.02784 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.22219 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.03114 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.22179 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.03065 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.22174 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.0306  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.9931  | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.76437 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.99283 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.76407 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.9928  | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.76404 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.24887 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.46537 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.24823 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.46476 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.24816 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.46469 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.19773 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.39034 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.19713 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.38979 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.19707 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.38973 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.12493 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.17924 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.12443 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.17875 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.12437 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.1787  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.47715 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.31799 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.47657 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.31741 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.47651 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.31734 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.43504 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.2244  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.43457 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.22392 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.43451 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.22387 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.39911 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.25966 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.39875 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.25932 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.39871 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.25928 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.86671 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.65151 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.86615 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.65097 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.86609 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.65091 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.08574 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.76634 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.08531 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.76592 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.08526 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.76587 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.95024 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.72294 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.94999 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.72267 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.94996 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.72264 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.33033 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.71791 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.3297  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.71732 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.32962 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.71725 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.36862 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.71572 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.36803 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.71519 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.36796 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.71514 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.40512 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.71997 | val_0_accuracy: 0.0     |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.0\n",
      "epoch 0  | loss: 2.40462 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.71951 | val_0_accuracy: 0.0     |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.0\n",
      "epoch 0  | loss: 2.40457 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.71946 | val_0_accuracy: 0.0     |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.0\n",
      "epoch 0  | loss: 1.92696 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.77731 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 1.92642 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.77677 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 1.92636 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.77671 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 1.94337 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.8975  | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 1.9429  | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.89702 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 1.94285 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.89697 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 1.90547 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.77287 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.90513 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.77252 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.90509 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.77248 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 5.66315 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.62958 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.66273 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.62906 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.66268 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.62901 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.36043 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.52303 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.36012 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.52266 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.36008 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.52262 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.53885 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 5.8569  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.53865 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 5.85667 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.53862 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 5.85664 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.38421 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.18819 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.38358 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.18759 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.38351 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.18752 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.44257 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.13975 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.442   | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.13921 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.44194 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.13915 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.44793 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.08547 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.44746 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.08503 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.44741 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.08498 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.40314 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.93546 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.40261 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.93487 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.40255 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.93481 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.35544 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.78981 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.35498 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.78933 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.35493 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.78927 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.19529 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.80842 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.19495 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.80808 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.19492 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.80804 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.19849 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.03461 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 5.198   | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.03411 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 5.19794 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.03405 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 5.58931 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 5.34205 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.58892 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 5.34166 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.58888 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 5.34161 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.69246 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.44382 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 5.69221 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.44356 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 5.69218 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.44353 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.17101 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 3.11833 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.17035 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 3.11775 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.17028 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 3.11769 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.14711 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.13521 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.14651 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.1347  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.14645 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.13464 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.06746 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.16289 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.06695 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.16249 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.06689 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.16245 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.13231 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.7492  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.13174 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.74868 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.13168 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.74862 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.12556 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.62489 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.12508 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.62445 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.12503 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.6244  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.12946 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.72895 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.12912 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.72864 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.12909 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.7286  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.81287 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.91834 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.81239 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.91788 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.81233 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.91783 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.89833 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.0005  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.89795 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.00014 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.89791 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.0001  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.60885 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.94264 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.60862 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.94241 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.6086  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.94238 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.47029 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.82346 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.46965 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.82287 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.46958 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.8228  | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.44732 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.83603 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.44675 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.83549 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.44669 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.83543 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.42758 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.92237 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.42711 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.92192 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.42706 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.92187 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 3.95217 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.65658 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.95165 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.656   | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.95159 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.65593 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.84303 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.83163 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.84261 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.83113 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.84256 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.83107 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.87413 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.76676 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.87382 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.76639 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.87378 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.76635 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.84191 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.98981 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.8414  | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.98926 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.84135 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.9892  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.76644 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.36171 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 3.76605 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.36129 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 3.76601 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.36124 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 4.11981 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.07086 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.11956 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.07056 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.11953 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.07053 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.36268 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.68674 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.362   | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.68607 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.36192 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.686   | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.37467 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.74893 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.37403 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.74834 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.37396 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.74827 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.4067  | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.96965 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.40618 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.96916 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.40612 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.96911 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.13913 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.25922 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.1385  | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.2586  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.13843 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.25853 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.20948 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.41656 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.20895 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.41605 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.2089  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.41599 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.42543 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.55177 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.42508 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.55141 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.42504 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.55137 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.86088 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.24587 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.86039 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.24531 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.86033 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.24525 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.55349 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 4.06773 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.55313 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 4.06729 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.55309 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 4.06724 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.04571 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.1782  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 4.04546 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.1779  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 4.04543 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.17787 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.49267 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.08447 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.49204 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.08385 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.49197 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.08379 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.45228 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.05726 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.4517  | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.05671 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.45164 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.05665 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.44111 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.00583 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.44062 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.00535 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.44056 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.00529 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.16085 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.76872 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.16033 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.76815 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.16027 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.76809 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.05982 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.68852 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.05937 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.68806 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.05932 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.68801 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.20795 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.73784 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.20765 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.7375  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.20761 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.73746 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.93101 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.7524  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.93054 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.75194 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.93049 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.75189 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.81173 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 5.00762 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.81137 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 5.00727 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.81134 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 5.00723 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.76486 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.65459 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.7646  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.65435 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.76458 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.65432 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.90392 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.9465  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.9034  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.9459  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.90334 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.94583 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.91336 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.98532 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.9129  | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.98477 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.91285 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.98471 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.89035 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.95363 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.89    | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.95315 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.88996 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.9531  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.80307 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.44993 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.80256 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.44937 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.8025  | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.4493  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.81218 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.5595  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.81175 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.55902 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.81171 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.55897 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.42636 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.468   | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.42604 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.46763 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.42601 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.46759 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.82321 | val_0_accuracy: 0.0303  |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.0303\n",
      "epoch 0  | loss: 3.03745 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.82273 | val_0_accuracy: 0.0303  |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.0303\n",
      "epoch 0  | loss: 3.03696 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.82268 | val_0_accuracy: 0.0303  |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.0303\n",
      "epoch 0  | loss: 3.0369  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.66344 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.89498 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.66305 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.89461 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.66301 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.89457 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.22366 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.10804 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.2234  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.1078  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.22338 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.10777 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.34176 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.42149 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.77302 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.42084 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.77243 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.42076 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.77237 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.40721 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.76193 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.4066  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.7614  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.40653 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.76134 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.39936 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.75275 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.39882 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.75231 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.39876 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.75226 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.78669 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.75686 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.7861  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.75627 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.78604 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.7562  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.74742 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.92018 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.74695 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.91966 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.7469  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.91961 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.92057 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.10117 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.92019 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.10079 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.92015 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.10074 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.16116 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.59473 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.16069 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.59419 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.16063 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.59413 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.29245 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.56251 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.29206 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.56209 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.29201 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.56204 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.95607 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.90801 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.95582 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.90772 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.95579 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.90769 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.68482 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.16745 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.68423 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.16681 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.68416 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.16673 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.68685 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.11363 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.6863  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.11306 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.68624 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.113   | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.60633 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.0741  | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.60589 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.07361 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.60584 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.07355 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.28896 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.20397 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.28838 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.2034  | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.28832 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.20333 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.25439 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.10622 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.25387 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.10575 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.25382 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.1057  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.20823 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.1877  | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.20788 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.18738 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.20785 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.18735 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.16669 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.19447 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.16622 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.19401 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.16616 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.19396 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.52972 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.34457 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.52934 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.34421 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.5293  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.34418 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.29052 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.42002 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.29026 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.4198  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.29024 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.41977 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.43019 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.20856 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.42958 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.20796 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.42952 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.20789 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.33609 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.21748 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.33553 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.21692 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.33547 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.21686 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.24357 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.13272 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.2431  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.13224 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.24305 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.13219 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.36368 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.6765  | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.36314 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.67594 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.36308 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.67587 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.41252 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.74093 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.41208 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.74046 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.41203 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.74041 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.47616 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.76854 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.47581 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.76816 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.47577 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.76812 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 6.28741 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.13737 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 6.28698 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.1369  | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 6.28693 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.13684 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 6.11384 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.2247  | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 6.11352 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.22433 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 6.11349 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.22429 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 6.46894 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.60451 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 6.46872 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.60428 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 6.4687  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.60425 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.30587 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.24995 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.30527 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.24934 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.3052  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.24928 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.35166 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.2519  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.35112 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.25136 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.35105 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.25129 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.47969 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.28257 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.47922 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.28212 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.47917 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.28207 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.34871 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.69978 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.34821 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.69923 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.34816 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.69917 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.32904 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.95601 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.32859 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.95555 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.32854 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.9555  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.43371 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.78097 | val_0_accuracy: 0.0303  |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.0303\n",
      "epoch 0  | loss: 3.43336 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.78064 | val_0_accuracy: 0.0303  |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.0303\n",
      "epoch 0  | loss: 3.43332 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.7806  | val_0_accuracy: 0.0303  |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.0303\n",
      "epoch 0  | loss: 6.51721 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 6.31668 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 6.51675 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 6.31615 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 6.5167  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 6.31609 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 6.74329 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 6.00455 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 6.74295 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 6.00414 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 6.74291 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 6.0041  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 6.23005 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 6.34387 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 6.22981 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 6.34362 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 6.22978 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 6.34359 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.67731 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.80601 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.67668 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.80541 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.67661 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.80535 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.63094 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.81563 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.63037 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.81508 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.63031 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.81502 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.58517 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.81381 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.58471 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.81334 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.58466 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.81328 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 1.86091 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.44836 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.86037 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.44778 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.86031 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.44772 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.95626 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.59816 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.95582 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.59768 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.95577 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.59763 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.98949 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.74363 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 1.98919 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.74327 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 1.98915 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.74323 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.48039 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.54993 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.47992 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.54943 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.47986 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.54937 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.56355 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.67007 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.56318 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.66967 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.56314 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.66962 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.87316 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.77346 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.87292 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.7732  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.87289 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.77317 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.38392 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.43557 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.38334 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.435   | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.38328 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.43494 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.37298 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.42259 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.37243 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.42208 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.37237 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.42202 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.34694 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.44252 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.34646 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.44211 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.34641 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.44207 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.70164 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.91777 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.70113 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.91721 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.70107 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.91715 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.46544 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.80162 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.465   | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.80116 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.46495 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.80111 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.47817 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.94056 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.47785 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.94023 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.47782 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.94019 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.94237 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.18868 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.94186 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.18822 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.94181 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.18817 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.27044 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.13637 | val_0_accuracy: 0.42424 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.42424\n",
      "epoch 0  | loss: 3.27006 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.136   | val_0_accuracy: 0.42424 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.42424\n",
      "epoch 0  | loss: 3.27002 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.13596 | val_0_accuracy: 0.42424 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.42424\n",
      "epoch 0  | loss: 3.6105  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.12322 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.61025 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.12299 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.61022 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.12296 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.54649 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.32688 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.54587 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.32623 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.5458  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.32616 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.55435 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.35437 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.55378 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.35377 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.55372 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.35371 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.57734 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.3156  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.57685 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.31511 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.57679 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.31505 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.15808 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.35588 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.15747 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.35524 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.1574  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.35517 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.73095 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.2871  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.73039 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.28657 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.73033 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.28651 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.99579 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.00396 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.99541 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.00358 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.99536 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.00353 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.69374 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.80649 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.69323 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.80593 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.69317 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.80587 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.53975 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.58622 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.53935 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.5858  | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.53931 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.58575 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 5.09478 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.89962 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 5.09454 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.89935 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 5.09451 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.89932 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.27309 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 1.8095  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.27254 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 1.80882 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.27248 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 1.80874 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.34238 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.78365 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.34189 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.78303 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.34183 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.78296 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.42169 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.83623 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.42126 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.83571 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.42122 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.83566 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.08336 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.41453 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.08283 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.414   | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.08277 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.41394 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.98107 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.36611 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.98061 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.36564 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.98056 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.36558 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.01178 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.6319  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.01144 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.63155 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.01141 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.63151 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.83084 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.54481 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.83038 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.54435 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.83033 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.5443  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.72186 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.50269 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.72149 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.50232 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.72144 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.50228 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.70656 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.38975 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.70631 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.38952 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.70628 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.38949 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.32823 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.57825 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.32768 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.57766 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.32762 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.57759 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.28317 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.57573 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.28267 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.5752  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.28262 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.57514 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.27099 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.56388 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.27054 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.56345 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.27049 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.5634  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.51247 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.73894 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.51191 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.7384  | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.51185 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.73834 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.50045 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.94046 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.49998 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.94    | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.49993 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.93995 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.40029 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.70647 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.39995 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.70613 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.39991 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.70609 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.91514 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.06004 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.91465 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.05954 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.91459 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.05949 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.99837 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.94745 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.99802 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.94707 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.99798 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.94703 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.16156 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.96979 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.1613  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.96955 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.16127 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.96952 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.26103 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "Average accuracy score from outer cross-validation: 0.1818181818181818\n",
      "Best hyperparameters from each fold: [{'n_d': 8, 'n_a': 24, 'n_steps': 5, 'gamma': 1.5, 'lambda_sparse': 0.001}, {'n_d': 8, 'n_a': 24, 'n_steps': 5, 'gamma': 1.5, 'lambda_sparse': 0.0001}, {'n_d': 16, 'n_a': 24, 'n_steps': 7, 'gamma': 1.5, 'lambda_sparse': 0.001}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "\n",
    "\n",
    "data_sample = data.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# X 是特征，y 是目标变量\n",
    "X = data_sample.drop('review_score', axis=1)\n",
    "y = data_sample['review_score'].astype(int)  \n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "def manual_kfold(X, y, k=3, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(len(X))\n",
    "    fold_size = len(X) // k\n",
    "    folds = []\n",
    "\n",
    "    for i in range(k):\n",
    "        test_indices = indices[i * fold_size:(i + 1) * fold_size]\n",
    "        train_indices = np.concatenate([indices[:i * fold_size], indices[(i + 1) * fold_size:]])\n",
    "        folds.append((train_indices, test_indices))\n",
    "    \n",
    "    return folds\n",
    "\n",
    "\n",
    "outer_folds = manual_kfold(X_scaled, y, k=3, random_state=42)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_d': [8, 16, 24],\n",
    "    'n_a': [8, 16, 24],\n",
    "    'n_steps': [3, 5, 7],\n",
    "    'gamma': [1.3, 1.5, 2.0],\n",
    "    'lambda_sparse': [1e-3, 1e-4, 0]\n",
    "}\n",
    "\n",
    "best_params = []\n",
    "outer_scores = []\n",
    "\n",
    "\n",
    "for train_indices, test_indices in outer_folds:\n",
    "    X_train, X_test = X_scaled[train_indices], X_scaled[test_indices]\n",
    "    y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]\n",
    "\n",
    "\n",
    "    inner_folds = manual_kfold(X_train, y_train, k=2, random_state=42)\n",
    "\n",
    "    best_inner_score = -np.inf\n",
    "    best_inner_params = None\n",
    "\n",
    "  \n",
    "    for n_d in param_grid['n_d']:\n",
    "        for n_a in param_grid['n_a']:\n",
    "            for n_steps in param_grid['n_steps']:\n",
    "                for gamma in param_grid['gamma']:\n",
    "                    for lambda_sparse in param_grid['lambda_sparse']:\n",
    "                        \n",
    "                        inner_scores = []\n",
    "\n",
    "                        \n",
    "                        for inner_train_indices, inner_val_indices in inner_folds:\n",
    "                            X_inner_train, X_inner_val = X_train[inner_train_indices], X_train[inner_val_indices]\n",
    "                            y_inner_train, y_inner_val = y_train.iloc[inner_train_indices], y_train.iloc[inner_val_indices]\n",
    "\n",
    "                          \n",
    "                            tabnet = TabNetClassifier(\n",
    "                                n_d=n_d,\n",
    "                                n_a=n_a,\n",
    "                                n_steps=n_steps,\n",
    "                                gamma=gamma,\n",
    "                                lambda_sparse=lambda_sparse\n",
    "                            )\n",
    "\n",
    "                            tabnet.fit(\n",
    "                                X_inner_train, y_inner_train,\n",
    "                                eval_set=[(X_inner_val, y_inner_val)],\n",
    "                                eval_metric=['accuracy'],\n",
    "                                max_epochs=1,\n",
    "                                patience=3,\n",
    "                                batch_size=1024,\n",
    "                                virtual_batch_size=128,\n",
    "                                num_workers=0,\n",
    "                                drop_last=False\n",
    "                            )\n",
    "\n",
    "                           \n",
    "                            y_val_pred = tabnet.predict(X_inner_val)\n",
    "                            accuracy = accuracy_score(y_inner_val, y_val_pred)\n",
    "                            inner_scores.append(accuracy)\n",
    "\n",
    "                       \n",
    "                        avg_inner_score = np.mean(inner_scores)\n",
    "\n",
    "                       \n",
    "                        if avg_inner_score > best_inner_score:\n",
    "                            best_inner_score = avg_inner_score\n",
    "                            best_inner_params = {\n",
    "                                'n_d': n_d,\n",
    "                                'n_a': n_a,\n",
    "                                'n_steps': n_steps,\n",
    "                                'gamma': gamma,\n",
    "                                'lambda_sparse': lambda_sparse\n",
    "                            }\n",
    "\n",
    "    \n",
    "    best_model = TabNetClassifier(**best_inner_params)\n",
    "    best_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric=['accuracy'],\n",
    "        max_epochs=1,\n",
    "        patience=3,\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    \n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    outer_scores.append(test_accuracy)\n",
    "    best_params.append(best_inner_params)\n",
    "\n",
    "\n",
    "mean_score = np.mean(outer_scores)\n",
    "print(f\"Average accuracy score from outer cross-validation: {mean_score}\")\n",
    "print(f\"Best hyperparameters from each fold: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:43:39.977926Z",
     "start_time": "2024-10-07T14:40:14.105121Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 1/100 [00:05<09:34,  5.81s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.22281 | test_accuracy: 0.58585 |  0:00:05s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 2/100 [00:10<08:47,  5.38s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  | loss: 1.1606  | test_accuracy: 0.58832 |  0:00:10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 3/100 [00:16<08:51,  5.48s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2  | loss: 1.15524 | test_accuracy: 0.5923  |  0:00:16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 4/100 [00:21<08:41,  5.43s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 1.14739 | test_accuracy: 0.59203 |  0:00:21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▌         | 5/100 [00:27<08:37,  5.45s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4  | loss: 1.14556 | test_accuracy: 0.59305 |  0:00:27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 6/100 [00:33<09:05,  5.80s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5  | loss: 1.14585 | test_accuracy: 0.59216 |  0:00:33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 7/100 [00:40<09:15,  5.97s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6  | loss: 1.14361 | test_accuracy: 0.59238 |  0:00:40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 8/100 [00:46<09:21,  6.10s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7  | loss: 1.14421 | test_accuracy: 0.59375 |  0:00:46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   9%|▉         | 9/100 [00:53<09:31,  6.28s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8  | loss: 1.14266 | test_accuracy: 0.59344 |  0:00:53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|█         | 10/100 [00:59<09:37,  6.42s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9  | loss: 1.14394 | test_accuracy: 0.59216 |  0:00:59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  11%|█         | 11/100 [01:06<09:38,  6.50s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 | loss: 1.14447 | test_accuracy: 0.59053 |  0:01:06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  12%|█▏        | 12/100 [01:12<09:07,  6.22s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 | loss: 1.14194 | test_accuracy: 0.59199 |  0:01:12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  13%|█▎        | 13/100 [01:18<08:56,  6.17s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 | loss: 1.13992 | test_accuracy: 0.59526 |  0:01:18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  14%|█▍        | 14/100 [01:24<08:46,  6.13s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 | loss: 1.13668 | test_accuracy: 0.59481 |  0:01:24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  15%|█▌        | 15/100 [01:29<08:28,  5.98s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 | loss: 1.13413 | test_accuracy: 0.59645 |  0:01:29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  16%|█▌        | 16/100 [01:35<08:06,  5.80s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 | loss: 1.13153 | test_accuracy: 0.59574 |  0:01:35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  17%|█▋        | 17/100 [01:41<08:02,  5.81s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 | loss: 1.13001 | test_accuracy: 0.59517 |  0:01:41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  18%|█▊        | 18/100 [01:46<07:55,  5.80s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 | loss: 1.13068 | test_accuracy: 0.59446 |  0:01:46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  19%|█▉        | 19/100 [01:52<07:52,  5.84s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 | loss: 1.12841 | test_accuracy: 0.59645 |  0:01:52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  20%|██        | 20/100 [01:58<07:53,  5.91s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 | loss: 1.12592 | test_accuracy: 0.59574 |  0:01:58s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  21%|██        | 21/100 [02:04<07:43,  5.87s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 | loss: 1.12559 | test_accuracy: 0.59521 |  0:02:04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  22%|██▏       | 22/100 [02:10<07:26,  5.72s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21 | loss: 1.12892 | test_accuracy: 0.59521 |  0:02:10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  23%|██▎       | 23/100 [02:15<07:09,  5.58s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22 | loss: 1.12607 | test_accuracy: 0.5953  |  0:02:15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  24%|██▍       | 24/100 [02:20<07:06,  5.61s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23 | loss: 1.12413 | test_accuracy: 0.59685 |  0:02:20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  25%|██▌       | 25/100 [02:26<06:53,  5.52s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24 | loss: 1.12332 | test_accuracy: 0.59733 |  0:02:26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  26%|██▌       | 26/100 [02:31<06:51,  5.56s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25 | loss: 1.1243  | test_accuracy: 0.59645 |  0:02:31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  27%|██▋       | 27/100 [02:37<06:37,  5.45s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26 | loss: 1.12174 | test_accuracy: 0.59583 |  0:02:37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  28%|██▊       | 28/100 [02:42<06:27,  5.39s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27 | loss: 1.1224  | test_accuracy: 0.59724 |  0:02:42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  29%|██▉       | 29/100 [02:47<06:21,  5.37s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28 | loss: 1.12274 | test_accuracy: 0.59632 |  0:02:47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  30%|███       | 30/100 [02:53<06:32,  5.60s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29 | loss: 1.12176 | test_accuracy: 0.59724 |  0:02:53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  31%|███       | 31/100 [02:59<06:29,  5.64s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 1.12101 | test_accuracy: 0.59433 |  0:02:59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  32%|███▏      | 32/100 [03:05<06:19,  5.58s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31 | loss: 1.12276 | test_accuracy: 0.59693 |  0:03:04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  33%|███▎      | 33/100 [03:10<06:10,  5.53s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32 | loss: 1.1228  | test_accuracy: 0.59512 |  0:03:10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  34%|███▍      | 34/100 [03:16<06:08,  5.58s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33 | loss: 1.12212 | test_accuracy: 0.59707 |  0:03:16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  35%|███▌      | 35/100 [03:21<06:05,  5.63s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34 | loss: 1.12104 | test_accuracy: 0.59627 |  0:03:21s\n",
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 24 and best_test_accuracy = 0.59733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  35%|███▌      | 35/100 [03:25<06:21,  5.86s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Actual  Predicted\n",
      "0       2          5\n",
      "1       1          5\n",
      "2       5          5\n",
      "3       4          5\n",
      "4       5          5\n",
      "Loss history: [1.222808775235779, 1.1605984834820398, 1.1552352344863133, 1.1473934672827024, 1.1455642134092705, 1.1458457621499771, 1.1436136813278235, 1.1442117822526485, 1.1426642932800637, 1.1439424119110568, 1.1444660649481924, 1.1419429721181844, 1.1399197324066837, 1.1366809869472363, 1.1341324111062374, 1.131532362443829, 1.1300123741989185, 1.1306789731838254, 1.1284116840216163, 1.125922789658343, 1.1255912338778735, 1.1289179363438617, 1.1260696790884839, 1.1241294108039117, 1.1233177950676223, 1.1242993919530868, 1.1217356777908365, 1.122400068514291, 1.1227410118374026, 1.1217580409185535, 1.1210091464639615, 1.1227585764443315, 1.1227996413445556, 1.1221246161266993, 1.1210424820436389]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "X = data.drop('review_score', axis=1)\n",
    "y = data['review_score'].astype(int)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "tabnet_model = TabNetClassifier(\n",
    "    n_d=8,\n",
    "    n_a=24,\n",
    "    n_steps=5,\n",
    "    gamma=1.5,\n",
    "    lambda_sparse=0.001\n",
    ")\n",
    "\n",
    "\n",
    "max_epochs = 100\n",
    "\n",
    "\n",
    "with tqdm(total=max_epochs, desc=\"Training Progress\", unit=\"epoch\") as pbar:\n",
    "\n",
    "    \n",
    "    class TqdmCallback(Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "    tabnet_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_name=['test'],\n",
    "        eval_metric=['accuracy'], \n",
    "        max_epochs=max_epochs,  \n",
    "        patience=10,\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        callbacks=[TqdmCallback()] \n",
    "    )\n",
    "\n",
    "\n",
    "predictions = tabnet_model.predict(X_test)\n",
    "\n",
    "\n",
    "results = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': predictions.flatten()})\n",
    "print(results.head())\n",
    "\n",
    "print(f\"Loss history: {tabnet_model.history['loss']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T03:31:32.021824Z",
     "start_time": "2024-10-09T03:28:32.866960Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = manual_train_test_split(X_smt, y_smt, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "tabnet_model = TabNetClassifier(\n",
    "    n_d=8,\n",
    "    n_a=24,\n",
    "    n_steps=5,\n",
    "    gamma=1.5,\n",
    "    lambda_sparse=0.001\n",
    ")\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "\n",
    "with tqdm(total=max_epochs, desc=\"Training Progress\", unit=\"epoch\") as pbar:\n",
    "    class TqdmCallback(Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            pbar.update(1)\n",
    "\n",
    "    \n",
    "    tabnet_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_name=['test'],\n",
    "        eval_metric=['accuracy'],\n",
    "        max_epochs=max_epochs,\n",
    "        patience=10,\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        callbacks=[TqdmCallback()]\n",
    "    )\n",
    "\n",
    "\n",
    "predictions = tabnet_model.predict(X_test)\n",
    "\n",
    "\n",
    "results = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': predictions.flatten()})\n",
    "print(results.head())\n",
    "\n",
    "\n",
    "print(f\"Loss history: {tabnet_model.history['loss']}\")\n",
    "\n",
    "\n",
    "train_sizes = [len(X_train) * 0.01, len(X_train) * 0.1, len(X_train) * 0.5, len(X_train)]  # [0.01, 0.1, 0.5, 1.0]\n",
    "accuracies = []\n",
    "\n",
    "\n",
    "for size in train_sizes:\n",
    "    subset_idx = np.random.choice(len(X_train), int(size), replace=False)\n",
    "    X_train_subset = X_train[subset_idx]\n",
    "    y_train_subset = y_train[subset_idx]\n",
    "    \n",
    "  \n",
    "    tabnet_model.fit(\n",
    "        X_train_subset, y_train_subset,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_name=['test'],\n",
    "        eval_metric=['accuracy'],\n",
    "        max_epochs=max_epochs,\n",
    "        patience=5,\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        callbacks=[TqdmCallback()]\n",
    "    )\n",
    "    \n",
    "    y_pred_subset = tabnet_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred_subset)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_sizes, accuracies, marker='o', label='Accuracy')\n",
    "plt.title(\"Learning Curve with Train Sizes\")\n",
    "plt.xlabel(\"Training Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(x=train_sizes, y=accuracies, yerr=std_accuracy, fmt='o', label='Accuracy with error bar')\n",
    "plt.title(\"Learning Curve with Error Bars\")\n",
    "plt.xlabel(\"Training Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix for Original 5-Class Classification\")\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "\n",
    "y_test_coarse = pd.Series(y_test).replace({1: 0, 2: 0, 4: 1, 5: 1})\n",
    "y_pred_coarse = pd.Series(predictions).replace({1: 0, 2: 0, 4: 1, 5: 1})\n",
    "\n",
    "valid_indices = y_test_coarse != 3\n",
    "y_test_coarse = y_test_coarse[valid_indices]\n",
    "y_pred_coarse = y_pred_coarse[valid_indices]\n",
    "\n",
    "conf_matrix_coarse = confusion_matrix(y_test_coarse, y_pred_coarse)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_coarse, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Bad', 'Good'], yticklabels=['Bad', 'Good'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix for Coarse Classification (Good vs. Bad)\")\n",
    "plt.show()\n",
    "\n",
    "accuracy_coarse = accuracy_score(y_test_coarse, y_pred_coarse)\n",
    "print(f\"Accuracy for Coarse Classification (Good vs. Bad): {accuracy_coarse:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▌         | 1/20 [00:02<00:56,  2.99s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.71822 | test_accuracy: 0.26525 |  0:00:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|█         | 2/20 [00:05<00:52,  2.93s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  | loss: 1.56225 | test_accuracy: 0.2793  |  0:00:05s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  15%|█▌        | 3/20 [00:08<00:48,  2.86s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2  | loss: 1.55346 | test_accuracy: 0.28375 |  0:00:08s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  20%|██        | 4/20 [00:11<00:46,  2.91s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 1.54486 | test_accuracy: 0.29089 |  0:00:11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  25%|██▌       | 5/20 [00:14<00:43,  2.93s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4  | loss: 1.54303 | test_accuracy: 0.27677 |  0:00:14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  30%|███       | 6/20 [00:17<00:42,  3.03s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5  | loss: 1.54372 | test_accuracy: 0.28206 |  0:00:17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  35%|███▌      | 7/20 [00:21<00:42,  3.27s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6  | loss: 1.53506 | test_accuracy: 0.29695 |  0:00:21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  40%|████      | 8/20 [00:24<00:38,  3.22s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7  | loss: 1.53785 | test_accuracy: 0.29619 |  0:00:24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  45%|████▌     | 9/20 [00:28<00:35,  3.26s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8  | loss: 1.53359 | test_accuracy: 0.30148 |  0:00:28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  50%|█████     | 10/20 [00:30<00:31,  3.15s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9  | loss: 1.53066 | test_accuracy: 0.30125 |  0:00:30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  55%|█████▌    | 11/20 [00:33<00:27,  3.11s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 | loss: 1.53187 | test_accuracy: 0.30271 |  0:00:33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  60%|██████    | 12/20 [00:36<00:24,  3.06s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 | loss: 1.52669 | test_accuracy: 0.30294 |  0:00:36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  65%|██████▌   | 13/20 [00:39<00:21,  3.02s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 | loss: 1.52034 | test_accuracy: 0.30041 |  0:00:39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  70%|███████   | 14/20 [00:42<00:18,  3.00s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 | loss: 1.51838 | test_accuracy: 0.31768 |  0:00:42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  75%|███████▌  | 15/20 [00:45<00:14,  2.99s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 | loss: 1.51467 | test_accuracy: 0.31583 |  0:00:45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  80%|████████  | 16/20 [00:48<00:11,  2.98s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 | loss: 1.50874 | test_accuracy: 0.31898 |  0:00:48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  85%|████████▌ | 17/20 [00:51<00:08,  2.99s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 | loss: 1.50878 | test_accuracy: 0.3123  |  0:00:51s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  90%|█████████ | 18/20 [00:54<00:05,  3.00s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 | loss: 1.5053  | test_accuracy: 0.32228 |  0:00:54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  95%|█████████▌| 19/20 [00:57<00:03,  3.03s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 | loss: 1.50399 | test_accuracy: 0.31246 |  0:00:57s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 20/20 [01:00<00:00,  3.00s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 | loss: 1.50226 | test_accuracy: 0.32274 |  0:01:00s\n",
      "Stop training because you reached max_epochs = 20 with best_epoch = 19 and best_test_accuracy = 0.32274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 20/20 [01:02<00:00,  3.12s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Actual  Predicted\n",
      "0     3.0        4.0\n",
      "1     5.0        1.0\n",
      "2     3.0        4.0\n",
      "3     3.0        1.0\n",
      "4     1.0        1.0\n",
      "Loss history: [1.7182206094781933, 1.5622458429857702, 1.5534587107814368, 1.5448605576537964, 1.5430259346568544, 1.543722166598955, 1.5350603340994928, 1.537853211496145, 1.533590649175062, 1.5306621248096797, 1.531867857904937, 1.526687248348412, 1.5203382523045754, 1.5183771653153029, 1.514666217095708, 1.5087380025875212, 1.5087820130212861, 1.505301037756258, 1.503988107942146, 1.502259922161171]\n",
      "epoch 0  | loss: 3.12347 | test_accuracy: 0.19334 |  0:00:00s\n",
      "epoch 1  | loss: 2.89884 | test_accuracy: 0.19265 |  0:00:00s\n",
      "epoch 2  | loss: 2.53188 | test_accuracy: 0.19756 |  0:00:01s\n",
      "epoch 3  | loss: 2.22899 | test_accuracy: 0.20009 |  0:00:01s\n",
      "epoch 4  | loss: 2.00098 | test_accuracy: 0.20232 |  0:00:01s\n",
      "epoch 5  | loss: 1.91625 | test_accuracy: 0.20508 |  0:00:02s\n",
      "epoch 6  | loss: 1.88891 | test_accuracy: 0.208   |  0:00:02s\n",
      "epoch 7  | loss: 1.7633  | test_accuracy: 0.20493 |  0:00:03s\n",
      "epoch 8  | loss: 1.73149 | test_accuracy: 0.20255 |  0:00:03s\n",
      "epoch 9  | loss: 1.70972 | test_accuracy: 0.20861 |  0:00:03s\n",
      "epoch 10 | loss: 1.67034 | test_accuracy: 0.21207 |  0:00:04s\n",
      "epoch 11 | loss: 1.62688 | test_accuracy: 0.20823 |  0:00:04s\n",
      "epoch 12 | loss: 1.61104 | test_accuracy: 0.20554 |  0:00:04s\n",
      "epoch 13 | loss: 1.60093 | test_accuracy: 0.20286 |  0:00:05s\n",
      "epoch 14 | loss: 1.68607 | test_accuracy: 0.20593 |  0:00:05s\n",
      "epoch 15 | loss: 1.61824 | test_accuracy: 0.20784 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 10 and best_test_accuracy = 0.21207\n",
      "epoch 0  | loss: 2.38376 | test_accuracy: 0.21406 |  0:00:00s\n",
      "epoch 1  | loss: 1.77332 | test_accuracy: 0.21207 |  0:00:01s\n",
      "epoch 2  | loss: 1.6518  | test_accuracy: 0.23954 |  0:00:01s\n",
      "epoch 3  | loss: 1.61203 | test_accuracy: 0.25006 |  0:00:02s\n",
      "epoch 4  | loss: 1.59324 | test_accuracy: 0.25988 |  0:00:03s\n",
      "epoch 5  | loss: 1.58274 | test_accuracy: 0.26065 |  0:00:03s\n",
      "epoch 6  | loss: 1.58122 | test_accuracy: 0.2572  |  0:00:04s\n",
      "epoch 7  | loss: 1.57441 | test_accuracy: 0.26395 |  0:00:04s\n",
      "epoch 8  | loss: 1.56676 | test_accuracy: 0.27078 |  0:00:05s\n",
      "epoch 9  | loss: 1.56235 | test_accuracy: 0.27278 |  0:00:06s\n",
      "epoch 10 | loss: 1.55946 | test_accuracy: 0.27055 |  0:00:06s\n",
      "epoch 11 | loss: 1.56538 | test_accuracy: 0.26694 |  0:00:07s\n",
      "epoch 12 | loss: 1.56435 | test_accuracy: 0.26932 |  0:00:08s\n",
      "epoch 13 | loss: 1.55828 | test_accuracy: 0.27285 |  0:00:09s\n",
      "epoch 14 | loss: 1.55522 | test_accuracy: 0.27101 |  0:00:09s\n",
      "epoch 15 | loss: 1.55486 | test_accuracy: 0.27347 |  0:00:10s\n",
      "epoch 16 | loss: 1.55708 | test_accuracy: 0.27646 |  0:00:10s\n",
      "epoch 17 | loss: 1.55418 | test_accuracy: 0.27738 |  0:00:11s\n",
      "epoch 18 | loss: 1.5548  | test_accuracy: 0.27984 |  0:00:12s\n",
      "epoch 19 | loss: 1.55147 | test_accuracy: 0.28114 |  0:00:12s\n",
      "Stop training because you reached max_epochs = 20 with best_epoch = 19 and best_test_accuracy = 0.28114\n",
      "epoch 0  | loss: 1.85673 | test_accuracy: 0.24323 |  0:00:01s\n",
      "epoch 1  | loss: 1.5949  | test_accuracy: 0.25597 |  0:00:03s\n",
      "epoch 2  | loss: 1.57074 | test_accuracy: 0.26257 |  0:00:04s\n",
      "epoch 3  | loss: 1.56419 | test_accuracy: 0.26932 |  0:00:06s\n",
      "epoch 4  | loss: 1.55759 | test_accuracy: 0.28014 |  0:00:08s\n",
      "epoch 5  | loss: 1.55405 | test_accuracy: 0.2816  |  0:00:09s\n",
      "epoch 6  | loss: 1.54961 | test_accuracy: 0.28437 |  0:00:11s\n",
      "epoch 7  | loss: 1.54606 | test_accuracy: 0.29227 |  0:00:13s\n",
      "epoch 8  | loss: 1.54234 | test_accuracy: 0.29496 |  0:00:14s\n",
      "epoch 9  | loss: 1.54146 | test_accuracy: 0.29465 |  0:00:16s\n",
      "epoch 10 | loss: 1.53765 | test_accuracy: 0.30148 |  0:00:18s\n",
      "epoch 11 | loss: 1.53861 | test_accuracy: 0.30163 |  0:00:19s\n",
      "epoch 12 | loss: 1.53303 | test_accuracy: 0.30079 |  0:00:21s\n",
      "epoch 13 | loss: 1.53297 | test_accuracy: 0.29734 |  0:00:23s\n",
      "epoch 14 | loss: 1.53163 | test_accuracy: 0.29772 |  0:00:24s\n",
      "epoch 15 | loss: 1.53096 | test_accuracy: 0.29542 |  0:00:26s\n",
      "epoch 16 | loss: 1.5295  | test_accuracy: 0.30286 |  0:00:28s\n",
      "epoch 17 | loss: 1.52789 | test_accuracy: 0.29803 |  0:00:29s\n",
      "epoch 18 | loss: 1.52929 | test_accuracy: 0.29941 |  0:00:31s\n",
      "epoch 19 | loss: 1.5311  | test_accuracy: 0.29856 |  0:00:32s\n",
      "Stop training because you reached max_epochs = 20 with best_epoch = 16 and best_test_accuracy = 0.30286\n",
      "epoch 0  | loss: 1.71922 | test_accuracy: 0.24545 |  0:00:02s\n",
      "epoch 1  | loss: 1.56746 | test_accuracy: 0.27186 |  0:00:06s\n",
      "epoch 2  | loss: 1.55605 | test_accuracy: 0.27823 |  0:00:09s\n",
      "epoch 3  | loss: 1.55293 | test_accuracy: 0.28306 |  0:00:12s\n",
      "epoch 4  | loss: 1.54855 | test_accuracy: 0.28344 |  0:00:15s\n",
      "epoch 5  | loss: 1.54676 | test_accuracy: 0.28797 |  0:00:18s\n",
      "epoch 6  | loss: 1.544   | test_accuracy: 0.28153 |  0:00:21s\n",
      "epoch 7  | loss: 1.54176 | test_accuracy: 0.28982 |  0:00:24s\n",
      "epoch 8  | loss: 1.53823 | test_accuracy: 0.28805 |  0:00:26s\n",
      "epoch 9  | loss: 1.5372  | test_accuracy: 0.29196 |  0:00:29s\n",
      "epoch 10 | loss: 1.53587 | test_accuracy: 0.29741 |  0:00:32s\n",
      "epoch 11 | loss: 1.53516 | test_accuracy: 0.29104 |  0:00:35s\n",
      "epoch 12 | loss: 1.53459 | test_accuracy: 0.29588 |  0:00:38s\n",
      "epoch 13 | loss: 1.53111 | test_accuracy: 0.3054  |  0:00:41s\n",
      "epoch 14 | loss: 1.52994 | test_accuracy: 0.30002 |  0:00:44s\n",
      "epoch 15 | loss: 1.52827 | test_accuracy: 0.30708 |  0:00:47s\n",
      "epoch 16 | loss: 1.52766 | test_accuracy: 0.29849 |  0:00:50s\n",
      "epoch 17 | loss: 1.52684 | test_accuracy: 0.30125 |  0:00:53s\n",
      "epoch 18 | loss: 1.52472 | test_accuracy: 0.30724 |  0:00:56s\n",
      "epoch 19 | loss: 1.52327 | test_accuracy: 0.30916 |  0:00:59s\n",
      "Stop training because you reached max_epochs = 20 with best_epoch = 19 and best_test_accuracy = 0.30916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.43      0.47      0.45      2614\n",
      "         2.0       0.29      0.24      0.26      2643\n",
      "         3.0       0.23      0.16      0.19      2553\n",
      "         4.0       0.26      0.31      0.28      2643\n",
      "         5.0       0.37      0.43      0.39      2576\n",
      "\n",
      "    accuracy                           0.32     13029\n",
      "   macro avg       0.31      0.32      0.32     13029\n",
      "weighted avg       0.32      0.32      0.32     13029\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Coarse Classification (Good vs. Bad): 0.6107\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:36:35.658098Z",
     "start_time": "2024-10-07T14:36:35.651586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_score\n",
      "4    0.575521\n",
      "3    0.192689\n",
      "0    0.113655\n",
      "2    0.084219\n",
      "1    0.033916\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": "\n"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
