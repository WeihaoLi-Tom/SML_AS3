{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:30.886693Z",
     "start_time": "2024-10-07T14:21:28.756910Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Af9TkeJYmtE1",
    "outputId": "e5df38f2-683e-45af-f058-a6b19722213b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n",
      "Number of available GPUs: 1\n",
      "GPU 0: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n",
      "Current GPU device: 0\n"
     ]
    }
   ],
   "source": [
    "# NVIDIA CUDA\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "    print(f\"Current GPU device: {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:31.370248Z",
     "start_time": "2024-10-07T14:21:31.366249Z"
    },
    "id": "2HQxZxkvm5bz"
   },
   "outputs": [],
   "source": [
    "# here for libs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:32.309758Z",
     "start_time": "2024-10-07T14:21:31.380835Z"
    },
    "id": "uSd37dItm9Tc"
   },
   "outputs": [],
   "source": [
    "# Load individual datasets\n",
    "olist_customers_df = pd.read_csv('./data/olist_customers_dataset.csv')\n",
    "order_items_df = pd.read_csv('./data/olist_order_items_dataset.csv')\n",
    "order_payments_df = pd.read_csv('./data/olist_order_payments_dataset.csv')\n",
    "order_reviews_df = pd.read_csv('./data/olist_order_reviews_dataset.csv')\n",
    "olist_orders_df = pd.read_csv('./data/olist_orders_dataset.csv')\n",
    "sellers_df = pd.read_csv('./data/olist_sellers_dataset.csv')\n",
    "products_df = pd.read_csv('./data/olist_products_dataset.csv')\n",
    "product_category_name_translation_df = pd.read_csv('./data/product_category_name_translation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:32.348238Z",
     "start_time": "2024-10-07T14:21:32.337991Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Aji1QZ1CoOK1",
    "outputId": "d7c60f4e-9eff-45cd-ed05-9a4e3ae7b219"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_unique_id</th>\n",
       "      <th>customer_zip_code_prefix</th>\n",
       "      <th>customer_city</th>\n",
       "      <th>customer_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>06b8999e2fba1a1fbc88172c00ba8bc7</td>\n",
       "      <td>861eff4711a542e4b93843c6dd7febb0</td>\n",
       "      <td>14409</td>\n",
       "      <td>franca</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18955e83d337fd6b2def6b18a428ac77</td>\n",
       "      <td>290c77bc529b7ac935b93aa66c333dc3</td>\n",
       "      <td>9790</td>\n",
       "      <td>sao bernardo do campo</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4e7b3e00288586ebd08712fdd0374a03</td>\n",
       "      <td>060e732b5b29e8181a18229c7b0b2b5e</td>\n",
       "      <td>1151</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b2b6027bc5c5109e529d4dc6358b12c3</td>\n",
       "      <td>259dac757896d24d7702b9acbbff3f3c</td>\n",
       "      <td>8775</td>\n",
       "      <td>mogi das cruzes</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4f2d8ab171c80ec8364f7c12e35b23ad</td>\n",
       "      <td>345ecd01c38d18a9036ed96c73b8d066</td>\n",
       "      <td>13056</td>\n",
       "      <td>campinas</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        customer_id                customer_unique_id  \\\n",
       "0  06b8999e2fba1a1fbc88172c00ba8bc7  861eff4711a542e4b93843c6dd7febb0   \n",
       "1  18955e83d337fd6b2def6b18a428ac77  290c77bc529b7ac935b93aa66c333dc3   \n",
       "2  4e7b3e00288586ebd08712fdd0374a03  060e732b5b29e8181a18229c7b0b2b5e   \n",
       "3  b2b6027bc5c5109e529d4dc6358b12c3  259dac757896d24d7702b9acbbff3f3c   \n",
       "4  4f2d8ab171c80ec8364f7c12e35b23ad  345ecd01c38d18a9036ed96c73b8d066   \n",
       "\n",
       "   customer_zip_code_prefix          customer_city customer_state  \n",
       "0                     14409                 franca             SP  \n",
       "1                      9790  sao bernardo do campo             SP  \n",
       "2                      1151              sao paulo             SP  \n",
       "3                      8775        mogi das cruzes             SP  \n",
       "4                     13056               campinas             SP  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "olist_customers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:32.407338Z",
     "start_time": "2024-10-07T14:21:32.399787Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "0ATiGySQoP4w",
    "outputId": "1d53f2b0-4297-436b-f007-c232248a5782"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>shipping_limit_date</th>\n",
       "      <th>price</th>\n",
       "      <th>freight_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00010242fe8c5a6d1ba2dd792cb16214</td>\n",
       "      <td>1</td>\n",
       "      <td>4244733e06e7ecb4970a6e2683c13e61</td>\n",
       "      <td>48436dade18ac8b2bce089ec2a041202</td>\n",
       "      <td>2017-09-19 09:45:35</td>\n",
       "      <td>58.90</td>\n",
       "      <td>13.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00018f77f2f0320c557190d7a144bdd3</td>\n",
       "      <td>1</td>\n",
       "      <td>e5f2d52b802189ee658865ca93d83a8f</td>\n",
       "      <td>dd7ddc04e1b6c2c614352b383efe2d36</td>\n",
       "      <td>2017-05-03 11:05:13</td>\n",
       "      <td>239.90</td>\n",
       "      <td>19.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000229ec398224ef6ca0657da4fc703e</td>\n",
       "      <td>1</td>\n",
       "      <td>c777355d18b72b67abbeef9df44fd0fd</td>\n",
       "      <td>5b51032eddd242adc84c38acab88f23d</td>\n",
       "      <td>2018-01-18 14:48:30</td>\n",
       "      <td>199.00</td>\n",
       "      <td>17.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00024acbcdf0a6daa1e931b038114c75</td>\n",
       "      <td>1</td>\n",
       "      <td>7634da152a4610f1595efa32f14722fc</td>\n",
       "      <td>9d7a1d34a5052409006425275ba1c2b4</td>\n",
       "      <td>2018-08-15 10:10:18</td>\n",
       "      <td>12.99</td>\n",
       "      <td>12.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00042b26cf59d7ce69dfabb4e55b4fd9</td>\n",
       "      <td>1</td>\n",
       "      <td>ac6c3623068f30de03045865e4e10089</td>\n",
       "      <td>df560393f3a51e74553ab94004ba5c87</td>\n",
       "      <td>2017-02-13 13:57:51</td>\n",
       "      <td>199.90</td>\n",
       "      <td>18.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id  order_item_id  \\\n",
       "0  00010242fe8c5a6d1ba2dd792cb16214              1   \n",
       "1  00018f77f2f0320c557190d7a144bdd3              1   \n",
       "2  000229ec398224ef6ca0657da4fc703e              1   \n",
       "3  00024acbcdf0a6daa1e931b038114c75              1   \n",
       "4  00042b26cf59d7ce69dfabb4e55b4fd9              1   \n",
       "\n",
       "                         product_id                         seller_id  \\\n",
       "0  4244733e06e7ecb4970a6e2683c13e61  48436dade18ac8b2bce089ec2a041202   \n",
       "1  e5f2d52b802189ee658865ca93d83a8f  dd7ddc04e1b6c2c614352b383efe2d36   \n",
       "2  c777355d18b72b67abbeef9df44fd0fd  5b51032eddd242adc84c38acab88f23d   \n",
       "3  7634da152a4610f1595efa32f14722fc  9d7a1d34a5052409006425275ba1c2b4   \n",
       "4  ac6c3623068f30de03045865e4e10089  df560393f3a51e74553ab94004ba5c87   \n",
       "\n",
       "   shipping_limit_date   price  freight_value  \n",
       "0  2017-09-19 09:45:35   58.90          13.29  \n",
       "1  2017-05-03 11:05:13  239.90          19.93  \n",
       "2  2018-01-18 14:48:30  199.00          17.87  \n",
       "3  2018-08-15 10:10:18   12.99          12.79  \n",
       "4  2017-02-13 13:57:51  199.90          18.14  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_items_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:32.513122Z",
     "start_time": "2024-10-07T14:21:32.504934Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "RxmEcHjRoPwR",
    "outputId": "b95a2caf-19dd-4ff6-ba19-5b89e28078cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>payment_sequential</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>payment_installments</th>\n",
       "      <th>payment_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b81ef226f3fe1789b1e8b2acac839d17</td>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>8</td>\n",
       "      <td>99.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a9810da82917af2d9aefd1278f1dcfa0</td>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>1</td>\n",
       "      <td>24.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25e8ea4e93396b6fa0d3dd708e76c1bd</td>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>1</td>\n",
       "      <td>65.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ba78997921bbcdc1373bb41e913ab953</td>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>8</td>\n",
       "      <td>107.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42fdf880ba16b47b59251dd489d4441a</td>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>2</td>\n",
       "      <td>128.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id  payment_sequential payment_type  \\\n",
       "0  b81ef226f3fe1789b1e8b2acac839d17                   1  credit_card   \n",
       "1  a9810da82917af2d9aefd1278f1dcfa0                   1  credit_card   \n",
       "2  25e8ea4e93396b6fa0d3dd708e76c1bd                   1  credit_card   \n",
       "3  ba78997921bbcdc1373bb41e913ab953                   1  credit_card   \n",
       "4  42fdf880ba16b47b59251dd489d4441a                   1  credit_card   \n",
       "\n",
       "   payment_installments  payment_value  \n",
       "0                     8          99.33  \n",
       "1                     1          24.39  \n",
       "2                     1          65.71  \n",
       "3                     8         107.78  \n",
       "4                     2         128.45  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_payments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:32.853979Z",
     "start_time": "2024-10-07T14:21:32.845125Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "id": "3p6g6V0C5J2V",
    "outputId": "2732d716-51c2-4f3d-aff2-b4e0eea7402e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_comment_title</th>\n",
       "      <th>review_comment_message</th>\n",
       "      <th>review_creation_date</th>\n",
       "      <th>review_answer_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7bc2406110b926393aa56f80a40eba40</td>\n",
       "      <td>73fc7af87114b39712e6da79b0a377eb</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-18 00:00:00</td>\n",
       "      <td>2018-01-18 21:46:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80e641a11e56f04c1ad469d5645fdfde</td>\n",
       "      <td>a548910a1c6147796b98fdf73dbeba33</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-03-10 00:00:00</td>\n",
       "      <td>2018-03-11 03:05:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>228ce5500dc1d8e020d8d1322874b6f0</td>\n",
       "      <td>f9e4b658b201a9f2ecdecbb34bed034b</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-02-17 00:00:00</td>\n",
       "      <td>2018-02-18 14:36:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e64fb393e7b32834bb789ff8bb30750e</td>\n",
       "      <td>658677c97b385a9be170737859d3511b</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Recebi bem antes do prazo estipulado.</td>\n",
       "      <td>2017-04-21 00:00:00</td>\n",
       "      <td>2017-04-21 22:02:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f7c4243c7fe1938f181bec41a392bdeb</td>\n",
       "      <td>8e6bfb81e283fa7e4f11123a3fb894f1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parabéns lojas lannister adorei comprar pela I...</td>\n",
       "      <td>2018-03-01 00:00:00</td>\n",
       "      <td>2018-03-02 10:26:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          review_id                          order_id  \\\n",
       "0  7bc2406110b926393aa56f80a40eba40  73fc7af87114b39712e6da79b0a377eb   \n",
       "1  80e641a11e56f04c1ad469d5645fdfde  a548910a1c6147796b98fdf73dbeba33   \n",
       "2  228ce5500dc1d8e020d8d1322874b6f0  f9e4b658b201a9f2ecdecbb34bed034b   \n",
       "3  e64fb393e7b32834bb789ff8bb30750e  658677c97b385a9be170737859d3511b   \n",
       "4  f7c4243c7fe1938f181bec41a392bdeb  8e6bfb81e283fa7e4f11123a3fb894f1   \n",
       "\n",
       "   review_score review_comment_title  \\\n",
       "0             4                  NaN   \n",
       "1             5                  NaN   \n",
       "2             5                  NaN   \n",
       "3             5                  NaN   \n",
       "4             5                  NaN   \n",
       "\n",
       "                              review_comment_message review_creation_date  \\\n",
       "0                                                NaN  2018-01-18 00:00:00   \n",
       "1                                                NaN  2018-03-10 00:00:00   \n",
       "2                                                NaN  2018-02-17 00:00:00   \n",
       "3              Recebi bem antes do prazo estipulado.  2017-04-21 00:00:00   \n",
       "4  Parabéns lojas lannister adorei comprar pela I...  2018-03-01 00:00:00   \n",
       "\n",
       "  review_answer_timestamp  \n",
       "0     2018-01-18 21:46:59  \n",
       "1     2018-03-11 03:05:13  \n",
       "2     2018-02-18 14:36:24  \n",
       "3     2017-04-21 22:02:06  \n",
       "4     2018-03-02 10:26:53  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:33.213122Z",
     "start_time": "2024-10-07T14:21:33.204367Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "nmOFoku25JzK",
    "outputId": "2502bd23-ae9d-4ebf-cb91-3ad2605a4148"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>order_approved_at</th>\n",
       "      <th>order_delivered_carrier_date</th>\n",
       "      <th>order_delivered_customer_date</th>\n",
       "      <th>order_estimated_delivery_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53cdb2fc8bc7dce0b6741e2150273451</td>\n",
       "      <td>b0830fb4747a6c6d20dea0b8c802d7ef</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-07-24 20:41:37</td>\n",
       "      <td>2018-07-26 03:24:27</td>\n",
       "      <td>2018-07-26 14:31:00</td>\n",
       "      <td>2018-08-07 15:27:45</td>\n",
       "      <td>2018-08-13 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47770eb9100c2d0c44946d9cf07ec65d</td>\n",
       "      <td>41ce2a54c0b03bf3443c3d931a367089</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-08-08 08:38:49</td>\n",
       "      <td>2018-08-08 08:55:23</td>\n",
       "      <td>2018-08-08 13:50:00</td>\n",
       "      <td>2018-08-17 18:06:29</td>\n",
       "      <td>2018-09-04 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>949d5b44dbf5de918fe9c16f97b45f8a</td>\n",
       "      <td>f88197465ea7920adcdbec7375364d82</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-11-18 19:28:06</td>\n",
       "      <td>2017-11-18 19:45:59</td>\n",
       "      <td>2017-11-22 13:39:59</td>\n",
       "      <td>2017-12-02 00:28:42</td>\n",
       "      <td>2017-12-15 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ad21c59c0840e6cb83a9ceb5573f8159</td>\n",
       "      <td>8ab97904e6daea8866dbdbc4fb7aad2c</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-02-13 21:18:39</td>\n",
       "      <td>2018-02-13 22:20:29</td>\n",
       "      <td>2018-02-14 19:46:34</td>\n",
       "      <td>2018-02-16 18:17:02</td>\n",
       "      <td>2018-02-26 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id                       customer_id  \\\n",
       "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "1  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
       "2  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
       "3  949d5b44dbf5de918fe9c16f97b45f8a  f88197465ea7920adcdbec7375364d82   \n",
       "4  ad21c59c0840e6cb83a9ceb5573f8159  8ab97904e6daea8866dbdbc4fb7aad2c   \n",
       "\n",
       "  order_status order_purchase_timestamp    order_approved_at  \\\n",
       "0    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
       "1    delivered      2018-07-24 20:41:37  2018-07-26 03:24:27   \n",
       "2    delivered      2018-08-08 08:38:49  2018-08-08 08:55:23   \n",
       "3    delivered      2017-11-18 19:28:06  2017-11-18 19:45:59   \n",
       "4    delivered      2018-02-13 21:18:39  2018-02-13 22:20:29   \n",
       "\n",
       "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
       "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "1          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
       "2          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
       "3          2017-11-22 13:39:59           2017-12-02 00:28:42   \n",
       "4          2018-02-14 19:46:34           2018-02-16 18:17:02   \n",
       "\n",
       "  order_estimated_delivery_date  \n",
       "0           2017-10-18 00:00:00  \n",
       "1           2018-08-13 00:00:00  \n",
       "2           2018-09-04 00:00:00  \n",
       "3           2017-12-15 00:00:00  \n",
       "4           2018-02-26 00:00:00  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "olist_orders_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:33.637305Z",
     "start_time": "2024-10-07T14:21:33.629798Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "hg9LVjUr5Jwh",
    "outputId": "68cdd701-69de-42ff-ccd4-15886d89d923"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seller_id</th>\n",
       "      <th>seller_zip_code_prefix</th>\n",
       "      <th>seller_city</th>\n",
       "      <th>seller_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3442f8959a84dea7ee197c632cb2df15</td>\n",
       "      <td>13023</td>\n",
       "      <td>campinas</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d1b65fc7debc3361ea86b5f14c68d2e2</td>\n",
       "      <td>13844</td>\n",
       "      <td>mogi guacu</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ce3ad9de960102d0677a81f5d0bb7b2d</td>\n",
       "      <td>20031</td>\n",
       "      <td>rio de janeiro</td>\n",
       "      <td>RJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c0f3eea2e14555b6faeea3dd58c1b1c3</td>\n",
       "      <td>4195</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51a04a8a6bdcb23deccc82b0b80742cf</td>\n",
       "      <td>12914</td>\n",
       "      <td>braganca paulista</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          seller_id  seller_zip_code_prefix  \\\n",
       "0  3442f8959a84dea7ee197c632cb2df15                   13023   \n",
       "1  d1b65fc7debc3361ea86b5f14c68d2e2                   13844   \n",
       "2  ce3ad9de960102d0677a81f5d0bb7b2d                   20031   \n",
       "3  c0f3eea2e14555b6faeea3dd58c1b1c3                    4195   \n",
       "4  51a04a8a6bdcb23deccc82b0b80742cf                   12914   \n",
       "\n",
       "         seller_city seller_state  \n",
       "0           campinas           SP  \n",
       "1         mogi guacu           SP  \n",
       "2     rio de janeiro           RJ  \n",
       "3          sao paulo           SP  \n",
       "4  braganca paulista           SP  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sellers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:34.060687Z",
     "start_time": "2024-10-07T14:21:34.048133Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "gAKZCAPj5OGk",
    "outputId": "083c69de-6c5a-4e09-fd19-fc7e9f6d1955"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_category_name</th>\n",
       "      <th>product_name_lenght</th>\n",
       "      <th>product_description_lenght</th>\n",
       "      <th>product_photos_qty</th>\n",
       "      <th>product_weight_g</th>\n",
       "      <th>product_length_cm</th>\n",
       "      <th>product_height_cm</th>\n",
       "      <th>product_width_cm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1e9e8ef04dbcff4541ed26657ea517e5</td>\n",
       "      <td>perfumaria</td>\n",
       "      <td>40.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3aa071139cb16b67ca9e5dea641aaa2f</td>\n",
       "      <td>artes</td>\n",
       "      <td>44.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96bd76ec8810374ed1b65e291975717f</td>\n",
       "      <td>esporte_lazer</td>\n",
       "      <td>46.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cef67bcfe19066a932b7673e239eb23d</td>\n",
       "      <td>bebes</td>\n",
       "      <td>27.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9dc1a7de274444849c219cff195d0b71</td>\n",
       "      <td>utilidades_domesticas</td>\n",
       "      <td>37.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>625.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         product_id  product_category_name  \\\n",
       "0  1e9e8ef04dbcff4541ed26657ea517e5             perfumaria   \n",
       "1  3aa071139cb16b67ca9e5dea641aaa2f                  artes   \n",
       "2  96bd76ec8810374ed1b65e291975717f          esporte_lazer   \n",
       "3  cef67bcfe19066a932b7673e239eb23d                  bebes   \n",
       "4  9dc1a7de274444849c219cff195d0b71  utilidades_domesticas   \n",
       "\n",
       "   product_name_lenght  product_description_lenght  product_photos_qty  \\\n",
       "0                 40.0                       287.0                 1.0   \n",
       "1                 44.0                       276.0                 1.0   \n",
       "2                 46.0                       250.0                 1.0   \n",
       "3                 27.0                       261.0                 1.0   \n",
       "4                 37.0                       402.0                 4.0   \n",
       "\n",
       "   product_weight_g  product_length_cm  product_height_cm  product_width_cm  \n",
       "0             225.0               16.0               10.0              14.0  \n",
       "1            1000.0               30.0               18.0              20.0  \n",
       "2             154.0               18.0                9.0              15.0  \n",
       "3             371.0               26.0                4.0              26.0  \n",
       "4             625.0               20.0               17.0              13.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:34.492398Z",
     "start_time": "2024-10-07T14:21:34.484338Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "tpBlW2FK5ODg",
    "outputId": "38750f33-5de0-4ed5-95c0-8d851d312628"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_category_name</th>\n",
       "      <th>product_category_name_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beleza_saude</td>\n",
       "      <td>health_beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>informatica_acessorios</td>\n",
       "      <td>computers_accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>automotivo</td>\n",
       "      <td>auto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cama_mesa_banho</td>\n",
       "      <td>bed_bath_table</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>moveis_decoracao</td>\n",
       "      <td>furniture_decor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    product_category_name product_category_name_english\n",
       "0            beleza_saude                 health_beauty\n",
       "1  informatica_acessorios         computers_accessories\n",
       "2              automotivo                          auto\n",
       "3         cama_mesa_banho                bed_bath_table\n",
       "4        moveis_decoracao               furniture_decor"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_category_name_translation_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDrTdi375o-v"
   },
   "source": [
    "#### Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:34.922670Z",
     "start_time": "2024-10-07T14:21:34.840924Z"
    },
    "id": "QRf0a1Wm5OAk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 119143 entries, 0 to 119142\n",
      "Data columns (total 40 columns):\n",
      " #   Column                         Non-Null Count   Dtype  \n",
      "---  ------                         --------------   -----  \n",
      " 0   order_id                       119143 non-null  object \n",
      " 1   customer_id                    119143 non-null  object \n",
      " 2   order_status                   119143 non-null  object \n",
      " 3   order_purchase_timestamp       119143 non-null  object \n",
      " 4   order_approved_at              118966 non-null  object \n",
      " 5   order_delivered_carrier_date   117057 non-null  object \n",
      " 6   order_delivered_customer_date  115722 non-null  object \n",
      " 7   order_estimated_delivery_date  119143 non-null  object \n",
      " 8   customer_unique_id             119143 non-null  object \n",
      " 9   customer_zip_code_prefix       119143 non-null  int64  \n",
      " 10  customer_city                  119143 non-null  object \n",
      " 11  customer_state                 119143 non-null  object \n",
      " 12  seller_id                      118310 non-null  object \n",
      " 13  seller_zip_code_prefix         118310 non-null  float64\n",
      " 14  seller_city                    118310 non-null  object \n",
      " 15  seller_state                   118310 non-null  object \n",
      " 16  order_item_id                  118310 non-null  float64\n",
      " 17  product_id                     118310 non-null  object \n",
      " 18  shipping_limit_date            118310 non-null  object \n",
      " 19  price                          118310 non-null  float64\n",
      " 20  freight_value                  118310 non-null  float64\n",
      " 21  product_category_name          116601 non-null  object \n",
      " 22  product_name_lenght            116601 non-null  float64\n",
      " 23  product_description_lenght     116601 non-null  float64\n",
      " 24  product_photos_qty             116601 non-null  float64\n",
      " 25  product_weight_g               118290 non-null  float64\n",
      " 26  product_length_cm              118290 non-null  float64\n",
      " 27  product_height_cm              118290 non-null  float64\n",
      " 28  product_width_cm               118290 non-null  float64\n",
      " 29  product_category_name_english  116576 non-null  object \n",
      " 30  payment_sequential             119140 non-null  float64\n",
      " 31  payment_type                   119140 non-null  object \n",
      " 32  payment_installments           119140 non-null  float64\n",
      " 33  payment_value                  119140 non-null  float64\n",
      " 34  review_id                      118146 non-null  object \n",
      " 35  review_score                   118146 non-null  float64\n",
      " 36  review_comment_title           13989 non-null   object \n",
      " 37  review_comment_message         50245 non-null   object \n",
      " 38  review_creation_date           118146 non-null  object \n",
      " 39  review_answer_timestamp        118146 non-null  object \n",
      "dtypes: float64(15), int64(1), object(24)\n",
      "memory usage: 36.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# marging datasets\n",
    "merged_1 = pd.merge(olist_orders_df, olist_customers_df, on='customer_id', how='left')\n",
    "merged_2 = pd.merge(sellers_df, order_items_df, on='seller_id', how='left')\n",
    "merged_3 = pd.merge(products_df, product_category_name_translation_df, on = 'product_category_name', how = 'left')\n",
    "merged_4 = pd.merge(merged_2, merged_3, on='product_id', how='left')\n",
    "merged_5 = pd.merge(merged_1, merged_4, on='order_id', how='left')\n",
    "merged_6 = pd.merge(merged_5, order_payments_df, on='order_id', how='left')\n",
    "merged = pd.merge(merged_6, order_reviews_df, on='order_id', how='left')\n",
    "merged.to_csv('./data/merged_data.csv', index=False)\n",
    "merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:40.022629Z",
     "start_time": "2024-10-07T14:21:39.921593Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ms4Ryf8ZMOEP",
    "outputId": "746c13c1-48a9-42ce-9d4d-6f50d68f698c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_id                              0\n",
       "customer_id                           0\n",
       "order_status                          0\n",
       "order_purchase_timestamp              0\n",
       "order_approved_at                   177\n",
       "order_delivered_carrier_date       2086\n",
       "order_delivered_customer_date      3421\n",
       "order_estimated_delivery_date         0\n",
       "customer_unique_id                    0\n",
       "customer_zip_code_prefix              0\n",
       "customer_city                         0\n",
       "customer_state                        0\n",
       "seller_id                           833\n",
       "seller_zip_code_prefix              833\n",
       "seller_city                         833\n",
       "seller_state                        833\n",
       "order_item_id                       833\n",
       "product_id                          833\n",
       "shipping_limit_date                 833\n",
       "price                               833\n",
       "freight_value                       833\n",
       "product_category_name              2542\n",
       "product_name_lenght                2542\n",
       "product_description_lenght         2542\n",
       "product_photos_qty                 2542\n",
       "product_weight_g                    853\n",
       "product_length_cm                   853\n",
       "product_height_cm                   853\n",
       "product_width_cm                    853\n",
       "product_category_name_english      2567\n",
       "payment_sequential                    3\n",
       "payment_type                          3\n",
       "payment_installments                  3\n",
       "payment_value                         3\n",
       "review_id                           997\n",
       "review_score                        997\n",
       "review_comment_title             105154\n",
       "review_comment_message            68898\n",
       "review_creation_date                997\n",
       "review_answer_timestamp             997\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:40.504752Z",
     "start_time": "2024-10-07T14:21:40.252583Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rPiMXRJQMrnm",
    "outputId": "f1f74700-9df0-4580-bd61-537bc2ba7493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 113193 entries, 0 to 119142\n",
      "Data columns (total 38 columns):\n",
      " #   Column                         Non-Null Count   Dtype  \n",
      "---  ------                         --------------   -----  \n",
      " 0   order_id                       113193 non-null  object \n",
      " 1   customer_id                    113193 non-null  object \n",
      " 2   order_status                   113193 non-null  object \n",
      " 3   order_purchase_timestamp       113193 non-null  object \n",
      " 4   order_approved_at              113193 non-null  object \n",
      " 5   order_delivered_carrier_date   113193 non-null  object \n",
      " 6   order_delivered_customer_date  113193 non-null  object \n",
      " 7   order_estimated_delivery_date  113193 non-null  object \n",
      " 8   customer_unique_id             113193 non-null  object \n",
      " 9   customer_zip_code_prefix       113193 non-null  int64  \n",
      " 10  customer_city                  113193 non-null  object \n",
      " 11  customer_state                 113193 non-null  object \n",
      " 12  seller_id                      113193 non-null  object \n",
      " 13  seller_zip_code_prefix         113193 non-null  float64\n",
      " 14  seller_city                    113193 non-null  object \n",
      " 15  seller_state                   113193 non-null  object \n",
      " 16  order_item_id                  113193 non-null  float64\n",
      " 17  product_id                     113193 non-null  object \n",
      " 18  shipping_limit_date            113193 non-null  object \n",
      " 19  price                          113193 non-null  float64\n",
      " 20  freight_value                  113193 non-null  float64\n",
      " 21  product_category_name          113193 non-null  object \n",
      " 22  product_name_lenght            113193 non-null  float64\n",
      " 23  product_description_lenght     113193 non-null  float64\n",
      " 24  product_photos_qty             113193 non-null  float64\n",
      " 25  product_weight_g               113193 non-null  float64\n",
      " 26  product_length_cm              113193 non-null  float64\n",
      " 27  product_height_cm              113193 non-null  float64\n",
      " 28  product_width_cm               113193 non-null  float64\n",
      " 29  product_category_name_english  113193 non-null  object \n",
      " 30  payment_sequential             113193 non-null  float64\n",
      " 31  payment_type                   113193 non-null  object \n",
      " 32  payment_installments           113193 non-null  float64\n",
      " 33  payment_value                  113193 non-null  float64\n",
      " 34  review_id                      113193 non-null  object \n",
      " 35  review_score                   113193 non-null  float64\n",
      " 36  review_creation_date           113193 non-null  object \n",
      " 37  review_answer_timestamp        113193 non-null  object \n",
      "dtypes: float64(15), int64(1), object(22)\n",
      "memory usage: 33.7+ MB\n"
     ]
    }
   ],
   "source": [
    "merged = merged.drop(columns=['review_comment_title', 'review_comment_message'], axis=1)\n",
    "merged = merged.dropna()\n",
    "merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D47_wuPheJtH"
   },
   "source": [
    "#### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:41.059653Z",
     "start_time": "2024-10-07T14:21:40.537079Z"
    },
    "id": "wudCw2gBObuA"
   },
   "outputs": [],
   "source": [
    "datetime = ['order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date',\n",
    "                    'order_delivered_customer_date', 'order_estimated_delivery_date',\n",
    "                    'shipping_limit_date', 'review_creation_date', 'review_answer_timestamp']\n",
    "\n",
    "for col in datetime:\n",
    "    merged[col] = pd.to_datetime(merged[col])\n",
    "\n",
    "# Extract components of dates\n",
    "merged['purchase_day'] = merged['order_purchase_timestamp'].dt.weekday # Monday=0, Sunday=6\n",
    "merged['purchase_month'] = merged['order_purchase_timestamp'].dt.month\n",
    "\n",
    "# The total time from the purchase of the order to the actual delivery to the customer\n",
    "merged['total_delivery_time'] = (merged['order_delivered_customer_date'] - merged['order_purchase_timestamp']).dt.days\n",
    "\n",
    "# Time from order creation to payment approval\n",
    "merged['approval_time'] = (merged['order_approved_at'] - merged['order_purchase_timestamp']).dt.days\n",
    "\n",
    "# Time from logistics to customer delivery\n",
    "merged['shipping_time'] = (merged['order_delivered_customer_date'] - merged['order_delivered_carrier_date']).dt.days\n",
    "\n",
    "# Deviation between actual delivery time and estimated delivery time\n",
    "merged['delivery_estimate_deviation'] = (merged['order_estimated_delivery_date'] - merged['order_delivered_customer_date']).dt.days\n",
    "\n",
    "# Late delivery\n",
    "merged['late_delivery'] = (merged['order_delivered_customer_date'] > merged['order_estimated_delivery_date']).astype(int)\n",
    "\n",
    "# Total order item values and the total freight value\n",
    "merged['total_order_item_value'] = merged['order_item_id'] * merged['price']\n",
    "merged['total_freight_value'] = merged['order_item_id'] * merged['freight_value']\n",
    "\n",
    "# Total order freight value\n",
    "merged['total_order_freight_value'] = merged['total_order_item_value'] + merged['total_freight_value']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:41.091340Z",
     "start_time": "2024-10-07T14:21:41.080357Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6MpHbqQrHue",
    "outputId": "1bd57389-484a-42cd-a1c4-a0c6b10106f3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>order_approved_at</th>\n",
       "      <th>order_delivered_carrier_date</th>\n",
       "      <th>order_delivered_customer_date</th>\n",
       "      <th>order_estimated_delivery_date</th>\n",
       "      <th>customer_unique_id</th>\n",
       "      <th>customer_zip_code_prefix</th>\n",
       "      <th>...</th>\n",
       "      <th>purchase_day</th>\n",
       "      <th>purchase_month</th>\n",
       "      <th>total_delivery_time</th>\n",
       "      <th>approval_time</th>\n",
       "      <th>shipping_time</th>\n",
       "      <th>delivery_estimate_deviation</th>\n",
       "      <th>late_delivery</th>\n",
       "      <th>total_order_item_value</th>\n",
       "      <th>total_freight_value</th>\n",
       "      <th>total_order_freight_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>38.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>38.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>38.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53cdb2fc8bc7dce0b6741e2150273451</td>\n",
       "      <td>b0830fb4747a6c6d20dea0b8c802d7ef</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-07-24 20:41:37</td>\n",
       "      <td>2018-07-26 03:24:27</td>\n",
       "      <td>2018-07-26 14:31:00</td>\n",
       "      <td>2018-08-07 15:27:45</td>\n",
       "      <td>2018-08-13</td>\n",
       "      <td>af07308b275d755c9edb36a90c618231</td>\n",
       "      <td>47813</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>118.70</td>\n",
       "      <td>22.76</td>\n",
       "      <td>141.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47770eb9100c2d0c44946d9cf07ec65d</td>\n",
       "      <td>41ce2a54c0b03bf3443c3d931a367089</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-08-08 08:38:49</td>\n",
       "      <td>2018-08-08 08:55:23</td>\n",
       "      <td>2018-08-08 13:50:00</td>\n",
       "      <td>2018-08-17 18:06:29</td>\n",
       "      <td>2018-09-04</td>\n",
       "      <td>3a653a41f6f9fc3d2a113cf8398680e8</td>\n",
       "      <td>75265</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>159.90</td>\n",
       "      <td>19.22</td>\n",
       "      <td>179.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id                       customer_id  \\\n",
       "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "1  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "2  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "3  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
       "4  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
       "\n",
       "  order_status order_purchase_timestamp   order_approved_at  \\\n",
       "0    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15   \n",
       "1    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15   \n",
       "2    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15   \n",
       "3    delivered      2018-07-24 20:41:37 2018-07-26 03:24:27   \n",
       "4    delivered      2018-08-08 08:38:49 2018-08-08 08:55:23   \n",
       "\n",
       "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
       "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "1          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "2          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "3          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
       "4          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
       "\n",
       "  order_estimated_delivery_date                customer_unique_id  \\\n",
       "0                    2017-10-18  7c396fd4830fd04220f754e42b4e5bff   \n",
       "1                    2017-10-18  7c396fd4830fd04220f754e42b4e5bff   \n",
       "2                    2017-10-18  7c396fd4830fd04220f754e42b4e5bff   \n",
       "3                    2018-08-13  af07308b275d755c9edb36a90c618231   \n",
       "4                    2018-09-04  3a653a41f6f9fc3d2a113cf8398680e8   \n",
       "\n",
       "   customer_zip_code_prefix  ... purchase_day purchase_month  \\\n",
       "0                      3149  ...            0             10   \n",
       "1                      3149  ...            0             10   \n",
       "2                      3149  ...            0             10   \n",
       "3                     47813  ...            1              7   \n",
       "4                     75265  ...            2              8   \n",
       "\n",
       "  total_delivery_time  approval_time shipping_time  \\\n",
       "0                   8              0             6   \n",
       "1                   8              0             6   \n",
       "2                   8              0             6   \n",
       "3                  13              1            12   \n",
       "4                   9              0             9   \n",
       "\n",
       "  delivery_estimate_deviation  late_delivery total_order_item_value  \\\n",
       "0                           7              0                  29.99   \n",
       "1                           7              0                  29.99   \n",
       "2                           7              0                  29.99   \n",
       "3                           5              0                 118.70   \n",
       "4                          17              0                 159.90   \n",
       "\n",
       "  total_freight_value  total_order_freight_value  \n",
       "0                8.72                      38.71  \n",
       "1                8.72                      38.71  \n",
       "2                8.72                      38.71  \n",
       "3               22.76                     141.46  \n",
       "4               19.22                     179.12  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:41.322993Z",
     "start_time": "2024-10-07T14:21:41.291892Z"
    },
    "id": "uXcVDyEouB1b"
   },
   "outputs": [],
   "source": [
    "# Labeling\n",
    "category_to_label = {\n",
    "    'cama_mesa_banho': 0,\n",
    "    'esporte_lazer': 1,\n",
    "    'other': 2\n",
    "}\n",
    "merged['product_category_name'] = merged['product_category_name'].apply(lambda x: x if x in category_to_label else 'other')\n",
    "merged['product_category_name'] = merged['product_category_name'].map(category_to_label)\n",
    "\n",
    "\n",
    "name_to_label = {\n",
    "    'bed_bath_table': 0,\n",
    "    'sports_leisure': 1,\n",
    "    'other': 2\n",
    "}\n",
    "merged['product_category_name_english'] = merged['product_category_name_english'].apply(lambda x: x if x in name_to_label else 'other')\n",
    "merged['product_category_name_english'] = merged['product_category_name_english'].map(name_to_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:41.380708Z",
     "start_time": "2024-10-07T14:21:41.361235Z"
    },
    "id": "4KeODBwk1Tw_"
   },
   "outputs": [],
   "source": [
    "category_to_label = {\n",
    "    'credit_card': 0,\n",
    "    'boleto': 1,\n",
    "    'other': 2\n",
    "}\n",
    "merged['payment_type'] = merged['payment_type'].apply(lambda x: x if x in category_to_label else 'other')\n",
    "merged['payment_type'] = merged['payment_type'].map(category_to_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:41.497812Z",
     "start_time": "2024-10-07T14:21:41.489786Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "mprYe1TUqiRz",
    "outputId": "17b26111-2f57-4ace-e57d-33ef49184f2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_score\n",
       "5.0    65145\n",
       "4.0    21811\n",
       "1.0    12865\n",
       "3.0     9533\n",
       "2.0     3839\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged['review_score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:42.659467Z",
     "start_time": "2024-10-07T14:21:41.766818Z"
    },
    "id": "aoRUm2D4WV--"
   },
   "outputs": [],
   "source": [
    "# Remove unnecessary features\n",
    "merged = merged.select_dtypes(include=['int', 'float'])\n",
    "merged.drop(['customer_zip_code_prefix', 'seller_zip_code_prefix', 'order_item_id', 'product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'payment_sequential']\n",
    "               , axis=1, inplace=True)\n",
    "# save the cleaned dataset\n",
    "merged.to_csv('./data/merged_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:42.688901Z",
     "start_time": "2024-10-07T14:21:42.677070Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "AbgWgMH9fwGI",
    "outputId": "4c9248a5-c691-4f21-a9a5-2ee3db927fb6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>freight_value</th>\n",
       "      <th>product_category_name</th>\n",
       "      <th>product_weight_g</th>\n",
       "      <th>product_length_cm</th>\n",
       "      <th>product_height_cm</th>\n",
       "      <th>product_width_cm</th>\n",
       "      <th>product_category_name_english</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>payment_installments</th>\n",
       "      <th>...</th>\n",
       "      <th>purchase_day</th>\n",
       "      <th>purchase_month</th>\n",
       "      <th>total_delivery_time</th>\n",
       "      <th>approval_time</th>\n",
       "      <th>shipping_time</th>\n",
       "      <th>delivery_estimate_deviation</th>\n",
       "      <th>late_delivery</th>\n",
       "      <th>total_order_item_value</th>\n",
       "      <th>total_freight_value</th>\n",
       "      <th>total_order_freight_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>2</td>\n",
       "      <td>500.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>38.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>2</td>\n",
       "      <td>500.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>38.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>2</td>\n",
       "      <td>500.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>38.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>118.70</td>\n",
       "      <td>22.76</td>\n",
       "      <td>2</td>\n",
       "      <td>400.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>118.70</td>\n",
       "      <td>22.76</td>\n",
       "      <td>141.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>159.90</td>\n",
       "      <td>19.22</td>\n",
       "      <td>2</td>\n",
       "      <td>420.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>159.90</td>\n",
       "      <td>19.22</td>\n",
       "      <td>179.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  freight_value  product_category_name  product_weight_g  \\\n",
       "0   29.99           8.72                      2             500.0   \n",
       "1   29.99           8.72                      2             500.0   \n",
       "2   29.99           8.72                      2             500.0   \n",
       "3  118.70          22.76                      2             400.0   \n",
       "4  159.90          19.22                      2             420.0   \n",
       "\n",
       "   product_length_cm  product_height_cm  product_width_cm  \\\n",
       "0               19.0                8.0              13.0   \n",
       "1               19.0                8.0              13.0   \n",
       "2               19.0                8.0              13.0   \n",
       "3               19.0               13.0              19.0   \n",
       "4               24.0               19.0              21.0   \n",
       "\n",
       "   product_category_name_english  payment_type  payment_installments  ...  \\\n",
       "0                              2             0                   1.0  ...   \n",
       "1                              2             2                   1.0  ...   \n",
       "2                              2             2                   1.0  ...   \n",
       "3                              2             1                   1.0  ...   \n",
       "4                              2             0                   3.0  ...   \n",
       "\n",
       "   purchase_day  purchase_month  total_delivery_time  approval_time  \\\n",
       "0             0              10                    8              0   \n",
       "1             0              10                    8              0   \n",
       "2             0              10                    8              0   \n",
       "3             1               7                   13              1   \n",
       "4             2               8                    9              0   \n",
       "\n",
       "   shipping_time  delivery_estimate_deviation  late_delivery  \\\n",
       "0              6                            7              0   \n",
       "1              6                            7              0   \n",
       "2              6                            7              0   \n",
       "3             12                            5              0   \n",
       "4              9                           17              0   \n",
       "\n",
       "   total_order_item_value  total_freight_value  total_order_freight_value  \n",
       "0                   29.99                 8.72                      38.71  \n",
       "1                   29.99                 8.72                      38.71  \n",
       "2                   29.99                 8.72                      38.71  \n",
       "3                  118.70                22.76                     141.46  \n",
       "4                  159.90                19.22                     179.12  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=merged\n",
    "X = data.drop('review_score', axis=1)\n",
    "y = data['review_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_score                     1.000000\n",
       "delivery_estimate_deviation      0.228154\n",
       "product_category_name_english    0.032753\n",
       "product_category_name            0.032753\n",
       "purchase_month                   0.028419\n",
       "price                            0.003203\n",
       "payment_type                     0.000278\n",
       "product_width_cm                -0.011835\n",
       "purchase_day                    -0.013958\n",
       "approval_time                   -0.018635\n",
       "product_length_cm               -0.020080\n",
       "product_height_cm               -0.023240\n",
       "product_weight_g                -0.026894\n",
       "freight_value                   -0.033637\n",
       "total_order_item_value          -0.038652\n",
       "payment_installments            -0.043283\n",
       "total_order_freight_value       -0.047604\n",
       "payment_value                   -0.081312\n",
       "total_freight_value             -0.098066\n",
       "shipping_time                   -0.267258\n",
       "total_delivery_time             -0.303545\n",
       "late_delivery                   -0.329049\n",
       "Name: review_score, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the correlation matrix of review_score\n",
    "corr_matrix = data.corr()\n",
    "corr_matrix['review_score'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with high correlation with review_score:\n",
      "Index(['payment_value', 'total_delivery_time', 'shipping_time',\n",
      "       'delivery_estimate_deviation', 'late_delivery', 'total_freight_value'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Set the correlation threshold\n",
    "threshold = 0.05\n",
    "\n",
    "# Get the features with correlation greater than 5% or less than -5% with review_score\n",
    "high_corr_features = corr_matrix[(corr_matrix['review_score'].abs() > threshold) & (corr_matrix['review_score'].index != 'review_score')].index\n",
    "\n",
    "# Print the features with high correlation\n",
    "print(\"Features with high correlation with review_score:\")\n",
    "print(high_corr_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_corr = data[['payment_value', 'total_delivery_time', 'shipping_time',\n",
    "       'delivery_estimate_deviation', 'late_delivery', 'total_freight_value']]\n",
    "y_corr = data['review_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:12:55.065235Z",
     "start_time": "2024-10-07T15:12:54.894650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_score\n",
      "4.0    65145\n",
      "5.0    65145\n",
      "1.0    65145\n",
      "2.0    65145\n",
      "3.0    65145\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smt, y_smt = smote.fit_resample(X, y)\n",
    "\n",
    "\n",
    "scaling_factor = 0.2\n",
    "X_smt, y_smt = resample(X_smt, y_smt, replace=False, n_samples=int(len(X_smt) * scaling_factor), random_state=42)\n",
    "\n",
    "\n",
    "print(y_smt.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Validation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:42.927839Z",
     "start_time": "2024-10-07T14:21:42.923105Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def manual_train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    np.random.seed(random_state) \n",
    "    indices = np.random.permutation(len(X))  \n",
    "    test_set_size = int(len(X) * test_size)  \n",
    "    test_indices = indices[:test_set_size]  \n",
    "    train_indices = indices[test_set_size:]  \n",
    "    \n",
    "\n",
    "    X_train = X.iloc[train_indices]\n",
    "    X_test = X.iloc[test_indices]\n",
    "    y_train = y.iloc[train_indices]\n",
    "    y_test = y.iloc[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 k fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:43.026761Z",
     "start_time": "2024-10-07T14:21:43.018250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Train size = 260580, Test size = 65145\n",
      "Fold 2: Train size = 260580, Test size = 65145\n",
      "Fold 3: Train size = 260580, Test size = 65145\n",
      "Fold 4: Train size = 260580, Test size = 65145\n",
      "Fold 5: Train size = 260580, Test size = 65145\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def manual_kfold(X, y, k=5, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(len(X))\n",
    "    fold_size = len(X) // k\n",
    "    folds = []\n",
    "\n",
    "    for i in range(k):\n",
    "        test_indices = indices[i*fold_size:(i+1)*fold_size]\n",
    "        train_indices = np.concatenate([indices[:i*fold_size], indices[(i+1)*fold_size:]])\n",
    "        folds.append((train_indices, test_indices))\n",
    "    \n",
    "    return folds\n",
    "\n",
    "\n",
    "k_folds = manual_kfold(X, y, k=5, random_state=42)\n",
    "\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(k_folds):\n",
    "    print(f\"Fold {i+1}: Train size = {len(train_idx)}, Test size = {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.ML algorithms and Experimental results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Simple algorithms: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:13:14.894643Z",
     "start_time": "2024-10-07T15:13:14.888131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_score\n",
      "4.0    65145\n",
      "5.0    65145\n",
      "1.0    65145\n",
      "2.0    65145\n",
      "3.0    65145\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:32:41.129038500Z",
     "start_time": "2024-10-07T15:13:24.976127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment with Train Size 2605: Accuracy = 0.361\n",
      "Experiment with Train Size 2605: Accuracy = 0.365\n",
      "Experiment with Train Size 2605: Accuracy = 0.364\n",
      "Experiment with Train Size 2605: Accuracy = 0.361\n",
      "Experiment with Train Size 2605: Accuracy = 0.361\n",
      "Experiment with Train Size 26058: Accuracy = 0.549\n",
      "Experiment with Train Size 26058: Accuracy = 0.552\n",
      "Experiment with Train Size 26058: Accuracy = 0.552\n",
      "Experiment with Train Size 26058: Accuracy = 0.552\n",
      "Experiment with Train Size 26058: Accuracy = 0.549\n",
      "Experiment with Train Size 82402: Accuracy = 0.691\n",
      "Experiment with Train Size 82402: Accuracy = 0.695\n",
      "Experiment with Train Size 82402: Accuracy = 0.697\n",
      "Experiment with Train Size 82402: Accuracy = 0.695\n",
      "Experiment with Train Size 82402: Accuracy = 0.690\n",
      "Experiment with Train Size 260580: Accuracy = 0.830\n",
      "Experiment with Train Size 260580: Accuracy = 0.830\n",
      "Experiment with Train Size 260580: Accuracy = 0.830\n",
      "Experiment with Train Size 260580: Accuracy = 0.830\n",
      "Experiment with Train Size 260580: Accuracy = 0.828\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.70      0.64      0.67    261796\n",
      "         2.0       0.70      0.68      0.69    259912\n",
      "         3.0       0.58      0.58      0.58    258612\n",
      "         4.0       0.53      0.46      0.49    261900\n",
      "         5.0       0.55      0.68      0.60    260680\n",
      "\n",
      "    accuracy                           0.61   1302900\n",
      "   macro avg       0.61      0.61      0.61   1302900\n",
      "weighted avg       0.61      0.61      0.61   1302900\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Coarse Classification (Good vs. Bad): 0.7579\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "train_sizes = np.concatenate(([0.01], np.logspace(-1, 0, 3)))  # [0.01, 0.1, 0.5, 1.0]\n",
    "n_repeats = 20  \n",
    "accuracies = []\n",
    "train_sizes_actual = []\n",
    "all_y_test = []\n",
    "all_y_pred = []\n",
    "\n",
    "experiments_per_size = n_repeats // len(train_sizes)\n",
    "\n",
    "for size in train_sizes:\n",
    "    for i in range(experiments_per_size):\n",
    "        X_train, X_test, y_train, y_test = manual_train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "       \n",
    "       \n",
    "        subset_train_idx = X_train.index[:int(len(X_train) * size)]\n",
    "        X_train_subset = X_train.loc[subset_train_idx]\n",
    "        y_train_subset = y_train.loc[subset_train_idx]\n",
    "        \n",
    "       \n",
    "        model = RandomForestClassifier(n_estimators=200, random_state=42, max_depth=None)\n",
    "        model.fit(X_train_subset, y_train_subset)\n",
    "        \n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        train_sizes_actual.append(len(X_train_subset))\n",
    "        \n",
    "        all_y_test.extend(y_test)\n",
    "        all_y_pred.extend(y_pred)\n",
    "        \n",
    "        print(f\"Experiment with Train Size {len(X_train_subset)}: Accuracy = {accuracy:.3f}\")\n",
    "\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_sizes_actual, accuracies, marker='o', label='Accuracy')\n",
    "plt.title(\"Learning Curve with Train Sizes\")\n",
    "plt.xlabel(\"Training Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(x=train_sizes_actual, y=accuracies, yerr=std_accuracy, fmt='o', label='Accuracy with error bar')\n",
    "plt.title(\"Learning Curve with Error Bars\")\n",
    "plt.xlabel(\"Training Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "conf_matrix = confusion_matrix(all_y_test, all_y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix for Original 5-Class Classification\")\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(all_y_test, all_y_pred))\n",
    "\n",
    "\n",
    "y_test_coarse = pd.Series(all_y_test).replace({1: 0, 2: 0, 4: 1, 5: 1})\n",
    "y_pred_coarse = pd.Series(all_y_pred).replace({1: 0, 2: 0, 4: 1, 5: 1})\n",
    "\n",
    "valid_indices = y_test_coarse != 3\n",
    "y_test_coarse = y_test_coarse[valid_indices]\n",
    "y_pred_coarse = y_pred_coarse[valid_indices]\n",
    "\n",
    "\n",
    "conf_matrix_coarse = confusion_matrix(y_test_coarse, y_pred_coarse)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_coarse, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Bad', 'Good'], yticklabels=['Bad', 'Good'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix for Coarse Classification (Good vs. Bad)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "accuracy_coarse = accuracy_score(y_test_coarse, y_pred_coarse)\n",
    "print(f\"Accuracy for Coarse Classification (Good vs. Bad): {accuracy_coarse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Best Params = {'n_estimators': 200, 'max_depth': None}, Test Accuracy = 0.541\n",
      "Fold 2: Best Params = {'n_estimators': 200, 'max_depth': None}, Test Accuracy = 0.549\n",
      "Fold 3: Best Params = {'n_estimators': 200, 'max_depth': None}, Test Accuracy = 0.537\n",
      "Fold 4: Best Params = {'n_estimators': 200, 'max_depth': None}, Test Accuracy = 0.556\n",
      "Fold 5: Best Params = {'n_estimators': 200, 'max_depth': None}, Test Accuracy = 0.558\n",
      "Nested CV Mean Accuracy: 0.548, Std: 0.009\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def nested_cross_validation(X, y, param_grid, k_outer=5, k_inner=3):\n",
    "    outer_folds = manual_kfold(X, y, k=k_outer, random_state=42)\n",
    "    outer_scores = []\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(outer_folds):\n",
    "        X_train_outer, X_test_outer = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train_outer, y_test_outer = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        best_params = None\n",
    "        best_score = -np.inf\n",
    "        \n",
    "        inner_folds = manual_kfold(X_train_outer, y_train_outer, k=k_inner, random_state=42)\n",
    "        \n",
    "        for params in param_grid:\n",
    "            inner_scores = []\n",
    "            \n",
    "            for inner_train_idx, inner_val_idx in inner_folds:\n",
    "                X_train_inner, X_val_inner = X_train_outer.iloc[inner_train_idx], X_train_outer.iloc[inner_val_idx]\n",
    "                y_train_inner, y_val_inner = y_train_outer.iloc[inner_train_idx], y_train_outer.iloc[inner_val_idx]\n",
    "                \n",
    "                model = RandomForestClassifier(n_estimators=params['n_estimators'], max_depth=params['max_depth'], random_state=42)\n",
    "                model.fit(X_train_inner, y_train_inner)\n",
    "                \n",
    "                y_val_pred = model.predict(X_val_inner)\n",
    "                inner_scores.append(accuracy_score(y_val_inner, y_val_pred))\n",
    "            \n",
    "            mean_inner_score = np.mean(inner_scores)\n",
    "            \n",
    "            if mean_inner_score > best_score:\n",
    "                best_score = mean_inner_score\n",
    "                best_params = params\n",
    "        \n",
    "        best_model = RandomForestClassifier(n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], random_state=42)\n",
    "        best_model.fit(X_train_outer, y_train_outer)\n",
    "        y_test_pred = best_model.predict(X_test_outer)\n",
    "        \n",
    "        test_accuracy = accuracy_score(y_test_outer, y_test_pred)\n",
    "        outer_scores.append(test_accuracy)\n",
    "        \n",
    "        print(f\"Fold {fold_idx+1}: Best Params = {best_params}, Test Accuracy = {test_accuracy:.3f}\")\n",
    "    \n",
    "    mean_score = np.mean(outer_scores)\n",
    "    std_score = np.std(outer_scores)\n",
    "    print(f\"Nested CV Mean Accuracy: {mean_score:.3f}, Std: {std_score:.3f}\")\n",
    "\n",
    "# Sample 10% of the dataset\n",
    "X_sample, _, y_sample, _ = train_test_split(X, y, test_size=0.9, random_state=42)\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': 50, 'max_depth': 10},\n",
    "    {'n_estimators': 100, 'max_depth': 20},\n",
    "    {'n_estimators': 200, 'max_depth': None}\n",
    "]\n",
    "\n",
    "nested_cross_validation(X_sample, y_sample, param_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Medium algorithms: MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_model(X_train_scaled, X_test_scaled, y_train, y_test, n_epochs=20, eta=0.01, optimizer='Adam'):\n",
    "    # Detect if CUDA is available and set the device accordingly\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Convert Pandas DataFrames to NumPy arrays\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = X_train_scaled.to_numpy(), X_test_scaled.to_numpy(), y_train.to_numpy(), y_test.to_numpy()\n",
    "    # Convert to PyTorch tensors and move them to the appropriate device\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = torch.from_numpy(X_train_scaled).to(device), torch.from_numpy(X_test_scaled).to(device), torch.from_numpy(y_train).to(device), torch.from_numpy(y_test).to(device)\n",
    "\n",
    "    # Check number of classes\n",
    "    n_classes = len(torch.unique(y_train))\n",
    "\n",
    "    # Subtract 1 from target values to ensurezero-indexing\n",
    "    y_train = y_train - 1\n",
    "    y_test = y_test - 1\n",
    "\n",
    "    # Create iterable dataset in Torch format\n",
    "    train_ds = torch.utils.data.TensorDataset(X_train_scaled, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=32)\n",
    "    test_ds = torch.utils.data.TensorDataset(X_test_scaled, y_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "    # Create the multi-layer perceptron model and move it to the device\n",
    "    model = nn.Sequential(\n",
    "            nn.Linear(X_train_scaled.shape[1], 16, dtype=torch.float64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8, dtype=torch.float64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, n_classes, dtype=torch.float64),\n",
    "            ).to(device)  # Move model to device\n",
    "\n",
    "    # Define the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Setup the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=eta)\n",
    "\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, (xi, yi) in enumerate(train_loader):\n",
    "            # Compute loss F(w)\n",
    "            xi, yi = xi.to(device), yi.to(device).long()  # Move data to the device\n",
    "            logits = model(xi)\n",
    "            loss = criterion(logits, yi)\n",
    "\n",
    "            #predictions = torch.argmax(logits, dim=1)\n",
    "            #train_acc = torch.mean(torch.eq(predictions, yi).float()).item()\n",
    "\n",
    "            loss.backward()               # Backward pass (compute parameter gradients)\n",
    "            optimizer.step()              # Update weight parameter using Adam\n",
    "            optimizer.zero_grad()         # Reset gradients to zero for next iteration\n",
    "\n",
    "    # Evaluate the model, not necessary for training\n",
    "    test_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for xi, yi in test_loader:\n",
    "            xi, yi = xi.to(device), yi.to(device).long()  # Move data to the device\n",
    "            logits = model(xi)\n",
    "            loss = criterion(logits, yi)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            test_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = (np.array(test_predictions) == y_test.cpu().numpy()).mean()\n",
    "\n",
    "    return accuracy, y_test.cpu().numpy(), np.array(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes = np.concatenate(([0.01], np.logspace(-1, 0, 3)))  # [0.01, 0.1, 0.5, 1.0]\n",
    "n_repeats = 20\n",
    "accuracies = []\n",
    "train_sizes_actual = []\n",
    "all_y_test = []\n",
    "all_y_pred = []\n",
    "\n",
    "\n",
    "experiments_per_size = n_repeats // len(train_sizes)\n",
    "\n",
    "\n",
    "for size in train_sizes:\n",
    "    for i in range(experiments_per_size):\n",
    "        X_train, X_test, y_train, y_test = manual_train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "\n",
    "\n",
    "        subset_train_idx = X_train.index[:int(len(X_train) * size)]\n",
    "        X_train_subset = X_train.loc[subset_train_idx]\n",
    "        y_train_subset = y_train.loc[subset_train_idx]\n",
    "\n",
    "\n",
    "        accuracy, y_test_vals, y_pred_vals = train_eval_model(X_train_subset, X_test, y_train_subset, y_test, n_epochs=20, eta=0.01, optimizer='Adam')\n",
    "        accuracies.append(accuracy)\n",
    "        train_sizes_actual.append(len(X_train_subset))\n",
    "\n",
    "        all_y_test.extend(y_test_vals)\n",
    "        all_y_pred.extend(y_pred_vals)\n",
    "\n",
    "        print(f\"Experiment with Train Size {len(X_train_subset)}: Accuracy = {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_sizes_actual' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Calculate mean accuracy for each train size\u001B[39;00m\n\u001B[1;32m      2\u001B[0m mean_accuracies \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m----> 3\u001B[0m unique_train_sizes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mset\u001B[39m(\u001B[43mtrain_sizes_actual\u001B[49m)))  \u001B[38;5;66;03m# Get unique train sizes\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m train_size \u001B[38;5;129;01min\u001B[39;00m unique_train_sizes:\n\u001B[1;32m      5\u001B[0m     mean_accuracies\u001B[38;5;241m.\u001B[39mappend(np\u001B[38;5;241m.\u001B[39mmean([acc \u001B[38;5;28;01mfor\u001B[39;00m acc, size \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(accuracies, train_sizes_actual) \u001B[38;5;28;01mif\u001B[39;00m size \u001B[38;5;241m==\u001B[39m train_size]))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_sizes_actual' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate mean accuracy for each train size\n",
    "mean_accuracies = []\n",
    "unique_train_sizes = sorted(list(set(train_sizes_actual)))  # Get unique train sizes\n",
    "for train_size in unique_train_sizes:\n",
    "    mean_accuracies.append(np.mean([acc for acc, size in zip(accuracies, train_sizes_actual) if size == train_size]))\n",
    "std_accuracies = []\n",
    "for train_size in unique_train_sizes:\n",
    "    std_accuracies.append(np.std([acc for acc, size in zip(accuracies, train_sizes_actual) if size == train_size]))\n",
    "# learning curve\n",
    "plt.figure()\n",
    "plt.plot(unique_train_sizes, mean_accuracies, marker='o', label='Accuracy')\n",
    "plt.title(\"Learning Curve with Train Sizes\")\n",
    "plt.xlabel(\"Training Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Error bar\n",
    "plt.figure()\n",
    "plt.errorbar(x=unique_train_sizes, y=mean_accuracies, yerr=std_accuracies, fmt='o', label='Accuracy with error bar')\n",
    "plt.title(\"Learning Curve with Error Bars\")\n",
    "plt.xlabel(\"Training Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(all_y_test, all_y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix for Original 5-Class Classification\")\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(all_y_test, all_y_pred))\n",
    "\n",
    "\n",
    "# Coarse classification\n",
    "y_test_coarse = pd.Series(all_y_test).replace({1: 0, 2: 0, 4: 1, 5: 1})\n",
    "y_pred_coarse = pd.Series(all_y_pred).replace({1: 0, 2: 0, 4: 1, 5: 1})\n",
    "\n",
    "# Remove neutral class\n",
    "valid_indices = y_test_coarse != 3\n",
    "y_test_coarse = y_test_coarse[valid_indices]\n",
    "y_pred_coarse = y_pred_coarse[valid_indices]\n",
    "\n",
    "# Confusion matrix for coarse classification\n",
    "conf_matrix_coarse = confusion_matrix(y_test_coarse, y_pred_coarse)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_coarse, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Bad', 'Good'], yticklabels=['Bad', 'Good'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix for Coarse Classification (Good vs. Bad)\")\n",
    "plt.show()\n",
    "\n",
    "accuracy_coarse = accuracy_score(y_test_coarse, y_pred_coarse)\n",
    "print(f\"Accuracy for Coarse Classification (Good vs. Bad): {accuracy_coarse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict custom review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron_pytorch(X_train_scaled, X_test_scaled, y_train, y_test, params=None):\n",
    "\n",
    "    # Set default values and override them if provided in params\n",
    "    n_epochs = params.get('n_epochs', 30)\n",
    "    batch_size = params.get('batch_size', 32)\n",
    "    layer_sizes = params.get('layer_sizes', [64, 32, 16, 8])\n",
    "\n",
    "\n",
    "    # Detect if CUDA is available and set the device accordingly\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Convert Pandas DataFrames to NumPy arrays\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = X_train_scaled.to_numpy(), X_test_scaled.to_numpy(), y_train.to_numpy(), y_test.to_numpy()\n",
    "    # Convert to PyTorch tensors and move them to the appropriate device\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = torch.from_numpy(X_train_scaled).to(device), torch.from_numpy(X_test_scaled).to(device), torch.from_numpy(y_train).to(device), torch.from_numpy(y_test).to(device)\n",
    "\n",
    "    # Check number of classes\n",
    "    n_classes = len(torch.unique(y_train))\n",
    "\n",
    "    # Subtract 1 from target values to ensurezero-indexing\n",
    "    y_train = y_train - 1\n",
    "    y_test = y_test - 1\n",
    "\n",
    "    # Create iterable dataset in Torch format\n",
    "    train_ds = torch.utils.data.TensorDataset(X_train_scaled, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size)\n",
    "    test_ds = torch.utils.data.TensorDataset(X_test_scaled, y_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test_ds, batch_size)\n",
    "\n",
    "    # Create the multi-layer perceptron model and move it to the device\n",
    "    model = nn.Sequential()\n",
    "    input_size = X_train_scaled.shape[1]\n",
    "    for i, layer_size in enumerate(layer_sizes):\n",
    "        model.add_module(f'Linear_{i}', nn.Linear(input_size, layer_size, dtype=torch.float64))\n",
    "        model.add_module(f'ReLU_{i}', nn.ReLU())\n",
    "        input_size = layer_size\n",
    "    model.add_module('Output', nn.Linear(layer_sizes[-1], n_classes, dtype=torch.float64))\n",
    "    model.to(device)\n",
    "\n",
    "    # Define the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Setup the optimizer - Adam\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, (xi, yi) in enumerate(train_loader):\n",
    "            # Compute loss F(w)\n",
    "            xi, yi = xi.to(device), yi.to(device).long()  # Move data to the device\n",
    "            logits = model(xi)\n",
    "            loss = criterion(logits, yi)\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            train_acc = torch.mean(torch.eq(predictions, yi).float()).item()\n",
    "\n",
    "            loss.backward()               # Backward pass (compute parameter gradients)\n",
    "            optimizer.step()              # Update weight parameter using Adam\n",
    "            optimizer.zero_grad()         # Reset gradients to zero for next iteration\n",
    "\n",
    "    # Evaluate the model, not necessary for training\n",
    "    with torch.no_grad():\n",
    "        for xi, yi in test_loader:\n",
    "            xi, yi = xi.to(device), yi.to(device).long()  # Move data to the device\n",
    "            logits = model(xi)\n",
    "            loss = criterion(logits, yi)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            accuracy = torch.mean(torch.eq(predictions, yi).float()).item()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:31:33.731649Z",
     "start_time": "2024-10-07T14:29:42.273649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled shape: torch.Size([260580, 19])\n",
      "X_test_scaled shape: torch.Size([65145, 19])\n",
      "Test Accuracy: 0.2800\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = manual_train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "\n",
    "params = {\n",
    "    'n_epochs': 20,\n",
    "    'batch_size': 32,\n",
    "    'layer_sizes': [64, 32, 16, 8]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "accuracy = train_perceptron_pytorch(X_train_scaled, X_test_scaled, y_train, y_test, params)\n",
    "\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nested cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cross_validation(X, y, param_grid, outer_k=5, inner_k=3):\n",
    "    outer_kf = manual_kfold(X, y, k=outer_k, random_state=42)\n",
    "    outer_scores = []\n",
    "\n",
    "    for outer_fold, (train_index, test_index) in enumerate(outer_kf):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        best_params = None\n",
    "        best_score = -np.inf\n",
    "\n",
    "        inner_kf = manual_kfold(X_train, y_train, k=inner_k, random_state=42)\n",
    "\n",
    "        for params in param_grid:\n",
    "            inner_scores = []\n",
    "            for inner_fold, (inner_train_index, inner_val_index) in enumerate(inner_kf):\n",
    "                X_train_inner, X_val_inner = X_train.iloc[inner_train_index], X_train.iloc[inner_val_index]\n",
    "                y_train_inner, y_val_inner = y_train.iloc[inner_train_index], y_train.iloc[inner_val_index]\n",
    "\n",
    "                # Scaling\n",
    "                scaler = StandardScaler()\n",
    "                X_train_inner_scaled = scaler.fit_transform(X_train_inner)\n",
    "                X_val_inner_scaled = scaler.transform(X_val_inner)\n",
    "\n",
    "                # Convert to Pandas DataFrame\n",
    "                X_train_inner_scaled = pd.DataFrame(X_train_inner_scaled, columns=X_train_inner.columns)\n",
    "                X_val_inner_scaled = pd.DataFrame(X_val_inner_scaled, columns=X_val_inner.columns)\n",
    "\n",
    "                # Train model\n",
    "                val_acc = train_perceptron_pytorch(X_train_inner_scaled, X_val_inner_scaled, y_train_inner, y_val_inner, params)\n",
    "                inner_scores.append(val_acc)\n",
    "\n",
    "            mean_inner_score = np.mean(inner_scores)\n",
    "            if mean_inner_score > best_score:\n",
    "                best_score = mean_inner_score\n",
    "                best_params = params\n",
    "\n",
    "        # Train the model with the best parameters\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Convert to Pandas DataFrame\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "        test_acc = train_perceptron_pytorch(X_train_scaled, X_test_scaled, y_train, y_test, best_params)\n",
    "        outer_scores.append(test_acc)\n",
    "\n",
    "        print(f\"Outer Fold {outer_fold + 1}: Best Params = {best_params}, Test Accuracy = {test_acc:.3f}\")\n",
    "\n",
    "    mean_score = np.mean(outer_scores)\n",
    "    std_score = np.std(outer_scores)\n",
    "    print(f\"Nested CV Mean Accuracy: {mean_score:.3f}, Std: {std_score:.3f}\")\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {'n_epochs': 20, 'batch_size': 32, 'layer_sizes': [128, 64, 32, 16]},\n",
    "    {'n_epochs': 20, 'batch_size': 64, 'layer_sizes': [128, 64, 32, 16]},\n",
    "    {'n_epochs': 20, 'batch_size': 64, 'layer_sizes': [128, 64, 32, 16, 8]},\n",
    "    {'n_epochs': 30, 'batch_size': 32, 'layer_sizes': [128, 64, 32, 16]},\n",
    "    {'n_epochs': 30, 'batch_size': 64, 'layer_sizes': [128, 64, 32, 16]},\n",
    "    {'n_epochs': 30, 'batch_size': 64, 'layer_sizes': [128, 64, 32, 16, 8]}\n",
    "]\n",
    "\n",
    "\n",
    "nested_cross_validation(X, y, param_grid, outer_k=5, inner_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Complex algorithms: TabNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:21:50.260911900Z",
     "start_time": "2024-10-07T14:04:38.632193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.93206 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.1077  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.93141 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.10706 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.93134 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.10699 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.93906 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.1322  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.93846 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.13163 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.93839 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.13156 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.94276 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.14698 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.94224 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.14649 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.94218 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.14644 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.39511 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.12997 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.39454 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.12939 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.39448 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.12933 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.34576 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.16998 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.3453  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.1695  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.34525 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.16945 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.32099 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.2419  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.32061 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.24156 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.32057 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.24152 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.90081 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.53675 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.90033 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.53621 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.90027 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.53614 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.7314  | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.2764  | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.731   | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.276   | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.73096 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.27595 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.05484 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.93188 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.05456 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.9316  | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.05453 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.93157 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 1.96192 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.21782 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.96131 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.2173  | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.96125 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.21724 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.95663 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.22336 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.95608 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.22287 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.95602 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.22282 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.00833 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.18083 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.00786 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.18041 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.00781 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.18036 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.58551 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.36224 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.58498 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.36172 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.58492 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.36167 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.52515 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.47105 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.52468 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.47061 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.52463 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.47056 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.50185 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.5227  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.50152 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.52238 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.50148 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.52234 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.98183 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.85071 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.98133 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.85021 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.98127 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.85015 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.77769 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.93552 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.77728 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.93515 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.77724 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.93511 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.78586 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 3.25939 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.78562 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 3.25914 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.7856  | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 3.25911 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.08224 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.97681 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.08163 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.97617 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.08156 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.9761  | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.09037 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 1.96101 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.0898  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 1.96045 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.08974 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 1.96038 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.05614 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.87184 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.05568 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.87138 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.05562 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.87133 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.91139 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.02853 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.91085 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.02805 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.91079 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.02799 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.92382 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.03277 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.92339 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.03238 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.92334 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.03234 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.88371 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.04135 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.88339 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.04107 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.88336 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.04104 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.94733 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.61451 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.94684 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.61403 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.94678 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.61397 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.99986 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.53891 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.99949 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.53857 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.99945 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.53853 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.9996  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 6.27984 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.99934 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 6.27963 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.99931 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 6.27961 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.11878 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.46273 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.11819 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.46208 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.11812 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.46201 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.09536 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.47371 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.09483 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.47312 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.09477 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.47306 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.0269  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.42973 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.02648 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.42922 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.02643 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.42917 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.25611 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.30217 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.25563 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.30157 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.25557 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.3015  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.09592 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.18456 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.09552 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.18406 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.09547 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.18401 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.12181 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.23486 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.1215  | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.23453 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.12146 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.2345  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.33149 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.94563 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.33096 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.94513 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.3309  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.94507 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.32297 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 6.01628 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.32257 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 6.01591 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.32253 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 6.01587 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.35225 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.94024 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.35198 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.94    | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.35195 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.93998 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.36764 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.10414 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.36698 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.10349 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.36691 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.10342 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.43474 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.08091 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.43413 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.08033 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.43407 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.08026 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.45723 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.0105  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.45671 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.01002 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.45665 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.00996 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.62148 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.52857 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.62089 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.52802 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.62082 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.52796 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.52341 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.42272 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.52291 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.42228 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.52286 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.42223 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.3889  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.46056 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.38855 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.46027 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.38851 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.46023 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.64151 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.19564 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.64102 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.19515 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.64097 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.1951  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.54922 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.2857  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.54883 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.28533 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.54879 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.28529 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.06424 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.35433 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.06401 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.35408 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.06398 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.35405 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.67006 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.54756 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.66944 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.54696 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.66937 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.54689 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.65908 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.54002 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.65849 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.53948 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.65843 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.53942 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.64113 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.59821 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.64065 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.59776 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.64059 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.59771 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 4.11568 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.77697 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.11521 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.77642 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.11515 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.77636 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.2443  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.78217 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.2439  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.78169 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.24385 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.78163 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.02263 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.88459 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.02234 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.88425 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.02231 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.88421 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.93892 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.72413 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.93843 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.72363 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.93838 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.72357 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.58083 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.34162 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.58043 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.34122 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.58038 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.34118 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.56793 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.42115 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.56765 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.4209  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.56762 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.42087 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.13173 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.68678 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.13104 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.68616 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.13096 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.68609 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.04801 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.66899 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.04739 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.66843 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.04732 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.66837 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.03598 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.61866 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.03545 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.61819 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.0354  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.61813 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.05316 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.33089 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.05253 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.33035 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.05246 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.33029 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.04487 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.34157 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.04433 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.34109 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.04427 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.34104 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.18815 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.27534 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.18776 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.27501 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.18772 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.27497 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.52251 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.02819 | val_0_accuracy: 0.45455 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.45455\n",
      "epoch 0  | loss: 3.52197 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.0277  | val_0_accuracy: 0.45455 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.45455\n",
      "epoch 0  | loss: 3.52191 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.02764 | val_0_accuracy: 0.45455 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.45455\n",
      "epoch 0  | loss: 3.54677 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.64495 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.54636 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.64458 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.54631 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.64453 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.44931 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.41154 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.44905 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.41129 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.44902 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.41126 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.06608 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 1.98212 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.0655  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 1.98152 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.06544 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 1.98145 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.08609 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.95104 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.08558 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.9505  | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.08552 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.95044 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.06599 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.87012 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.06557 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.86967 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.06552 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.86962 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.17294 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 4.00217 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.17241 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 4.00163 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.17235 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 4.00156 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.27034 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.93051 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.26991 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.93008 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.26986 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.93003 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.36932 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.09096 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.36899 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.09063 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.36895 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.09059 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.13615 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.68603 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.13567 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.6856  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.13562 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.68555 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.30218 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.91781 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.30183 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.91748 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.30179 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.91744 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.95876 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.16017 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.9585  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.15993 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.95847 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.15991 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.96821 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.73747 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.96763 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.73691 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.96756 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.73685 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.00784 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.74807 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.00731 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.74757 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.00725 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.74751 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.12126 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.95301 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.12084 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.95258 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.1208  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.95254 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.88835 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.52414 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.8878  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.52357 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.88774 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.5235  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.80295 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.68281 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.80248 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.6823  | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.80243 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.68224 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.03423 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.56487 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 3.03391 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.56449 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 3.03387 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.56445 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.37291 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.05406 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.37246 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.05358 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.37242 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.05353 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.23403 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.41296 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.23368 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.41261 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.23364 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.41258 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.21195 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.3316  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.2117  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.33136 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.21167 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.33133 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.16766 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.39674 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.81132 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.39611 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.8107  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.39604 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.81063 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.36678 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.80813 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.36619 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.80757 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.36612 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.80751 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.30727 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.7823  | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.30677 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.78183 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.30671 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.78178 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.28542 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.78802 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.28486 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.78748 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.2848  | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.78742 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.34726 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.86025 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.34678 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.85977 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.34672 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.85972 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.23365 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.86989 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.2333  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.86952 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.23327 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.86948 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.96895 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.02849 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.96842 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.02791 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.96836 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.02784 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.22219 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.03114 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.22179 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.03065 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.22174 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.0306  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.9931  | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.76437 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.99283 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.76407 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.9928  | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.76404 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.24887 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.46537 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.24823 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.46476 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.24816 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.46469 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.19773 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.39034 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.19713 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.38979 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.19707 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.38973 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.12493 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.17924 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.12443 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.17875 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.12437 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.1787  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.47715 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.31799 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.47657 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.31741 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.47651 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.31734 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.43504 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.2244  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.43457 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.22392 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.43451 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.22387 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.39911 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.25966 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.39875 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.25932 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.39871 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.25928 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.86671 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.65151 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.86615 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.65097 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.86609 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.65091 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.08574 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.76634 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.08531 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.76592 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 4.08526 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.76587 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.95024 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.72294 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.94999 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.72267 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.94996 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.72264 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.33033 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.71791 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.3297  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.71732 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.32962 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.71725 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.36862 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.71572 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.36803 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.71519 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.36796 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.71514 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.40512 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.71997 | val_0_accuracy: 0.0     |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.0\n",
      "epoch 0  | loss: 2.40462 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.71951 | val_0_accuracy: 0.0     |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.0\n",
      "epoch 0  | loss: 2.40457 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.71946 | val_0_accuracy: 0.0     |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.0\n",
      "epoch 0  | loss: 1.92696 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.77731 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 1.92642 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.77677 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 1.92636 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.77671 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 1.94337 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.8975  | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 1.9429  | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.89702 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 1.94285 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.89697 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 1.90547 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.77287 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.90513 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.77252 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 1.90509 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.77248 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 5.66315 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.62958 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.66273 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.62906 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.66268 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.62901 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.36043 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.52303 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.36012 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.52266 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.36008 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.52262 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.53885 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 5.8569  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.53865 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 5.85667 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 5.53862 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 5.85664 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.38421 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.18819 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.38358 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.18759 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.38351 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.18752 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.44257 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.13975 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.442   | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.13921 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.44194 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.13915 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.44793 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.08547 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.44746 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.08503 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.44741 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.08498 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.40314 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.93546 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.40261 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.93487 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.40255 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.93481 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.35544 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.78981 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.35498 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.78933 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.35493 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.78927 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.19529 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.80842 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.19495 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.80808 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.19492 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.80804 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.19849 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.03461 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 5.198   | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.03411 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 5.19794 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.03405 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 5.58931 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 5.34205 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.58892 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 5.34166 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.58888 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 5.34161 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.69246 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.44382 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 5.69221 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.44356 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 5.69218 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.44353 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.17101 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 3.11833 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.17035 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 3.11775 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.17028 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 3.11769 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.14711 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.13521 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.14651 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.1347  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.14645 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.13464 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.06746 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.16289 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.06695 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.16249 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.06689 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.16245 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.13231 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.7492  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.13174 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.74868 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.13168 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.74862 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.12556 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.62489 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.12508 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.62445 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.12503 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.6244  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.12946 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.72895 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.12912 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.72864 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.12909 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.7286  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.81287 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.91834 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.81239 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.91788 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.81233 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.91783 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.89833 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.0005  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.89795 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.00014 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.89791 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.0001  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.60885 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.94264 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.60862 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.94241 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.6086  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.94238 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.47029 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.82346 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.46965 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.82287 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.46958 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.8228  | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.44732 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.83603 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.44675 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.83549 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.44669 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.83543 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.42758 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.92237 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.42711 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.92192 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 2.42706 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.92187 | val_0_accuracy: 0.39394 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.39394\n",
      "epoch 0  | loss: 3.95217 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.65658 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.95165 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.656   | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.95159 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.65593 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.84303 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.83163 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.84261 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.83113 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.84256 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.83107 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.87413 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.76676 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.87382 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.76639 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.87378 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.76635 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.84191 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.98981 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.8414  | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.98926 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.84135 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.9892  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.76644 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.36171 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 3.76605 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.36129 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 3.76601 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.36124 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 4.11981 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.07086 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.11956 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.07056 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.11953 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.07053 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.36268 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.68674 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.362   | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.68607 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.36192 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.686   | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.37467 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.74893 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.37403 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.74834 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.37396 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.74827 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.4067  | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.96965 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.40618 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.96916 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.40612 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.96911 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.13913 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.25922 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.1385  | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.2586  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.13843 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.25853 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.20948 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.41656 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.20895 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.41605 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.2089  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.41599 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.42543 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.55177 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.42508 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.55141 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.42504 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.55137 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.86088 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.24587 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.86039 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.24531 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.86033 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.24525 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.55349 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 4.06773 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.55313 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 4.06729 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.55309 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 4.06724 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.04571 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.1782  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 4.04546 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.1779  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 4.04543 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 4.17787 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.49267 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.08447 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.49204 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.08385 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.49197 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.08379 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.45228 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.05726 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.4517  | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.05671 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.45164 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.05665 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.44111 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.00583 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.44062 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.00535 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.44056 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.00529 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.16085 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.76872 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.16033 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.76815 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.16027 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.76809 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.05982 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.68852 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.05937 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.68806 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.05932 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.68801 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.20795 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.73784 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.20765 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.7375  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.20761 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.73746 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.93101 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.7524  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.93054 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.75194 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.93049 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.75189 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.81173 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 5.00762 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.81137 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 5.00727 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.81134 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 5.00723 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.76486 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.65459 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.7646  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.65435 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.76458 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.65432 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.90392 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.9465  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.9034  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.9459  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.90334 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.94583 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.91336 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.98532 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.9129  | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.98477 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.91285 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.98471 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.89035 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.95363 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.89    | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.95315 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.88996 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.9531  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.80307 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.44993 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.80256 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.44937 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.8025  | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.4493  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.81218 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.5595  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.81175 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.55902 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.81171 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.55897 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.42636 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.468   | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.42604 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.46763 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.42601 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.46759 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.82321 | val_0_accuracy: 0.0303  |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.0303\n",
      "epoch 0  | loss: 3.03745 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.82273 | val_0_accuracy: 0.0303  |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.0303\n",
      "epoch 0  | loss: 3.03696 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.82268 | val_0_accuracy: 0.0303  |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.0303\n",
      "epoch 0  | loss: 3.0369  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.66344 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.89498 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.66305 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.89461 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.66301 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.89457 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.22366 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.10804 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.2234  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.1078  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.22338 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.10777 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.34176 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.42149 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.77302 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.42084 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.77243 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.42076 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 1.77237 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.40721 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.76193 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.4066  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.7614  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.40653 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.76134 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.39936 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.75275 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.39882 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.75231 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.39876 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.75226 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.78669 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.75686 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.7861  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.75627 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.78604 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.7562  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.74742 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.92018 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.74695 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.91966 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.7469  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.91961 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.92057 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.10117 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.92019 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.10079 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.92015 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.10074 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.16116 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.59473 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.16069 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.59419 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.16063 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.59413 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.29245 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.56251 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.29206 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.56209 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.29201 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.56204 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.95607 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.90801 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.95582 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.90772 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.95579 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.90769 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.68482 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.16745 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.68423 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.16681 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.68416 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.16673 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.68685 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.11363 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.6863  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.11306 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.68624 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.113   | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.60633 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.0741  | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.60589 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.07361 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.60584 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.07355 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.28896 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.20397 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.28838 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.2034  | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.28832 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.20333 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "epoch 0  | loss: 2.25439 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.10622 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.25387 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.10575 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.25382 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.1057  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.20823 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.1877  | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.20788 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.18738 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.20785 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.18735 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.16669 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.19447 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.16622 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.19401 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.16616 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.19396 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.52972 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.34457 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.52934 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.34421 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.5293  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.34418 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.29052 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.42002 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.29026 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.4198  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.29024 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.41977 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.43019 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.20856 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.42958 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.20796 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.42952 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.20789 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.33609 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.21748 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.33553 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.21692 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.33547 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.21686 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.24357 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.13272 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.2431  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.13224 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.24305 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.13219 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.36368 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.6765  | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.36314 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.67594 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.36308 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.67587 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.41252 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.74093 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.41208 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.74046 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.41203 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.74041 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.47616 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.76854 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.47581 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.76816 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.47577 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.76812 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 6.28741 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.13737 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 6.28698 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.1369  | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 6.28693 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 5.13684 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 6.11384 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.2247  | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 6.11352 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.22433 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 6.11349 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 5.22429 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 6.46894 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.60451 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 6.46872 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.60428 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 6.4687  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 5.60425 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.30587 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.24995 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.30527 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.24934 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.3052  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.24928 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.35166 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.2519  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.35112 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.25136 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.35105 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.25129 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.47969 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.28257 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.47922 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.28212 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 2.47917 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.28207 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.34871 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.69978 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.34821 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.69923 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.34816 | val_0_accuracy: 0.36364 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.36364\n",
      "epoch 0  | loss: 2.69917 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.32904 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.95601 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.32859 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.95555 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.32854 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.9555  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.43371 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.78097 | val_0_accuracy: 0.0303  |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.0303\n",
      "epoch 0  | loss: 3.43336 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.78064 | val_0_accuracy: 0.0303  |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.0303\n",
      "epoch 0  | loss: 3.43332 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.7806  | val_0_accuracy: 0.0303  |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.0303\n",
      "epoch 0  | loss: 6.51721 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 6.31668 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 6.51675 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 6.31615 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 6.5167  | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 6.31609 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 6.74329 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 6.00455 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 6.74295 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 6.00414 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 6.74291 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 6.0041  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 6.23005 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 6.34387 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 6.22981 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 6.34362 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 6.22978 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 6.34359 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.67731 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.80601 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.67668 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.80541 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.67661 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.80535 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.63094 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.81563 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.63037 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.81508 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.63031 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.81502 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.58517 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.81381 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.58471 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.81334 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 3.58466 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.81328 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 1.86091 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.44836 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.86037 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.44778 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.86031 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.44772 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 1.95626 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.59816 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.95582 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.59768 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.95577 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.59763 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.98949 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.74363 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 1.98919 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.74327 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 1.98915 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.74323 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 3.48039 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.54993 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.47992 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.54943 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.47986 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.54937 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.56355 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.67007 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.56318 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.66967 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.56314 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.66962 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.87316 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.77346 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.87292 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.7732  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.87289 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.77317 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.38392 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.43557 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.38334 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.435   | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.38328 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.43494 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.37298 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.42259 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.37243 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.42208 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.37237 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.42202 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.34694 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.44252 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.34646 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.44211 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.34641 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.44207 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.70164 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.91777 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.70113 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.91721 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.70107 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.91715 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.46544 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.80162 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.465   | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.80116 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.46495 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.80111 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.33333\n",
      "epoch 0  | loss: 3.47817 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.94056 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.47785 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.94023 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.47782 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.94019 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.94237 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.18868 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.94186 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.18822 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.94181 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.18817 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.27044 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.13637 | val_0_accuracy: 0.42424 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.42424\n",
      "epoch 0  | loss: 3.27006 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.136   | val_0_accuracy: 0.42424 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.42424\n",
      "epoch 0  | loss: 3.27002 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.13596 | val_0_accuracy: 0.42424 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.42424\n",
      "epoch 0  | loss: 3.6105  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.12322 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.61025 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.12299 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.61022 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 4.12296 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.54649 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.32688 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.54587 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.32623 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.5458  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.32616 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.55435 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.35437 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.55378 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.35377 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.55372 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.35371 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.57734 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.3156  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.57685 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.31511 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.57679 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.31505 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.15808 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.35588 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.15747 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.35524 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.1574  | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.35517 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.73095 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.2871  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.73039 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.28657 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.73033 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.28651 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.99579 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.00396 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.99541 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.00358 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.99536 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.00353 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.69374 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.80649 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.69323 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.80593 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.69317 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 4.80587 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.53975 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.58622 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.53935 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.5858  | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 4.53931 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 4.58575 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 5.09478 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.89962 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 5.09454 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.89935 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 5.09451 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.89932 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.27309 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 1.8095  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.27254 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 1.80882 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.27248 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 1.80874 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.34238 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.78365 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.34189 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.78303 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.34183 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 1.78296 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.42169 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.83623 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.42126 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.83571 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.42122 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 1.83566 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.08336 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.41453 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.08283 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.414   | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.08277 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.41394 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.98107 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.36611 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.98061 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.36564 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.98056 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.36558 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.01178 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.6319  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.01144 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.63155 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.01141 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.63151 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.83084 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.54481 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 3.83038 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.54435 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.83033 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 3.5443  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.72186 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.50269 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.72149 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.50232 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.72144 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.50228 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 3.70656 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.38975 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.70631 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.38952 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.70628 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 3.38949 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.32823 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.57825 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.32768 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.57766 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.32762 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.57759 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.28317 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.57573 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.28267 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.5752  | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.28262 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.57514 | val_0_accuracy: 0.12121 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.12121\n",
      "epoch 0  | loss: 2.27099 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.56388 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.27054 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.56345 | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.27049 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.5634  | val_0_accuracy: 0.15152 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.15152\n",
      "epoch 0  | loss: 2.51247 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.73894 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.51191 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.7384  | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.51185 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.73834 | val_0_accuracy: 0.30303 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.30303\n",
      "epoch 0  | loss: 2.50045 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.94046 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.49998 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.94    | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.49993 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.93995 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.40029 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.70647 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.39995 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.70613 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.39991 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.70609 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 2.91514 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.06004 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.91465 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.05954 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.91459 | val_0_accuracy: 0.18182 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.18182\n",
      "epoch 0  | loss: 3.05949 | val_0_accuracy: 0.09091 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.09091\n",
      "epoch 0  | loss: 2.99837 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.94745 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.99802 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.94707 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 2.99798 | val_0_accuracy: 0.21212 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.21212\n",
      "epoch 0  | loss: 2.94703 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.16156 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.96979 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.1613  | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.96955 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.16127 | val_0_accuracy: 0.27273 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.27273\n",
      "epoch 0  | loss: 2.96952 | val_0_accuracy: 0.24242 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.24242\n",
      "epoch 0  | loss: 3.26103 | val_0_accuracy: 0.06061 |  0:00:00s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_accuracy = 0.06061\n",
      "Average accuracy score from outer cross-validation: 0.1818181818181818\n",
      "Best hyperparameters from each fold: [{'n_d': 8, 'n_a': 24, 'n_steps': 5, 'gamma': 1.5, 'lambda_sparse': 0.001}, {'n_d': 8, 'n_a': 24, 'n_steps': 5, 'gamma': 1.5, 'lambda_sparse': 0.0001}, {'n_d': 16, 'n_a': 24, 'n_steps': 7, 'gamma': 1.5, 'lambda_sparse': 0.001}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "\n",
    "\n",
    "data_sample = data.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# X 是特征，y 是目标变量\n",
    "X = data_sample.drop('review_score', axis=1)\n",
    "y = data_sample['review_score'].astype(int)  \n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "def manual_kfold(X, y, k=3, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(len(X))\n",
    "    fold_size = len(X) // k\n",
    "    folds = []\n",
    "\n",
    "    for i in range(k):\n",
    "        test_indices = indices[i * fold_size:(i + 1) * fold_size]\n",
    "        train_indices = np.concatenate([indices[:i * fold_size], indices[(i + 1) * fold_size:]])\n",
    "        folds.append((train_indices, test_indices))\n",
    "    \n",
    "    return folds\n",
    "\n",
    "\n",
    "outer_folds = manual_kfold(X_scaled, y, k=3, random_state=42)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_d': [8, 16, 24],\n",
    "    'n_a': [8, 16, 24],\n",
    "    'n_steps': [3, 5, 7],\n",
    "    'gamma': [1.3, 1.5, 2.0],\n",
    "    'lambda_sparse': [1e-3, 1e-4, 0]\n",
    "}\n",
    "\n",
    "best_params = []\n",
    "outer_scores = []\n",
    "\n",
    "\n",
    "for train_indices, test_indices in outer_folds:\n",
    "    X_train, X_test = X_scaled[train_indices], X_scaled[test_indices]\n",
    "    y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]\n",
    "\n",
    "\n",
    "    inner_folds = manual_kfold(X_train, y_train, k=2, random_state=42)\n",
    "\n",
    "    best_inner_score = -np.inf\n",
    "    best_inner_params = None\n",
    "\n",
    "  \n",
    "    for n_d in param_grid['n_d']:\n",
    "        for n_a in param_grid['n_a']:\n",
    "            for n_steps in param_grid['n_steps']:\n",
    "                for gamma in param_grid['gamma']:\n",
    "                    for lambda_sparse in param_grid['lambda_sparse']:\n",
    "                        \n",
    "                        inner_scores = []\n",
    "\n",
    "                        \n",
    "                        for inner_train_indices, inner_val_indices in inner_folds:\n",
    "                            X_inner_train, X_inner_val = X_train[inner_train_indices], X_train[inner_val_indices]\n",
    "                            y_inner_train, y_inner_val = y_train.iloc[inner_train_indices], y_train.iloc[inner_val_indices]\n",
    "\n",
    "                          \n",
    "                            tabnet = TabNetClassifier(\n",
    "                                n_d=n_d,\n",
    "                                n_a=n_a,\n",
    "                                n_steps=n_steps,\n",
    "                                gamma=gamma,\n",
    "                                lambda_sparse=lambda_sparse\n",
    "                            )\n",
    "\n",
    "                            tabnet.fit(\n",
    "                                X_inner_train, y_inner_train,\n",
    "                                eval_set=[(X_inner_val, y_inner_val)],\n",
    "                                eval_metric=['accuracy'],\n",
    "                                max_epochs=1,\n",
    "                                patience=3,\n",
    "                                batch_size=1024,\n",
    "                                virtual_batch_size=128,\n",
    "                                num_workers=0,\n",
    "                                drop_last=False\n",
    "                            )\n",
    "\n",
    "                           \n",
    "                            y_val_pred = tabnet.predict(X_inner_val)\n",
    "                            accuracy = accuracy_score(y_inner_val, y_val_pred)\n",
    "                            inner_scores.append(accuracy)\n",
    "\n",
    "                       \n",
    "                        avg_inner_score = np.mean(inner_scores)\n",
    "\n",
    "                       \n",
    "                        if avg_inner_score > best_inner_score:\n",
    "                            best_inner_score = avg_inner_score\n",
    "                            best_inner_params = {\n",
    "                                'n_d': n_d,\n",
    "                                'n_a': n_a,\n",
    "                                'n_steps': n_steps,\n",
    "                                'gamma': gamma,\n",
    "                                'lambda_sparse': lambda_sparse\n",
    "                            }\n",
    "\n",
    "    \n",
    "    best_model = TabNetClassifier(**best_inner_params)\n",
    "    best_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric=['accuracy'],\n",
    "        max_epochs=1,\n",
    "        patience=3,\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    \n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    outer_scores.append(test_accuracy)\n",
    "    best_params.append(best_inner_params)\n",
    "\n",
    "\n",
    "mean_score = np.mean(outer_scores)\n",
    "print(f\"Average accuracy score from outer cross-validation: {mean_score}\")\n",
    "print(f\"Best hyperparameters from each fold: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:43:39.977926Z",
     "start_time": "2024-10-07T14:40:14.105121Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 1/100 [00:05<09:34,  5.81s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.22281 | test_accuracy: 0.58585 |  0:00:05s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 2/100 [00:10<08:47,  5.38s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  | loss: 1.1606  | test_accuracy: 0.58832 |  0:00:10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 3/100 [00:16<08:51,  5.48s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2  | loss: 1.15524 | test_accuracy: 0.5923  |  0:00:16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 4/100 [00:21<08:41,  5.43s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 1.14739 | test_accuracy: 0.59203 |  0:00:21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▌         | 5/100 [00:27<08:37,  5.45s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4  | loss: 1.14556 | test_accuracy: 0.59305 |  0:00:27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 6/100 [00:33<09:05,  5.80s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5  | loss: 1.14585 | test_accuracy: 0.59216 |  0:00:33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 7/100 [00:40<09:15,  5.97s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6  | loss: 1.14361 | test_accuracy: 0.59238 |  0:00:40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 8/100 [00:46<09:21,  6.10s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7  | loss: 1.14421 | test_accuracy: 0.59375 |  0:00:46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   9%|▉         | 9/100 [00:53<09:31,  6.28s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8  | loss: 1.14266 | test_accuracy: 0.59344 |  0:00:53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|█         | 10/100 [00:59<09:37,  6.42s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9  | loss: 1.14394 | test_accuracy: 0.59216 |  0:00:59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  11%|█         | 11/100 [01:06<09:38,  6.50s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 | loss: 1.14447 | test_accuracy: 0.59053 |  0:01:06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  12%|█▏        | 12/100 [01:12<09:07,  6.22s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 | loss: 1.14194 | test_accuracy: 0.59199 |  0:01:12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  13%|█▎        | 13/100 [01:18<08:56,  6.17s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 | loss: 1.13992 | test_accuracy: 0.59526 |  0:01:18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  14%|█▍        | 14/100 [01:24<08:46,  6.13s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 | loss: 1.13668 | test_accuracy: 0.59481 |  0:01:24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  15%|█▌        | 15/100 [01:29<08:28,  5.98s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 | loss: 1.13413 | test_accuracy: 0.59645 |  0:01:29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  16%|█▌        | 16/100 [01:35<08:06,  5.80s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 | loss: 1.13153 | test_accuracy: 0.59574 |  0:01:35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  17%|█▋        | 17/100 [01:41<08:02,  5.81s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 | loss: 1.13001 | test_accuracy: 0.59517 |  0:01:41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  18%|█▊        | 18/100 [01:46<07:55,  5.80s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 | loss: 1.13068 | test_accuracy: 0.59446 |  0:01:46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  19%|█▉        | 19/100 [01:52<07:52,  5.84s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 | loss: 1.12841 | test_accuracy: 0.59645 |  0:01:52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  20%|██        | 20/100 [01:58<07:53,  5.91s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 | loss: 1.12592 | test_accuracy: 0.59574 |  0:01:58s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  21%|██        | 21/100 [02:04<07:43,  5.87s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 | loss: 1.12559 | test_accuracy: 0.59521 |  0:02:04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  22%|██▏       | 22/100 [02:10<07:26,  5.72s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21 | loss: 1.12892 | test_accuracy: 0.59521 |  0:02:10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  23%|██▎       | 23/100 [02:15<07:09,  5.58s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22 | loss: 1.12607 | test_accuracy: 0.5953  |  0:02:15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  24%|██▍       | 24/100 [02:20<07:06,  5.61s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23 | loss: 1.12413 | test_accuracy: 0.59685 |  0:02:20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  25%|██▌       | 25/100 [02:26<06:53,  5.52s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24 | loss: 1.12332 | test_accuracy: 0.59733 |  0:02:26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  26%|██▌       | 26/100 [02:31<06:51,  5.56s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25 | loss: 1.1243  | test_accuracy: 0.59645 |  0:02:31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  27%|██▋       | 27/100 [02:37<06:37,  5.45s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26 | loss: 1.12174 | test_accuracy: 0.59583 |  0:02:37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  28%|██▊       | 28/100 [02:42<06:27,  5.39s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27 | loss: 1.1224  | test_accuracy: 0.59724 |  0:02:42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  29%|██▉       | 29/100 [02:47<06:21,  5.37s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28 | loss: 1.12274 | test_accuracy: 0.59632 |  0:02:47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  30%|███       | 30/100 [02:53<06:32,  5.60s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29 | loss: 1.12176 | test_accuracy: 0.59724 |  0:02:53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  31%|███       | 31/100 [02:59<06:29,  5.64s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 1.12101 | test_accuracy: 0.59433 |  0:02:59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  32%|███▏      | 32/100 [03:05<06:19,  5.58s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31 | loss: 1.12276 | test_accuracy: 0.59693 |  0:03:04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  33%|███▎      | 33/100 [03:10<06:10,  5.53s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32 | loss: 1.1228  | test_accuracy: 0.59512 |  0:03:10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  34%|███▍      | 34/100 [03:16<06:08,  5.58s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33 | loss: 1.12212 | test_accuracy: 0.59707 |  0:03:16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  35%|███▌      | 35/100 [03:21<06:05,  5.63s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34 | loss: 1.12104 | test_accuracy: 0.59627 |  0:03:21s\n",
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 24 and best_test_accuracy = 0.59733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  35%|███▌      | 35/100 [03:25<06:21,  5.86s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Actual  Predicted\n",
      "0       2          5\n",
      "1       1          5\n",
      "2       5          5\n",
      "3       4          5\n",
      "4       5          5\n",
      "Loss history: [1.222808775235779, 1.1605984834820398, 1.1552352344863133, 1.1473934672827024, 1.1455642134092705, 1.1458457621499771, 1.1436136813278235, 1.1442117822526485, 1.1426642932800637, 1.1439424119110568, 1.1444660649481924, 1.1419429721181844, 1.1399197324066837, 1.1366809869472363, 1.1341324111062374, 1.131532362443829, 1.1300123741989185, 1.1306789731838254, 1.1284116840216163, 1.125922789658343, 1.1255912338778735, 1.1289179363438617, 1.1260696790884839, 1.1241294108039117, 1.1233177950676223, 1.1242993919530868, 1.1217356777908365, 1.122400068514291, 1.1227410118374026, 1.1217580409185535, 1.1210091464639615, 1.1227585764443315, 1.1227996413445556, 1.1221246161266993, 1.1210424820436389]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "X = data.drop('review_score', axis=1)\n",
    "y = data['review_score'].astype(int)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "tabnet_model = TabNetClassifier(\n",
    "    n_d=8,\n",
    "    n_a=24,\n",
    "    n_steps=5,\n",
    "    gamma=1.5,\n",
    "    lambda_sparse=0.001\n",
    ")\n",
    "\n",
    "\n",
    "max_epochs = 100\n",
    "\n",
    "\n",
    "with tqdm(total=max_epochs, desc=\"Training Progress\", unit=\"epoch\") as pbar:\n",
    "\n",
    "    \n",
    "    class TqdmCallback(Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "    tabnet_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_name=['test'],\n",
    "        eval_metric=['accuracy'], \n",
    "        max_epochs=max_epochs,  \n",
    "        patience=10,\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        callbacks=[TqdmCallback()] \n",
    "    )\n",
    "\n",
    "\n",
    "predictions = tabnet_model.predict(X_test)\n",
    "\n",
    "\n",
    "results = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': predictions.flatten()})\n",
    "print(results.head())\n",
    "\n",
    "print(f\"Loss history: {tabnet_model.history['loss']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:40:01.712219Z",
     "start_time": "2024-10-07T14:39:59.775746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6096\n",
      "        Actual  Predicted\n",
      "38570        1          4\n",
      "108792       0          4\n",
      "7489         4          4\n",
      "113161       3          4\n",
      "97084        4          4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "n_repeats = 4\n",
    "train_sizes = [0.01, 0.1, 0.5, 1.0]  \n",
    "accuracies = []\n",
    "train_sizes_actual = []\n",
    "all_y_test = []\n",
    "all_y_pred = []\n",
    "\n",
    "for size in train_sizes:\n",
    "    for i in range(n_repeats):\n",
    "       \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_corr, y_corr, test_size=0.2, random_state=i)\n",
    "\n",
    "        \n",
    "        train_subset_size = int(size * len(X_train))\n",
    "        X_train_subset = X_train[:train_subset_size]\n",
    "        y_train_subset = y_train[:train_subset_size]\n",
    "        \n",
    "        \n",
    "        y_train_subset = y_train_subset - y_train_subset.min()\n",
    "        y_test = y_test - y_test.min()\n",
    "        \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_subset = scaler.fit_transform(X_train_subset)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "       \n",
    "        class_weights = {label: len(y_train_subset) / count for label, count in y_train_subset.value_counts().items()}\n",
    "        sample_weights = y_train_subset.map(class_weights)\n",
    "        sample_weights = sample_weights.clip(lower=1e-6) \n",
    "\n",
    "       \n",
    "        xgb_model = XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=50,  \n",
    "            objective='multi:softmax',\n",
    "            num_class=len(np.unique(y_train_subset)),\n",
    "            tree_method='hist',\n",
    "            device='cuda'  \n",
    "        )\n",
    "\n",
    "        \n",
    "        xgb_model.fit(X_train_subset, y_train_subset, sample_weight=sample_weights)\n",
    "\n",
    "       \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "      \n",
    "        accuracies.append(accuracy)\n",
    "        train_sizes_actual.append(len(X_train_subset))\n",
    "        all_y_test.extend(y_test)\n",
    "        all_y_pred.extend(y_pred)\n",
    "\n",
    "        print(f\"Experiment with Train Size {len(X_train_subset)}: Accuracy = {accuracy:.3f}\")\n",
    "\n",
    "\n",
    "mean_accuracies = []\n",
    "std_accuracies = []\n",
    "\n",
    "for size in train_sizes:\n",
    "    accuracies_for_size = [accuracies[i] for i in range(len(train_sizes_actual)) if train_sizes_actual[i] == size * len(X_corr)]\n",
    "    mean_accuracies.append(np.mean(accuracies_for_size))\n",
    "    std_accuracies.append(np.std(accuracies_for_size))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(x=np.array(train_sizes) * len(X_corr), y=mean_accuracies, yerr=std_accuracies, fmt='o', label='Accuracy with error bar')\n",
    "plt.title(\"Learning Curve with Error Bars\")\n",
    "plt.xlabel(\"Training Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "conf_matrix = confusion_matrix(all_y_test, all_y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_corr), yticklabels=np.unique(y_corr))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix for Original 5-Class Classification\")\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(all_y_test, all_y_pred))\n",
    "\n",
    "\n",
    "y_test_coarse = pd.Series(all_y_test).replace({1: 0, 2: 0, 4: 1, 5: 1})\n",
    "y_pred_coarse = pd.Series(all_y_pred).replace({1: 0, 2: 0, 4: 1, 5: 1})\n",
    "\n",
    "\n",
    "valid_indices = y_test_coarse != 3\n",
    "y_test_coarse = y_test_coarse[valid_indices]\n",
    "y_pred_coarse = y_pred_coarse[valid_indices]\n",
    "\n",
    "\n",
    "conf_matrix_coarse = confusion_matrix(y_test_coarse, y_pred_coarse)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_coarse, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Bad', 'Good'], yticklabels=['Bad', 'Good'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix for Coarse Classification (Good vs. Bad)\")\n",
    "plt.show()\n",
    "\n",
    "accuracy_coarse = accuracy_score(y_test_coarse, y_pred_coarse)\n",
    "print(f\"Accuracy for Coarse Classification (Good vs. Bad): {accuracy_coarse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:36:35.658098Z",
     "start_time": "2024-10-07T14:36:35.651586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_score\n",
      "4    0.575521\n",
      "3    0.192689\n",
      "0    0.113655\n",
      "2    0.084219\n",
      "1    0.033916\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(y.value_counts(normalize=True))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
